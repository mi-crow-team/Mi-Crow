{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#mi-crow","title":"Mi-Crow","text":"<p>Python library for mechanistic interpretability research on Large Language Models </p>"},{"location":"#what-is-mi-crow","title":"What is Mi-Crow?","text":"<p>Mi-Crow is a Python library designed for researchers working on mechanistic interpretability of Large Language Models (LLMs). It provides a unified interface for analyzing and controlling model behavior through mechanistic interpretability methods, making it easy to understand what's happening inside neural networks.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"<ul> <li> <p>:robot: Activation Analysis</p> <p>Save and analyze model activations from any layer with minimal performance overhead</p> </li> <li> <p>:brain: SAE Training</p> <p>Train sparse autoencoders to discover interpretable features and concepts</p> </li> <li> <p>:bulb: Concept Discovery</p> <p>Identify and name concepts learned by SAE neurons through automated analysis</p> </li> <li> <p>:video_game: Model Steering</p> <p>Manipulate model behavior through concept-based interventions and activation control</p> </li> <li> <p>:hook:Hook System</p> <p>Flexible framework for intercepting and modifying activations at any layer</p> </li> <li> <p>:floppy_disk: Data Persistence</p> <p>Efficient hierarchical storage for managing large-scale experiment data</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install mi-crow\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from mi_crow.language_model import LanguageModel\n\n# Initialize a language model\nlm = LanguageModel(model_id=\"bielik\")\n\n# Run inference\noutputs = lm.forwards([\"Hello, world!\"])\n\n# Access activations and outputs\nprint(outputs.logits)\n</code></pre>"},{"location":"#training-an-sae","title":"Training an SAE","text":"<pre><code>from mi_crow.language_model import LanguageModel\nfrom mi_crow.mechanistic.sae import SaeTrainer\nfrom mi_crow.mechanistic.sae.modules import TopKSae\n\n# Load model and collect activations\nlm = LanguageModel(model_id=\"bielik\")\nactivations = lm.save_activations(\n    dataset=[\"Your text data here\"],\n    layers=[\"transformer_h_0_attn_c_attn\"]\n)\n\n# Train SAE\ntrainer = SaeTrainer(\n    model=lm,\n    layer=\"transformer_h_0_attn_c_attn\",\n    sae_class=TopKSae,\n    hyperparams={\"epochs\": 10, \"batch_size\": 256}\n)\nsae = trainer.train(activations)\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>New to Mi-Crow? Start here:</p> <ul> <li>Installation Guide - Set up your environment</li> <li>Quick Start Tutorial - Run your first example in minutes</li> <li>Core Concepts - Understand the fundamentals</li> </ul>"},{"location":"#user-guide","title":"\ud83d\udcda User Guide","text":"<p>Comprehensive guides for all features:</p> <ul> <li>Hooks System - Complete guide to the powerful hooks framework</li> <li>Fundamentals - Core concepts</li> <li>Detectors - Observing activations</li> <li>Controllers - Modifying behavior</li> <li>Registration - Hook management</li> <li> <p>Advanced Usage - Advanced patterns</p> </li> <li> <p>Workflows - Step-by-step guides for common tasks</p> </li> <li>Saving Activations</li> <li>Training SAE Models</li> <li>Concept Discovery</li> <li>Concept Manipulation</li> <li> <p>Activation Control</p> </li> <li> <p>Best Practices - Tips for effective research</p> </li> <li>Troubleshooting - Common issues and solutions</li> <li>Examples - Example notebooks overview</li> </ul>"},{"location":"#experiments","title":"\ud83e\uddea Experiments","text":"<p>Real-world experiments demonstrating Mi-Crow usage:</p> <ul> <li>Experiments Overview - Available experiments</li> <li>Verify SAE Training - Complete SAE training workflow</li> <li>SLURM Pipeline - Distributed training setup</li> </ul>"},{"location":"#api-reference","title":"\ud83d\udcd6 API Reference","text":"<p>Complete API documentation:</p> <ul> <li>API Overview - API structure and organization</li> <li>Language Model - Model loading and inference</li> <li>SAE - Sparse autoencoder APIs</li> <li>Datasets - Dataset loading and processing</li> <li>Store - Persistence layer</li> <li>Hooks - Hook system APIs</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#unified-model-interface","title":"Unified Model Interface","text":"<p>Work with any HuggingFace language model through a consistent API. No need to handle model-specific initialization details.</p>"},{"location":"#research-focused-design","title":"Research-Focused Design","text":"<p>Built specifically for interpretability research workflows:</p> <ul> <li>Comprehensive Testing: 85%+ code coverage requirement</li> <li>Type Safety: Extensive use of Python type annotations</li> <li>Documentation: Complete API reference and user guides</li> <li>CI/CD: Automated testing and deployment</li> <li>Minimal Overhead: Hook system introduces negligible latency during inference</li> </ul>"},{"location":"#flexible-architecture","title":"Flexible Architecture","text":"<p>Five core modules that work independently or together:</p> <ol> <li>Language Model - Unified interface for any HuggingFace model</li> <li>Hooks - Flexible activation interception system</li> <li>Mechanistic - SAE training and concept manipulation</li> <li>Store - Hierarchical data persistence</li> <li>Datasets - Dataset loading and processing</li> </ol>"},{"location":"#repository-links","title":"Repository &amp; Links","text":"<ul> <li>GitHub: AdamKaniasty/Inzynierka</li> <li>PyPI: mi-crow</li> <li>Documentation: This site</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use Mi-Crow in your research, please cite:</p> <pre><code>@thesis{kaniasty2025microw,\n  title={Mechanistic Interpretability for Large Language Models: A Production-Ready Framework},\n  author={Kaniasty, Adam and Kowalski, Hubert},\n  year={2025},\n  school={Warsaw University of Technology},\n  note={Engineering Thesis}\n}\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>:rocket: Quick Start</p> <p>Get up and running in minutes</p> </li> <li> <p>:book: User Guide</p> <p>Comprehensive documentation</p> </li> <li> <p>:wrench: Examples</p> <p>Explore example notebooks</p> </li> <li> <p>\ud83e\uddea Experiments</p> <p>Real-world use cases</p> </li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>mi_crow's public Python API is documented automatically from docstrings</p>"},{"location":"api/#mi_crow","title":"mi_crow","text":"<p>mi_crow: helper package for the Engineer Thesis project.</p> <p>This module is intentionally minimal. It exists to define the top-level package and to enable code coverage to include the package. Importing it should succeed without side effects.</p>"},{"location":"api/#mi_crow.ping","title":"ping","text":"<pre><code>ping()\n</code></pre> <p>Return a simple response to verify the package is wired correctly.</p> Source code in <code>src/mi_crow/__init__.py</code> <pre><code>def ping() -&gt; str:\n    \"\"\"Return a simple response to verify the package is wired correctly.\"\"\"\n    return \"pong\"\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>mi_crow's public Python API is documented automatically from docstrings.</p> <p>The top-level <code>mi_crow</code> package is intentionally minimal (it only exports things like <code>ping</code>). The real functionality lives in subpackages, which are documented in the sections below.</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Language Model - Core language model API for loading models, running inference, and managing activations</li> <li>Mechanistic Interpretability (SAE) - Sparse Autoencoders, training, concepts, and related modules</li> <li>Datasets - Dataset loading and management utilities</li> <li>Store - Persistence layer for activations, models, and runs</li> <li>Hooks - Hook system for intercepting model activations</li> </ul>"},{"location":"api/#top-level-package","title":"Top-level Package","text":""},{"location":"api/#mi_crow","title":"mi_crow","text":"<p>mi_crow: helper package for the Engineer Thesis project.</p> <p>This module is intentionally minimal. It exists to define the top-level package and to enable code coverage to include the package. Importing it should succeed without side effects.</p>"},{"location":"api/#mi_crow.ping","title":"ping","text":"<pre><code>ping()\n</code></pre> <p>Return a simple response to verify the package is wired correctly.</p> Source code in <code>src/mi_crow/__init__.py</code> <pre><code>def ping() -&gt; str:\n    \"\"\"Return a simple response to verify the package is wired correctly.\"\"\"\n    return \"pong\"\n</code></pre>"},{"location":"api/datasets/","title":"Datasets API","text":"<p>Dataset loading and management utilities for text and classification datasets.</p>"},{"location":"api/datasets/#mi_crow.datasets","title":"mi_crow.datasets","text":""},{"location":"api/datasets/#mi_crow.datasets.BaseDataset","title":"BaseDataset","text":"<pre><code>BaseDataset(ds, store, loading_strategy=LoadingStrategy.MEMORY)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for datasets with support for multiple sources, loading strategies, and Store integration.</p> <p>Loading Strategies: - MEMORY: Load entire dataset into memory (fastest random access, highest memory usage) - DISK: Save to disk, read dynamically via memory-mapped Arrow files   (supports len/getitem, lower memory usage) - STREAMING: True streaming mode using IterableDataset   (lowest memory, no len/getitem support, no stratification and limit support)</p> <p>Initialize dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance for caching/persistence</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>How to load data (MEMORY, DISK, or STREAMING)</p> <code>MEMORY</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is None, loading_strategy is invalid, or dataset operations fail</p> <code>OSError</code> <p>If file system operations fail</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n):\n    \"\"\"\n    Initialize dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance for caching/persistence\n        loading_strategy: How to load data (MEMORY, DISK, or STREAMING)\n\n    Raises:\n        ValueError: If store is None, loading_strategy is invalid, or dataset operations fail\n        OSError: If file system operations fail\n    \"\"\"\n    self._validate_initialization_params(store, loading_strategy)\n\n    self._store = store\n    self._loading_strategy = loading_strategy\n    self._dataset_dir: Path = Path(store.base_path) / store.dataset_prefix\n\n    is_iterable_input = isinstance(ds, IterableDataset)\n\n    if loading_strategy == LoadingStrategy.MEMORY:\n        # MEMORY: Convert to Dataset if needed, save to disk, load fully into memory\n        self._is_iterable = False\n        if is_iterable_input:\n            ds = Dataset.from_generator(lambda: iter(ds))\n        self._ds = self._save_and_load_dataset(ds, use_memory_mapping=False)\n    elif loading_strategy == LoadingStrategy.DISK:\n        # DISK: Save to disk, use memory-mapped Arrow files (supports len/getitem)\n        self._is_iterable = False\n        if is_iterable_input:\n            ds = Dataset.from_generator(lambda: iter(ds))\n        self._ds = self._save_and_load_dataset(ds, use_memory_mapping=True)\n    elif loading_strategy == LoadingStrategy.STREAMING:\n        # STREAMING: Convert to IterableDataset, don't save to disk (no len/getitem)\n        if not is_iterable_input:\n            ds = ds.to_iterable_dataset()\n        self._is_iterable = True\n        self._ds = ds\n        # Don't save to disk for iterable-only mode\n    else:\n        raise ValueError(\n            f\"Unknown loading strategy: {loading_strategy}. Must be one of: {[s.value for s in LoadingStrategy]}\"\n        )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.is_streaming","title":"is_streaming  <code>property</code>","text":"<pre><code>is_streaming\n</code></pre> <p>Whether this dataset is streaming (DISK or STREAMING).</p>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get item(s) by index.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, idx: IndexLike) -&gt; Any:\n    \"\"\"Get item(s) by index.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the number of items in the dataset.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.extract_texts_from_batch","title":"extract_texts_from_batch  <code>abstractmethod</code>","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Any]</code> <p>A batch as returned by iter_batches()</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of text strings ready for model inference</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef extract_texts_from_batch(self, batch: List[Any]) -&gt; List[str]:\n    \"\"\"Extract text strings from a batch.\n\n    Args:\n        batch: A batch as returned by iter_batches()\n\n    Returns:\n        List of text strings ready for model inference\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', delimiter=',', stratify_by=None, stratify_seed=None, drop_na_columns=None, **kwargs)\n</code></pre> <p>Load dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to check for None/empty values</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>ValueError</code> <p>If store is None or source is invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na_columns: Optional[List[str]] = None,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na_columns: Optional list of columns to check for None/empty values\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        ValueError: If store is None or source is invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na_columns) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    ds = cls._load_csv_source(\n        source,\n        delimiter=delimiter,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    if not use_streaming and (stratify_by or drop_na_columns):\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_disk","title":"from_disk  <code>classmethod</code>","text":"<pre><code>from_disk(store, *, loading_strategy=LoadingStrategy.MEMORY, **kwargs)\n</code></pre> <p>Load dataset from already-saved Arrow files on disk.</p> <p>Use this when you've previously saved a dataset and want to reload it without re-downloading from HuggingFace or re-applying transformations.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>Store instance pointing to where the dataset was saved    (dataset will be loaded from store.base_path/store.dataset_prefix/)</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy (MEMORY or DISK only, not STREAMING)</p> <code>MEMORY</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments (for subclass compatibility)</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance loaded from disk</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is None or loading_strategy is STREAMING</p> <code>FileNotFoundError</code> <p>If dataset directory doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Example Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_disk(\n    cls,\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from already-saved Arrow files on disk.\n\n    Use this when you've previously saved a dataset and want to reload it\n    without re-downloading from HuggingFace or re-applying transformations.\n\n    Args:\n        store: Store instance pointing to where the dataset was saved\n               (dataset will be loaded from store.base_path/store.dataset_prefix/)\n        loading_strategy: Loading strategy (MEMORY or DISK only, not STREAMING)\n        **kwargs: Additional arguments (for subclass compatibility)\n\n    Returns:\n        BaseDataset instance loaded from disk\n\n    Raises:\n        ValueError: If store is None or loading_strategy is STREAMING\n        FileNotFoundError: If dataset directory doesn't exist\n        RuntimeError: If dataset loading fails\n\n    Example:\n        # First: save dataset\n        dataset_store = LocalStore(\"store/my_dataset\")\n        dataset = ClassificationDataset.from_huggingface(..., store=dataset_store)\n        # Dataset saved to: store/my_dataset/datasets/*.arrow\n\n        # Later: reload from disk\n        dataset_store = LocalStore(\"store/my_dataset\")\n        dataset = ClassificationDataset.from_disk(store=dataset_store)\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    if loading_strategy == LoadingStrategy.STREAMING:\n        raise ValueError(\"STREAMING loading strategy not supported for from_disk(). Use MEMORY or DISK.\")\n\n    dataset_dir = Path(store.base_path) / store.dataset_prefix\n\n    if not dataset_dir.exists():\n        raise FileNotFoundError(\n            f\"Dataset directory not found: {dataset_dir}. \"\n            f\"Make sure you've previously saved a dataset to this store location.\"\n        )\n\n    # Verify it's a valid Arrow dataset directory\n    arrow_files = list(dataset_dir.glob(\"*.arrow\"))\n    if not arrow_files:\n        raise FileNotFoundError(\n            f\"No Arrow files found in {dataset_dir}. Directory exists but doesn't contain a valid dataset.\"\n        )\n\n    try:\n        use_memory_mapping = loading_strategy == LoadingStrategy.DISK\n        ds = load_from_disk(str(dataset_dir), keep_in_memory=not use_memory_mapping)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load dataset from {dataset_dir}. Error: {e}\") from e\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_disk--first-save-dataset","title":"First: save dataset","text":"<p>dataset_store = LocalStore(\"store/my_dataset\") dataset = ClassificationDataset.from_huggingface(..., store=dataset_store)</p>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_disk--dataset-saved-to-storemy_datasetdatasetsarrow","title":"Dataset saved to: store/my_dataset/datasets/*.arrow","text":""},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_disk--later-reload-from-disk","title":"Later: reload from disk","text":"<p>dataset_store = LocalStore(\"store/my_dataset\") dataset = ClassificationDataset.from_disk(store=dataset_store)</p>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(repo_id, store, *, split='train', loading_strategy=LoadingStrategy.MEMORY, revision=None, streaming=None, filters=None, limit=None, stratify_by=None, stratify_seed=None, **kwargs)\n</code></pre> <p>Load dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split (e.g., \"train\", \"validation\")</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy (MEMORY, DISK, or STREAMING)</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision/branch/tag</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming (if None, uses loading_strategy)</p> <code>None</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict of column-&gt;value pairs used for exact-match filtering</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional maximum number of rows to keep (applied after filtering/stratification)</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for deterministic stratification</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If repo_id is empty or store is None</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    streaming: Optional[bool] = None,\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split (e.g., \"train\", \"validation\")\n        loading_strategy: Loading strategy (MEMORY, DISK, or STREAMING)\n        revision: Optional git revision/branch/tag\n        streaming: Optional override for streaming (if None, uses loading_strategy)\n        filters: Optional dict of column-&gt;value pairs used for exact-match filtering\n        limit: Optional maximum number of rows to keep (applied after filtering/stratification)\n        stratify_by: Optional column to use for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for deterministic stratification\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        ValueError: If repo_id is empty or store is None\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if not repo_id or not isinstance(repo_id, str) or not repo_id.strip():\n        raise ValueError(f\"repo_id must be a non-empty string, got: {repo_id!r}\")\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    # Determine if we should use streaming for HuggingFace load_dataset\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.STREAMING)\n\n    if stratify_by and loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"Stratification is not supported for STREAMING datasets.\")\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load dataset from HuggingFace Hub: repo_id={repo_id!r}, \"\n            f\"split={split!r}, revision={revision!r}. Error: {e}\"\n        ) from e\n\n    if use_streaming:\n        if filters or limit or stratify_by:\n            raise NotImplementedError(\n                \"filters, limit, and stratification are not supported when streaming datasets. \"\n                \"Choose MEMORY or DISK loading strategy instead.\"\n            )\n    else:\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            filters=filters,\n            limit=limit,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', stratify_by=None, stratify_seed=None, drop_na_columns=None, **kwargs)\n</code></pre> <p>Load dataset from JSON or JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text (for JSON objects)</p> <code>'text'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to check for None/empty values</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>ValueError</code> <p>If store is None or source is invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na_columns: Optional[List[str]] = None,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from JSON or JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text (for JSON objects)\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na_columns: Optional list of columns to check for None/empty values\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        ValueError: If store is None or source is invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na_columns) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    ds = cls._load_json_source(\n        source,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    if not use_streaming and (stratify_by or drop_na_columns):\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.get_all_texts","title":"get_all_texts  <code>abstractmethod</code>","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all text strings in the dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not supported for streaming datasets</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef get_all_texts(self) -&gt; List[str]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings in the dataset\n\n    Raises:\n        NotImplementedError: If not supported for streaming datasets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.get_batch","title":"get_batch","text":"<pre><code>get_batch(start, batch_size)\n</code></pre> <p>Get a contiguous batch of items.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting index</p> required <code>batch_size</code> <code>int</code> <p>Number of items to retrieve</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def get_batch(self, start: int, batch_size: int) -&gt; List[Any]:\n    \"\"\"\n    Get a contiguous batch of items.\n\n    Args:\n        start: Starting index\n        batch_size: Number of items to retrieve\n\n    Returns:\n        List of items\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"get_batch not supported for STREAMING datasets. Use iter_batches instead.\")\n    if batch_size &lt;= 0:\n        return []\n    end = min(start + batch_size, len(self))\n    if start &gt;= end:\n        return []\n    return self[start:end]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.head","title":"head","text":"<pre><code>head(n=5)\n</code></pre> <p>Get first n items.</p> <p>Works for all loading strategies.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to retrieve (default: 5)</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of first n items</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def head(self, n: int = 5) -&gt; List[Any]:\n    \"\"\"\n    Get first n items.\n\n    Works for all loading strategies.\n\n    Args:\n        n: Number of items to retrieve (default: 5)\n\n    Returns:\n        List of first n items\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        items = []\n        for i, item in enumerate(self.iter_items()):\n            if i &gt;= n:\n                break\n            items.append(item)\n        return items\n    return self[:n]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.iter_batches","title":"iter_batches  <code>abstractmethod</code>","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over items in batches.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef iter_batches(self, batch_size: int) -&gt; Iterator[List[Any]]:\n    \"\"\"Iterate over items in batches.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.iter_items","title":"iter_items  <code>abstractmethod</code>","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over items one by one.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef iter_items(self) -&gt; Iterator[Any]:\n    \"\"\"Iterate over items one by one.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.sample","title":"sample","text":"<pre><code>sample(n=5)\n</code></pre> <p>Get n random items from the dataset.</p> <p>Works for MEMORY and DISK strategies only.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to sample</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of n randomly sampled items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def sample(self, n: int = 5) -&gt; List[Any]:\n    \"\"\"\n    Get n random items from the dataset.\n\n    Works for MEMORY and DISK strategies only.\n\n    Args:\n        n: Number of items to sample\n\n    Returns:\n        List of n randomly sampled items\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\n            \"sample() not supported for STREAMING datasets. Use iter_items() and sample manually.\"\n        )\n\n    dataset_len = len(self)\n    if n &lt;= 0:\n        return []\n    if n &gt;= dataset_len:\n        # Return all items in random order\n        indices = list(range(dataset_len))\n        random.shuffle(indices)\n        return [self[i] for i in indices]\n\n    # Sample n random indices\n    indices = random.sample(range(dataset_len), n)\n    # Use __getitem__ with list of indices\n    return self[indices]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset","title":"ClassificationDataset","text":"<pre><code>ClassificationDataset(ds, store, loading_strategy=LoadingStrategy.MEMORY, text_field='text', category_field='category')\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Classification dataset with text and category/label columns. Each item is a dict with 'text' and label column(s) as keys. Supports single or multiple label columns.</p> <p>Initialize classification dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label.           Can be a single string or a list of strings for multiple labels.</p> <code>'category'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_field or category_field is empty, or fields not found in dataset</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n):\n    \"\"\"\n    Initialize classification dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label.\n                      Can be a single string or a list of strings for multiple labels.\n\n    Raises:\n        ValueError: If text_field or category_field is empty, or fields not found in dataset\n    \"\"\"\n    self._validate_text_field(text_field)\n\n    # Normalize category_field to list\n    if isinstance(category_field, str):\n        self._category_fields = [category_field]\n    else:\n        self._category_fields = list(category_field)\n\n    self._validate_category_fields(self._category_fields)\n\n    # Validate dataset\n    is_iterable = isinstance(ds, IterableDataset)\n    if not is_iterable:\n        if text_field not in ds.column_names:\n            raise ValueError(f\"Dataset must have a '{text_field}' column; got columns: {ds.column_names}\")\n        for cat_field in self._category_fields:\n            if cat_field not in ds.column_names:\n                raise ValueError(f\"Dataset must have a '{cat_field}' column; got columns: {ds.column_names}\")\n        # Set format with all required columns\n        format_columns = [text_field] + self._category_fields\n        ds.set_format(\"python\", columns=format_columns)\n\n    self._text_field = text_field\n    self._category_field = category_field  # Keep original for backward compatibility\n    super().__init__(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get item(s) by index. Returns dict with 'text' and label column(s) as keys.</p> <p>For single label: {\"text\": \"...\", \"category\": \"...\"} For multiple labels: {\"text\": \"...\", \"label1\": \"...\", \"label2\": \"...\"}</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>IndexLike</code> <p>Index (int), slice, or sequence of indices</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>Single item dict or list of item dicts</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>IndexError</code> <p>If index is out of bounds</p> <code>ValueError</code> <p>If dataset is empty</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def __getitem__(self, idx: IndexLike) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Get item(s) by index. Returns dict with 'text' and label column(s) as keys.\n\n    For single label: {\"text\": \"...\", \"category\": \"...\"}\n    For multiple labels: {\"text\": \"...\", \"label1\": \"...\", \"label2\": \"...\"}\n\n    Args:\n        idx: Index (int), slice, or sequence of indices\n\n    Returns:\n        Single item dict or list of item dicts\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        IndexError: If index is out of bounds\n        ValueError: If dataset is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"Indexing not supported for STREAMING datasets. Use iter_items or iter_batches.\")\n\n    dataset_len = len(self)\n    if dataset_len == 0:\n        raise ValueError(\"Cannot index into empty dataset\")\n\n    if isinstance(idx, int):\n        if idx &lt; 0:\n            idx = dataset_len + idx\n        if idx &lt; 0 or idx &gt;= dataset_len:\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {dataset_len}\")\n        row = self._ds[idx]\n        return self._extract_item_from_row(row)\n\n    if isinstance(idx, slice):\n        start, stop, step = idx.indices(dataset_len)\n        if step != 1:\n            indices = list(range(start, stop, step))\n            selected = self._ds.select(indices)\n        else:\n            selected = self._ds.select(range(start, stop))\n        return [self._extract_item_from_row(row) for row in selected]\n\n    if isinstance(idx, Sequence):\n        # Validate all indices are in bounds\n        invalid_indices = [i for i in idx if not (0 &lt;= i &lt; dataset_len)]\n        if invalid_indices:\n            raise IndexError(f\"Indices out of bounds: {invalid_indices} (dataset length: {dataset_len})\")\n        selected = self._ds.select(list(idx))\n        return [self._extract_item_from_row(row) for row in selected]\n\n    raise TypeError(f\"Invalid index type: {type(idx)}\")\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of items in the dataset.\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"len() not supported for STREAMING datasets\")\n    return self._ds.num_rows\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.extract_texts_from_batch","title":"extract_texts_from_batch","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch of classification items.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Dict[str, Any]]</code> <p>List of dicts with 'text' and category fields</p> required <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of text strings from the batch</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'text' key is not found in any batch item</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def extract_texts_from_batch(self, batch: List[Dict[str, Any]]) -&gt; List[Optional[str]]:\n    \"\"\"Extract text strings from a batch of classification items.\n\n    Args:\n        batch: List of dicts with 'text' and category fields\n\n    Returns:\n        List of text strings from the batch\n\n    Raises:\n        ValueError: If 'text' key is not found in any batch item\n    \"\"\"\n    texts = []\n    for item in batch:\n        if \"text\" not in item:\n            raise ValueError(f\"'text' key not found in batch item. Available keys: {list(item.keys())}\")\n        texts.append(item[\"text\"])\n    return texts\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', category_field='category', delimiter=',', stratify_by=None, stratify_seed=None, drop_na=False, **kwargs)\n</code></pre> <p>Load classification dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column used for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    # Load CSV using parent's static method\n    ds = cls._load_csv_source(\n        source,\n        delimiter=delimiter,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    # Apply postprocessing if not streaming\n    if not use_streaming and (stratify_by or drop_na):\n        drop_na_columns = None\n        if drop_na:\n            cat_fields = [category_field] if isinstance(category_field, str) else category_field\n            drop_na_columns = [text_field] + list(cat_fields)\n\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_disk","title":"from_disk  <code>classmethod</code>","text":"<pre><code>from_disk(store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', category_field='category')\n</code></pre> <p>Load classification dataset from already-saved Arrow files on disk.</p> <p>Use this when you've previously saved a dataset and want to reload it without re-downloading from HuggingFace or re-applying transformations.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>Store instance pointing to where the dataset was saved</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy (MEMORY or DISK only)</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance loaded from disk</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If dataset directory doesn't exist or contains no Arrow files</p> <code>ValueError</code> <p>If required fields are not in the loaded dataset</p> Example Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_disk(\n    cls,\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from already-saved Arrow files on disk.\n\n    Use this when you've previously saved a dataset and want to reload it\n    without re-downloading from HuggingFace or re-applying transformations.\n\n    Args:\n        store: Store instance pointing to where the dataset was saved\n        loading_strategy: Loading strategy (MEMORY or DISK only)\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n\n    Returns:\n        ClassificationDataset instance loaded from disk\n\n    Raises:\n        FileNotFoundError: If dataset directory doesn't exist or contains no Arrow files\n        ValueError: If required fields are not in the loaded dataset\n\n    Example:\n        # First: save dataset\n        dataset_store = LocalStore(\"store/wgmix_test\")\n        dataset = ClassificationDataset.from_huggingface(\n            \"allenai/wildguardmix\",\n            store=dataset_store,\n            limit=100\n        )\n        # Dataset saved to: store/wgmix_test/datasets/*.arrow\n\n        # Later: reload from disk\n        dataset_store = LocalStore(\"store/wgmix_test\")\n        dataset = ClassificationDataset.from_disk(\n            store=dataset_store,\n            text_field=\"prompt\",\n            category_field=\"prompt_harm_label\"\n        )\n    \"\"\"\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    if loading_strategy == LoadingStrategy.STREAMING:\n        raise ValueError(\"STREAMING loading strategy not supported for from_disk(). Use MEMORY or DISK.\")\n\n    dataset_dir = Path(store.base_path) / store.dataset_prefix\n\n    if not dataset_dir.exists():\n        raise FileNotFoundError(\n            f\"Dataset directory not found: {dataset_dir}. \"\n            f\"Make sure you've previously saved a dataset to this store location.\"\n        )\n\n    # Verify it's a valid Arrow dataset directory\n    arrow_files = list(dataset_dir.glob(\"*.arrow\"))\n    if not arrow_files:\n        raise FileNotFoundError(\n            f\"No Arrow files found in {dataset_dir}. Directory exists but doesn't contain a valid dataset.\"\n        )\n\n    try:\n        use_memory_mapping = loading_strategy == LoadingStrategy.DISK\n        ds = load_from_disk(str(dataset_dir), keep_in_memory=not use_memory_mapping)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load dataset from {dataset_dir}. Error: {e}\") from e\n\n    # Create ClassificationDataset with the loaded dataset and field names\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_disk--first-save-dataset","title":"First: save dataset","text":"<p>dataset_store = LocalStore(\"store/wgmix_test\") dataset = ClassificationDataset.from_huggingface(     \"allenai/wildguardmix\",     store=dataset_store,     limit=100 )</p>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_disk--dataset-saved-to-storewgmix_testdatasetsarrow","title":"Dataset saved to: store/wgmix_test/datasets/*.arrow","text":""},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_disk--later-reload-from-disk","title":"Later: reload from disk","text":"<p>dataset_store = LocalStore(\"store/wgmix_test\") dataset = ClassificationDataset.from_disk(     store=dataset_store,     text_field=\"prompt\",     category_field=\"prompt_harm_label\" )</p>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(repo_id, store, *, split='train', loading_strategy=LoadingStrategy.MEMORY, revision=None, text_field='text', category_field='category', filters=None, limit=None, stratify_by=None, stratify_seed=None, streaming=None, drop_na=False, **kwargs)\n</code></pre> <p>Load classification dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision</p> <code>None</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional filters to apply (dict of column: value)</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of rows</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    streaming: Optional[bool] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split\n        loading_strategy: Loading strategy\n        revision: Optional git revision\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n        filters: Optional filters to apply (dict of column: value)\n        limit: Optional limit on number of rows\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        streaming: Optional override for streaming\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        ValueError: If parameters are invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.STREAMING)\n\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\n            \"Stratification and drop_na are not supported for streaming datasets. Use MEMORY or DISK.\"\n        )\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n\n        if use_streaming:\n            if filters or limit:\n                raise NotImplementedError(\n                    \"filters and limit are not supported when streaming datasets. Choose MEMORY or DISK.\"\n                )\n        else:\n            drop_na_columns = None\n            if drop_na:\n                cat_fields = [category_field] if isinstance(category_field, str) else category_field\n                drop_na_columns = [text_field] + list(cat_fields)\n\n            ds = cls._postprocess_non_streaming_dataset(\n                ds,\n                filters=filters,\n                limit=limit,\n                stratify_by=stratify_by,\n                stratify_seed=stratify_seed,\n                drop_na_columns=drop_na_columns,\n            )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load classification dataset from HuggingFace Hub: \"\n            f\"repo_id={repo_id!r}, split={split!r}, text_field={text_field!r}, \"\n            f\"category_field={category_field!r}. Error: {e}\"\n        ) from e\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', category_field='category', stratify_by=None, stratify_seed=None, drop_na=False, **kwargs)\n</code></pre> <p>Load classification dataset from JSON/JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the field(s) containing category/label</p> <code>'category'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from JSON/JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text\n        category_field: Name(s) of the field(s) containing category/label\n        stratify_by: Optional column used for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    # Load JSON using parent's static method\n    ds = cls._load_json_source(\n        source,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    # Apply postprocessing if not streaming\n    if not use_streaming and (stratify_by or drop_na):\n        drop_na_columns = None\n        if drop_na:\n            cat_fields = [category_field] if isinstance(category_field, str) else category_field\n            drop_na_columns = [text_field] + list(cat_fields)\n\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.get_all_texts","title":"get_all_texts","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of all text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING and dataset is very large</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def get_all_texts(self) -&gt; List[Optional[str]]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING and dataset is very large\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        return [item[\"text\"] for item in self.iter_items()]\n    return list(self._ds[self._text_field])\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.get_categories","title":"get_categories","text":"<pre><code>get_categories()\n</code></pre> <p>Get unique categories in the dataset, excluding None values.</p> <p>Returns:</p> Type Description <code>Union[List[Any], Dict[str, List[Any]]]</code> <ul> <li>For single label column: List of unique category values</li> </ul> <code>Union[List[Any], Dict[str, List[Any]]]</code> <ul> <li>For multiple label columns: Dict mapping column name to list of unique categories</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING and dataset is large</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def get_categories(self) -&gt; Union[List[Any], Dict[str, List[Any]]]:  # noqa: C901\n    \"\"\"\n    Get unique categories in the dataset, excluding None values.\n\n    Returns:\n        - For single label column: List of unique category values\n        - For multiple label columns: Dict mapping column name to list of unique categories\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING and dataset is large\n    \"\"\"\n    if len(self._category_fields) == 1:\n        # Single label: return list for backward compatibility\n        cat_field = self._category_fields[0]\n        if self._loading_strategy == LoadingStrategy.STREAMING:\n            categories = set()\n            for item in self.iter_items():\n                cat = item[cat_field]\n                if cat is not None:\n                    categories.add(cat)\n            return sorted(list(categories))  # noqa: C414\n        categories = [cat for cat in set(self._ds[cat_field]) if cat is not None]\n        return sorted(categories)\n    else:\n        # Multiple labels: return dict\n        result = {}\n        if self._loading_strategy == LoadingStrategy.STREAMING:\n            # Collect categories from all items\n            category_sets = {field: set() for field in self._category_fields}\n            for item in self.iter_items():\n                for field in self._category_fields:\n                    cat = item[field]\n                    if cat is not None:\n                        category_sets[field].add(cat)\n            for field in self._category_fields:\n                result[field] = sorted(list(category_sets[field]))  # noqa: C414\n        else:\n            # Use direct column access\n            for field in self._category_fields:\n                categories = [cat for cat in set(self._ds[field]) if cat is not None]\n                result[field] = sorted(categories)\n        return result\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.get_categories_for_texts","title":"get_categories_for_texts","text":"<pre><code>get_categories_for_texts(texts)\n</code></pre> <p>Get categories for given texts (if texts match dataset texts).</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[Optional[str]]</code> <p>List of text strings to look up</p> required <p>Returns:</p> Type Description <code>Union[List[Any], List[Dict[str, Any]]]</code> <ul> <li>For single label column: List of category values (one per text)</li> </ul> <code>Union[List[Any], List[Dict[str, Any]]]</code> <ul> <li>For multiple label columns: List of dicts with label columns as keys</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>ValueError</code> <p>If texts list is empty</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def get_categories_for_texts(self, texts: List[Optional[str]]) -&gt; Union[List[Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Get categories for given texts (if texts match dataset texts).\n\n    Args:\n        texts: List of text strings to look up\n\n    Returns:\n        - For single label column: List of category values (one per text)\n        - For multiple label columns: List of dicts with label columns as keys\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        ValueError: If texts list is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"get_categories_for_texts not supported for STREAMING datasets\")\n\n    if not texts:\n        raise ValueError(\"texts list cannot be empty\")\n\n    if len(self._category_fields) == 1:\n        # Single label: return list for backward compatibility\n        cat_field = self._category_fields[0]\n        text_to_category = {row[self._text_field]: row[cat_field] for row in self._ds}\n        return [text_to_category.get(text) for text in texts]\n    else:\n        # Multiple labels: return list of dicts\n        text_to_categories = {\n            row[self._text_field]: {field: row[field] for field in self._category_fields} for row in self._ds\n        }\n        return [text_to_categories.get(text) for text in texts]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over items in batches. Each batch is a list of dicts with 'text' and label column(s) as keys.</p> <p>For single label: [{\"text\": \"...\", \"category_column_1\": \"...\"}, ...] For multiple labels: [{\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}, ...]</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items per batch</p> required <p>Yields:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Lists of item dictionaries (batches)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt;= 0 or required fields are not found in any row</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Iterator[List[Dict[str, Any]]]:\n    \"\"\"\n    Iterate over items in batches. Each batch is a list of dicts with 'text' and label column(s) as keys.\n\n    For single label: [{\"text\": \"...\", \"category_column_1\": \"...\"}, ...]\n    For multiple labels: [{\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}, ...]\n\n    Args:\n        batch_size: Number of items per batch\n\n    Yields:\n        Lists of item dictionaries (batches)\n\n    Raises:\n        ValueError: If batch_size &lt;= 0 or required fields are not found in any row\n    \"\"\"\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be &gt; 0, got: {batch_size}\")\n\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        batch = []\n        for row in self._ds:\n            batch.append(self._extract_item_from_row(row))\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n    else:\n        # Use select to get batches with proper format\n        for i in range(0, len(self), batch_size):\n            end = min(i + batch_size, len(self))\n            batch_list = self[i:end]\n            yield batch_list\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.iter_items","title":"iter_items","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over items one by one. Yields dict with 'text' and label column(s) as keys.</p> <p>For single label: {\"text\": \"...\", \"category_column_1\": \"...\"} For multiple labels: {\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Item dictionaries with text and category fields</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are not found in any row</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def iter_items(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Iterate over items one by one. Yields dict with 'text' and label column(s) as keys.\n\n    For single label: {\"text\": \"...\", \"category_column_1\": \"...\"}\n    For multiple labels: {\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}\n\n    Yields:\n        Item dictionaries with text and category fields\n\n    Raises:\n        ValueError: If required fields are not found in any row\n    \"\"\"\n    for row in self._ds:\n        yield self._extract_item_from_row(row)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.LoadingStrategy","title":"LoadingStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Strategy for loading dataset data.</p> <p>Choose the best strategy for your use case:</p> <ul> <li> <p>MEMORY: Load entire dataset into memory (fastest random access, highest memory usage)   Best for: Small datasets that fit in memory, when you need fast random access</p> </li> <li> <p>DISK: Save to disk, read dynamically via memory-mapped Arrow files   (supports len/getitem, lower memory usage)   Best for: Large datasets that don't fit in memory, when you need random access</p> </li> <li> <p>STREAMING: True streaming mode using IterableDataset (lowest memory, no len/getitem support)   Best for: Very large datasets, when you only need sequential iteration</p> </li> </ul>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset","title":"TextDataset","text":"<pre><code>TextDataset(ds, store, loading_strategy=LoadingStrategy.DISK, text_field='text')\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Text-only dataset with support for multiple sources and loading strategies. Each item is a string (text snippet).</p> <p>Initialize text dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>DISK</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_field is empty or not found in dataset</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.DISK,\n    text_field: str = \"text\",\n):\n    \"\"\"\n    Initialize text dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n\n    Raises:\n        ValueError: If text_field is empty or not found in dataset\n    \"\"\"\n    self._validate_text_field(text_field)\n\n    # Validate and prepare dataset\n    is_iterable = isinstance(ds, IterableDataset)\n    if not is_iterable:\n        if text_field not in ds.column_names:\n            raise ValueError(f\"Dataset must have a '{text_field}' column; got columns: {ds.column_names}\")\n        # Keep only text column for memory efficiency\n        columns_to_remove = [c for c in ds.column_names if c != text_field]\n        if columns_to_remove:\n            ds = ds.remove_columns(columns_to_remove)\n        if text_field != \"text\":\n            ds = ds.rename_column(text_field, \"text\")\n        ds.set_format(\"python\", columns=[\"text\"])\n\n    self._text_field = text_field\n    super().__init__(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get text item(s) by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>IndexLike</code> <p>Index (int), slice, or sequence of indices</p> required <p>Returns:</p> Type Description <code>Union[Optional[str], List[Optional[str]]]</code> <p>Single text string or list of text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>IndexError</code> <p>If index is out of bounds</p> <code>ValueError</code> <p>If dataset is empty</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def __getitem__(self, idx: IndexLike) -&gt; Union[Optional[str], List[Optional[str]]]:\n    \"\"\"\n    Get text item(s) by index.\n\n    Args:\n        idx: Index (int), slice, or sequence of indices\n\n    Returns:\n        Single text string or list of text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        IndexError: If index is out of bounds\n        ValueError: If dataset is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"Indexing not supported for STREAMING datasets. Use iter_items or iter_batches.\")\n\n    dataset_len = len(self)\n    if dataset_len == 0:\n        raise ValueError(\"Cannot index into empty dataset\")\n\n    if isinstance(idx, int):\n        if idx &lt; 0:\n            idx = dataset_len + idx\n        if idx &lt; 0 or idx &gt;= dataset_len:\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {dataset_len}\")\n        return self._ds[idx][\"text\"]\n\n    if isinstance(idx, slice):\n        start, stop, step = idx.indices(dataset_len)\n        if step != 1:\n            indices = list(range(start, stop, step))\n            out = self._ds.select(indices)[\"text\"]\n        else:\n            out = self._ds.select(range(start, stop))[\"text\"]\n        return list(out)\n\n    if isinstance(idx, Sequence):\n        # Validate all indices are in bounds\n        invalid_indices = [i for i in idx if not (0 &lt;= i &lt; dataset_len)]\n        if invalid_indices:\n            raise IndexError(f\"Indices out of bounds: {invalid_indices} (dataset length: {dataset_len})\")\n        out = self._ds.select(list(idx))[\"text\"]\n        return list(out)\n\n    raise TypeError(f\"Invalid index type: {type(idx)}\")\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of items in the dataset.\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"len() not supported for STREAMING datasets\")\n    return self._ds.num_rows\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.extract_texts_from_batch","title":"extract_texts_from_batch","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch.</p> <p>For TextDataset, batch items are already strings, so return as-is.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Optional[str]]</code> <p>List of text strings</p> required <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of text strings (same as input)</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def extract_texts_from_batch(self, batch: List[Optional[str]]) -&gt; List[Optional[str]]:\n    \"\"\"Extract text strings from a batch.\n\n    For TextDataset, batch items are already strings, so return as-is.\n\n    Args:\n        batch: List of text strings\n\n    Returns:\n        List of text strings (same as input)\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', delimiter=',', stratify_by=None, stratify_seed=None, drop_na=False, **kwargs)\n</code></pre> <p>Load text dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column to use for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    # Load CSV using parent's static method\n    ds = cls._load_csv_source(\n        source,\n        delimiter=delimiter,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    # Apply postprocessing if not streaming\n    if not use_streaming and (stratify_by or drop_na):\n        drop_na_columns = [text_field] if drop_na else None\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_disk","title":"from_disk  <code>classmethod</code>","text":"<pre><code>from_disk(store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text')\n</code></pre> <p>Load text dataset from already-saved Arrow files on disk.</p> <p>Use this when you've previously saved a dataset and want to reload it without re-downloading from HuggingFace or re-applying transformations.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>Store instance pointing to where the dataset was saved</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy (MEMORY or DISK only)</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance loaded from disk</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If dataset directory doesn't exist or contains no Arrow files</p> Example Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_disk(\n    cls,\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from already-saved Arrow files on disk.\n\n    Use this when you've previously saved a dataset and want to reload it\n    without re-downloading from HuggingFace or re-applying transformations.\n\n    Args:\n        store: Store instance pointing to where the dataset was saved\n        loading_strategy: Loading strategy (MEMORY or DISK only)\n        text_field: Name of the column containing text\n\n    Returns:\n        TextDataset instance loaded from disk\n\n    Raises:\n        FileNotFoundError: If dataset directory doesn't exist or contains no Arrow files\n\n    Example:\n        # First: save dataset\n        dataset_store = LocalStore(\"store/my_texts\")\n        dataset = TextDataset.from_huggingface(\n            \"wikipedia\",\n            store=dataset_store,\n            limit=1000\n        )\n        # Dataset saved to: store/my_texts/datasets/*.arrow\n\n        # Later: reload from disk\n        dataset_store = LocalStore(\"store/my_texts\")\n        dataset = TextDataset.from_disk(store=dataset_store)\n    \"\"\"\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    if loading_strategy == LoadingStrategy.STREAMING:\n        raise ValueError(\"STREAMING loading strategy not supported for from_disk(). Use MEMORY or DISK.\")\n\n    dataset_dir = Path(store.base_path) / store.dataset_prefix\n\n    if not dataset_dir.exists():\n        raise FileNotFoundError(\n            f\"Dataset directory not found: {dataset_dir}. \"\n            f\"Make sure you've previously saved a dataset to this store location.\"\n        )\n\n    # Verify it's a valid Arrow dataset directory\n    arrow_files = list(dataset_dir.glob(\"*.arrow\"))\n    if not arrow_files:\n        raise FileNotFoundError(\n            f\"No Arrow files found in {dataset_dir}. Directory exists but doesn't contain a valid dataset.\"\n        )\n\n    try:\n        use_memory_mapping = loading_strategy == LoadingStrategy.DISK\n        ds = load_from_disk(str(dataset_dir), keep_in_memory=not use_memory_mapping)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load dataset from {dataset_dir}. Error: {e}\") from e\n\n    # Create TextDataset with the loaded dataset and field name\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_disk--first-save-dataset","title":"First: save dataset","text":"<p>dataset_store = LocalStore(\"store/my_texts\") dataset = TextDataset.from_huggingface(     \"wikipedia\",     store=dataset_store,     limit=1000 )</p>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_disk--dataset-saved-to-storemy_textsdatasetsarrow","title":"Dataset saved to: store/my_texts/datasets/*.arrow","text":""},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_disk--later-reload-from-disk","title":"Later: reload from disk","text":"<p>dataset_store = LocalStore(\"store/my_texts\") dataset = TextDataset.from_disk(store=dataset_store)</p>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(repo_id, store, *, split='train', loading_strategy=LoadingStrategy.MEMORY, revision=None, text_field='text', filters=None, limit=None, stratify_by=None, stratify_seed=None, streaming=None, drop_na=False, **kwargs)\n</code></pre> <p>Load text dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision</p> <code>None</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional filters to apply (dict of column: value)</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of rows</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for deterministic stratification</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    text_field: str = \"text\",\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    streaming: Optional[bool] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split\n        loading_strategy: Loading strategy\n        revision: Optional git revision\n        text_field: Name of the column containing text\n        filters: Optional filters to apply (dict of column: value)\n        limit: Optional limit on number of rows\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for deterministic stratification\n        streaming: Optional override for streaming\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        ValueError: If parameters are invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.STREAMING)\n\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\n            \"Stratification and drop_na are not supported for streaming datasets. Use MEMORY or DISK.\"\n        )\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n\n        if use_streaming:\n            if filters or limit:\n                raise NotImplementedError(\n                    \"filters and limit are not supported when streaming datasets. Choose MEMORY or DISK.\"\n                )\n        else:\n            drop_na_columns = [text_field] if drop_na else None\n            ds = cls._postprocess_non_streaming_dataset(\n                ds,\n                filters=filters,\n                limit=limit,\n                stratify_by=stratify_by,\n                stratify_seed=stratify_seed,\n                drop_na_columns=drop_na_columns,\n            )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load text dataset from HuggingFace Hub: \"\n            f\"repo_id={repo_id!r}, split={split!r}, text_field={text_field!r}. \"\n            f\"Error: {e}\"\n        ) from e\n\n    return cls(ds, store=store, loading_strategy=loading_strategy, text_field=text_field)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', stratify_by=None, stratify_seed=None, drop_na=False, **kwargs)\n</code></pre> <p>Load text dataset from JSON/JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text</p> <code>'text'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from JSON/JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text\n        stratify_by: Optional column to use for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    # Load JSON using parent's static method\n    ds = cls._load_json_source(\n        source,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    # Apply postprocessing if not streaming\n    if not use_streaming and (stratify_by or drop_na):\n        drop_na_columns = [text_field] if drop_na else None\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(source, store, *, loading_strategy=LoadingStrategy.MEMORY, text_field='text', recursive=True)\n</code></pre> <p>Load from a local directory or file(s).</p> Supported <ul> <li>Directory of .txt files (each file becomes one example)</li> <li>JSONL/JSON/CSV/TSV files with a text column</li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column/field containing text</p> <code>'text'</code> <code>recursive</code> <code>bool</code> <p>Whether to recursively search directories for .txt files</p> <code>True</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If source path doesn't exist</p> <code>ValueError</code> <p>If source is invalid or unsupported file type</p> <code>RuntimeError</code> <p>If file operations fail</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_local(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    recursive: bool = True,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load from a local directory or file(s).\n\n    Supported:\n      - Directory of .txt files (each file becomes one example)\n      - JSONL/JSON/CSV/TSV files with a text column\n\n    Args:\n        source: Path to directory or file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column/field containing text\n        recursive: Whether to recursively search directories for .txt files\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If source path doesn't exist\n        ValueError: If source is invalid or unsupported file type\n        RuntimeError: If file operations fail\n    \"\"\"\n    p = Path(source)\n    if not p.exists():\n        raise FileNotFoundError(f\"Source path does not exist: {source}\")\n\n    if p.is_dir():\n        txts: List[str] = []\n        pattern = \"**/*.txt\" if recursive else \"*.txt\"\n        try:\n            for fp in sorted(p.glob(pattern)):\n                txts.append(fp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n        except OSError as e:\n            raise RuntimeError(f\"Failed to read text files from directory {source}. Error: {e}\") from e\n\n        if not txts:\n            raise ValueError(f\"No .txt files found in directory: {source} (recursive={recursive})\")\n\n        ds = Dataset.from_dict({\"text\": txts})\n    else:\n        suffix = p.suffix.lower()\n        if suffix in {\".jsonl\", \".json\"}:\n            return cls.from_json(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n            )\n        elif suffix in {\".csv\"}:\n            return cls.from_csv(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n            )\n        elif suffix in {\".tsv\"}:\n            return cls.from_csv(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n                delimiter=\"\\t\",\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported file type: {suffix} for source: {source}. \"\n                f\"Use directory of .txt, or JSON/JSONL/CSV/TSV.\"\n            )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy, text_field=text_field)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.get_all_texts","title":"get_all_texts","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of all text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def get_all_texts(self) -&gt; List[Optional[str]]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        return list(self.iter_items())\n    return list(self._ds[\"text\"])\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over text items in batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items per batch</p> required <p>Yields:</p> Type Description <code>List[Optional[str]]</code> <p>Lists of text strings (batches)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt;= 0 or text field is not found in any row</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Iterator[List[Optional[str]]]:\n    \"\"\"\n    Iterate over text items in batches.\n\n    Args:\n        batch_size: Number of items per batch\n\n    Yields:\n        Lists of text strings (batches)\n\n    Raises:\n        ValueError: If batch_size &lt;= 0 or text field is not found in any row\n    \"\"\"\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be &gt; 0, got: {batch_size}\")\n\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        batch = []\n        for row in self._ds:\n            batch.append(self._extract_text_from_row(row))\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n    else:\n        for batch in self._ds.iter(batch_size=batch_size):\n            yield list(batch[\"text\"])\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.iter_items","title":"iter_items","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over text items one by one.</p> <p>Yields:</p> Type Description <code>Optional[str]</code> <p>Text strings from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text field is not found in any row</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def iter_items(self) -&gt; Iterator[Optional[str]]:\n    \"\"\"\n    Iterate over text items one by one.\n\n    Yields:\n        Text strings from the dataset\n\n    Raises:\n        ValueError: If text field is not found in any row\n    \"\"\"\n    for row in self._ds:\n        yield self._extract_text_from_row(row)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.random_sample","title":"random_sample","text":"<pre><code>random_sample(n, seed=None)\n</code></pre> <p>Create a new TextDataset with n randomly sampled items.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to sample</p> required <code>seed</code> <code>Optional[int]</code> <p>Optional random seed for reproducibility</p> <code>None</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>New TextDataset instance with sampled items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>ValueError</code> <p>If n &lt;= 0</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def random_sample(self, n: int, seed: Optional[int] = None) -&gt; \"TextDataset\":\n    \"\"\"Create a new TextDataset with n randomly sampled items.\n\n    Args:\n        n: Number of items to sample\n        seed: Optional random seed for reproducibility\n\n    Returns:\n        New TextDataset instance with sampled items\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        ValueError: If n &lt;= 0\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\n            \"random_sample() not supported for STREAMING datasets. Use iter_items() and sample manually.\"\n        )\n\n    if n &lt;= 0:\n        raise ValueError(f\"n must be &gt; 0, got: {n}\")\n\n    dataset_len = len(self)\n    if n &gt;= dataset_len:\n        if seed is not None:\n            random.seed(seed)\n        indices = list(range(dataset_len))\n        random.shuffle(indices)\n        sampled_ds = self._ds.select(indices)\n    else:\n        if seed is not None:\n            random.seed(seed)\n        indices = random.sample(range(dataset_len), n)\n        sampled_ds = self._ds.select(indices)\n\n    return TextDataset(\n        sampled_ds,\n        store=self._store,\n        loading_strategy=self._loading_strategy,\n        text_field=self._text_field,\n    )\n</code></pre>"},{"location":"api/datasets/#textdatasetrandom_sample","title":"TextDataset.random_sample()","text":"<p>The <code>TextDataset.random_sample()</code> method creates a new <code>TextDataset</code> instance with randomly sampled items from the original dataset. This is useful for creating smaller subsets of large datasets for testing or training.</p>"},{"location":"api/datasets/#parameters","title":"Parameters","text":"<ul> <li><code>n</code> (int): Number of items to sample. Must be greater than 0.</li> <li><code>seed</code> (Optional[int]): Optional random seed for reproducibility. If provided, ensures the same random sample is generated across runs.</li> </ul>"},{"location":"api/datasets/#returns","title":"Returns","text":"<p>A new <code>TextDataset</code> instance containing the randomly sampled items.</p>"},{"location":"api/datasets/#example","title":"Example","text":"<pre><code>from mi_crow.datasets import TextDataset\nfrom mi_crow.store import LocalStore\n\nstore = LocalStore(base_path=\"./store\")\n\n# Load a large dataset\ndataset = TextDataset.from_huggingface(\n    \"roneneldan/TinyStories\",\n    split=\"train\",\n    store=store,\n    text_field=\"text\"\n)\n\nprint(f\"Original dataset size: {len(dataset)}\")  # e.g., 2119719\n\n# Sample 1000 random items\nsampled_dataset = dataset.random_sample(1000, seed=42)\nprint(f\"Sampled dataset size: {len(sampled_dataset)}\")  # 1000\n\n# Use the sampled dataset for activation saving or training\nrun_id = lm.activations.save_activations_dataset(\n    dataset=sampled_dataset,\n    layer_signature=\"layer_0\",\n    batch_size=4\n)\n</code></pre>"},{"location":"api/datasets/#notes","title":"Notes","text":"<ul> <li>Works with <code>MEMORY</code> and <code>DISK</code> loading strategies only. Not supported for <code>STREAMING</code> datasets.</li> <li>If <code>n &gt;= len(dataset)</code>, returns all items in random order.</li> <li>The method preserves the original dataset's loading strategy, store, and text field configuration.</li> <li>For reproducible results, always specify a <code>seed</code> parameter.</li> </ul>"},{"location":"api/hooks/","title":"Hooks API","text":"<p>Hook system for intercepting and managing model activations during inference.</p>"},{"location":"api/hooks/#core-hook-classes","title":"Core Hook Classes","text":""},{"location":"api/hooks/#mi_crow.hooks.hook.Hook","title":"mi_crow.hooks.hook.Hook","text":"<pre><code>Hook(layer_signature=None, hook_type=HookType.FORWARD, hook_id=None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for hooks that can be registered on language model layers.</p> <p>Hooks provide a way to intercept and process activations during model inference. They expose PyTorch-compatible callables via get_torch_hook() while providing additional functionality like enable/disable and unique identification.</p> <p>Initialize a hook.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Layer name or index to attach hook to</p> <code>None</code> <code>hook_type</code> <code>HookType | str</code> <p>Type of hook - HookType.FORWARD or HookType.PRE_FORWARD</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier (auto-generated if not provided)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hook_type string is invalid</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def __init__(\n        self,\n        layer_signature: str | int | None = None,\n        hook_type: HookType | str = HookType.FORWARD,\n        hook_id: str | None = None\n):\n    \"\"\"\n    Initialize a hook.\n\n    Args:\n        layer_signature: Layer name or index to attach hook to\n        hook_type: Type of hook - HookType.FORWARD or HookType.PRE_FORWARD\n        hook_id: Unique identifier (auto-generated if not provided)\n\n    Raises:\n        ValueError: If hook_type string is invalid\n    \"\"\"\n    self.layer_signature = layer_signature\n    self.hook_type = self._normalize_hook_type(hook_type)\n    self.id = hook_id if hook_id is not None else str(uuid.uuid4())\n    self._enabled = True\n    self._torch_hook_handle = None\n    self._context: Optional[\"LanguageModelContext\"] = None\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.context","title":"context  <code>property</code>","text":"<pre><code>context\n</code></pre> <p>Get the LanguageModelContext associated with this hook.</p>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.enabled","title":"enabled  <code>property</code>","text":"<pre><code>enabled\n</code></pre> <p>Whether this hook is currently enabled.</p>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.disable","title":"disable","text":"<pre><code>disable()\n</code></pre> <p>Disable this hook.</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def disable(self) -&gt; None:\n    \"\"\"Disable this hook.\"\"\"\n    self._enabled = False\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.enable","title":"enable","text":"<pre><code>enable()\n</code></pre> <p>Enable this hook.</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def enable(self) -&gt; None:\n    \"\"\"Enable this hook.\"\"\"\n    self._enabled = True\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.get_torch_hook","title":"get_torch_hook","text":"<pre><code>get_torch_hook()\n</code></pre> <p>Return a PyTorch-compatible hook function.</p> <p>The returned callable will check the enabled flag before executing and call the abstract _hook_fn method.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A callable compatible with PyTorch's register_forward_hook or</p> <code>Callable</code> <p>register_forward_pre_hook APIs.</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def get_torch_hook(self) -&gt; Callable:\n    \"\"\"\n    Return a PyTorch-compatible hook function.\n\n    The returned callable will check the enabled flag before executing\n    and call the abstract _hook_fn method.\n\n    Returns:\n        A callable compatible with PyTorch's register_forward_hook or\n        register_forward_pre_hook APIs.\n    \"\"\"\n    if self.hook_type == HookType.PRE_FORWARD:\n        return self._create_pre_forward_wrapper()\n    else:\n        return self._create_forward_wrapper()\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.set_context","title":"set_context","text":"<pre><code>set_context(context)\n</code></pre> <p>Set the LanguageModelContext for this hook.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'LanguageModelContext'</code> <p>The LanguageModelContext instance</p> required Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def set_context(self, context: \"LanguageModelContext\") -&gt; None:\n    \"\"\"Set the LanguageModelContext for this hook.\n\n    Args:\n        context: The LanguageModelContext instance\n    \"\"\"\n    self._context = context\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.detector.Detector","title":"mi_crow.hooks.detector.Detector","text":"<pre><code>Detector(hook_type=HookType.FORWARD, hook_id=None, store=None, layer_signature=None)\n</code></pre> <p>               Bases: <code>Hook</code></p> <p>Abstract base class for detector hooks that collect metadata during inference.</p> <p>Detectors can accumulate data across batches and optionally save it to a Store. They are designed to observe and record information without modifying activations.</p> <p>Initialize a detector hook.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HookType | str</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier</p> <code>None</code> <code>store</code> <code>Store | None</code> <p>Optional Store for saving metadata</p> <code>None</code> <code>layer_signature</code> <code>str | int | None</code> <p>Layer to attach to (optional, for compatibility)</p> <code>None</code> Source code in <code>src/mi_crow/hooks/detector.py</code> <pre><code>def __init__(\n        self,\n        hook_type: HookType | str = HookType.FORWARD,\n        hook_id: str | None = None,\n        store: Store | None = None,\n        layer_signature: str | int | None = None\n):\n    \"\"\"\n    Initialize a detector hook.\n\n    Args:\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)\n        hook_id: Unique identifier\n        store: Optional Store for saving metadata\n        layer_signature: Layer to attach to (optional, for compatibility)\n    \"\"\"\n    super().__init__(layer_signature=layer_signature, hook_type=hook_type, hook_id=hook_id)\n    self.store = store\n    self.metadata: Dict[str, Any] = {}\n    self.tensor_metadata: Dict[str, torch.Tensor] = {}\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.detector.Detector.process_activations","title":"process_activations  <code>abstractmethod</code>","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Process activations from the hooked layer.</p> <p>This is where detector-specific logic goes (e.g., tracking top activations, computing statistics, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>Subclasses may raise exceptions for invalid inputs or processing errors</p> Source code in <code>src/mi_crow/hooks/detector.py</code> <pre><code>@abc.abstractmethod\ndef process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Process activations from the hooked layer.\n\n    This is where detector-specific logic goes (e.g., tracking top activations,\n    computing statistics, etc.).\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n\n    Raises:\n        Exception: Subclasses may raise exceptions for invalid inputs or processing errors\n    \"\"\"\n    raise NotImplementedError(\"process_activations must be implemented by subclasses\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.controller.Controller","title":"mi_crow.hooks.controller.Controller","text":"<pre><code>Controller(hook_type=HookType.FORWARD, hook_id=None, layer_signature=None)\n</code></pre> <p>               Bases: <code>Hook</code></p> <p>Abstract base class for controller hooks that modify activations during inference.</p> <p>Controllers can modify inputs (pre_forward) or outputs (forward) of layers. They are designed to actively change the behavior of the model during inference.</p> <p>Initialize a controller hook.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HookType | str</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier</p> <code>None</code> <code>layer_signature</code> <code>str | int | None</code> <p>Layer to attach to (optional, for compatibility)</p> <code>None</code> Source code in <code>src/mi_crow/hooks/controller.py</code> <pre><code>def __init__(\n        self,\n        hook_type: HookType | str = HookType.FORWARD,\n        hook_id: str | None = None,\n        layer_signature: str | int | None = None\n):\n    \"\"\"\n    Initialize a controller hook.\n\n    Args:\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)\n        hook_id: Unique identifier\n        layer_signature: Layer to attach to (optional, for compatibility)\n    \"\"\"\n    super().__init__(layer_signature=layer_signature, hook_type=hook_type, hook_id=hook_id)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.controller.Controller.modify_activations","title":"modify_activations  <code>abstractmethod</code>","text":"<pre><code>modify_activations(module, inputs, output)\n</code></pre> <p>Modify activations from the hooked layer.</p> <p>For pre_forward hooks: receives input tensor, should return modified input tensor. For forward hooks: receives input and output tensors, should return modified output tensor.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>inputs</code> <code>Tensor | None</code> <p>Input tensor (None for forward hooks if not available)</p> required <code>output</code> <code>Tensor | None</code> <p>Output tensor (None for pre_forward hooks)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>Modified input tensor (for pre_forward) or modified output tensor (for forward).</p> <code>Tensor | None</code> <p>Return None to keep original tensor unchanged.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Subclasses may raise exceptions for invalid inputs or modification errors</p> Source code in <code>src/mi_crow/hooks/controller.py</code> <pre><code>@abc.abstractmethod\ndef modify_activations(\n        self,\n        module: nn.Module,\n        inputs: torch.Tensor | None,\n        output: torch.Tensor | None\n) -&gt; torch.Tensor | None:\n    \"\"\"\n    Modify activations from the hooked layer.\n\n    For pre_forward hooks: receives input tensor, should return modified input tensor.\n    For forward hooks: receives input and output tensors, should return modified output tensor.\n\n    Args:\n        module: The PyTorch module being hooked\n        inputs: Input tensor (None for forward hooks if not available)\n        output: Output tensor (None for pre_forward hooks)\n\n    Returns:\n        Modified input tensor (for pre_forward) or modified output tensor (for forward).\n        Return None to keep original tensor unchanged.\n\n    Raises:\n        Exception: Subclasses may raise exceptions for invalid inputs or modification errors\n    \"\"\"\n    raise NotImplementedError(\"modify_activations must be implemented by subclasses\")\n</code></pre>"},{"location":"api/hooks/#implementations","title":"Implementations","text":""},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector","title":"mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector","text":"<pre><code>LayerActivationDetector(layer_signature, hook_id=None, target_dtype=None)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>Detector hook that captures and saves activations during inference.</p> <p>This detector extracts activations from layer outputs and stores them for later use (e.g., saving to disk, further analysis).</p> <p>Initialize the activation saver detector.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer to capture activations from</p> required <code>hook_id</code> <code>str | None</code> <p>Unique identifier for this hook</p> <code>None</code> <code>target_dtype</code> <code>dtype | None</code> <p>Optional dtype to convert activations to before storing</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If layer_signature is None</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def __init__(self, layer_signature: str | int, hook_id: str | None = None, target_dtype: torch.dtype | None = None):\n    \"\"\"\n    Initialize the activation saver detector.\n\n    Args:\n        layer_signature: Layer to capture activations from\n        hook_id: Unique identifier for this hook\n        target_dtype: Optional dtype to convert activations to before storing\n\n    Raises:\n        ValueError: If layer_signature is None\n    \"\"\"\n    if layer_signature is None:\n        raise ValueError(\"layer_signature cannot be None for LayerActivationDetector\")\n\n    super().__init__(hook_type=HookType.FORWARD, hook_id=hook_id, store=None, layer_signature=layer_signature)\n    self.target_dtype = target_dtype\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector.clear_captured","title":"clear_captured","text":"<pre><code>clear_captured()\n</code></pre> <p>Clear captured activations for current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def clear_captured(self) -&gt; None:\n    \"\"\"Clear captured activations for current batch.\"\"\"\n    self.tensor_metadata.pop(\"activations\", None)\n    self.metadata.pop(\"activations_shape\", None)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector.get_captured","title":"get_captured","text":"<pre><code>get_captured()\n</code></pre> <p>Get the captured activations from the current batch.</p> <p>Returns:</p> Type Description <code>Tensor | None</code> <p>The captured activation tensor from the current batch or None if no activations captured yet</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def get_captured(self) -&gt; torch.Tensor | None:\n    \"\"\"\n    Get the captured activations from the current batch.\n\n    Returns:\n        The captured activation tensor from the current batch or None if no activations captured yet\n    \"\"\"\n    return self.tensor_metadata.get(\"activations\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Extract and store activations from output.</p> <p>Handles various output types: - Plain tensors - Tuples/lists of tensors (takes first tensor) - Objects with last_hidden_state attribute (e.g., HuggingFace outputs)</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def process_activations(\n    self, module: torch.nn.Module, input: HOOK_FUNCTION_INPUT, output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Extract and store activations from output.\n\n    Handles various output types:\n    - Plain tensors\n    - Tuples/lists of tensors (takes first tensor)\n    - Objects with last_hidden_state attribute (e.g., HuggingFace outputs)\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        tensor = extract_tensor_from_output(output)\n\n        if tensor is not None:\n            if tensor.is_cuda:\n                tensor_cpu = tensor.detach().to(\"cpu\", non_blocking=True)\n            else:\n                tensor_cpu = tensor.detach()\n\n            if self.target_dtype is not None:\n                tensor_cpu = tensor_cpu.to(self.target_dtype)\n\n            self.tensor_metadata[\"activations\"] = tensor_cpu\n            self.metadata[\"activations_shape\"] = tuple(tensor_cpu.shape)\n    except Exception as e:\n        layer_sig = str(self.layer_signature) if self.layer_signature is not None else \"unknown\"\n        raise RuntimeError(\n            f\"Error extracting activations in LayerActivationDetector {self.id} (layer={layer_sig}): {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector","title":"mi_crow.hooks.implementations.model_input_detector.ModelInputDetector","text":"<pre><code>ModelInputDetector(layer_signature=None, hook_id=None, save_input_ids=True, save_attention_mask=False, special_token_ids=None)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>Detector hook that captures and saves tokenized inputs from model forward pass.</p> <p>This detector is designed to be attached to the root model module and captures: - Tokenized inputs (input_ids) from the model's forward pass - Attention masks (optional) that exclude both padding and special tokens</p> <p>Uses PRE_FORWARD hook to capture inputs before they are processed. Useful for saving tokenized inputs for analysis or training.</p> <p>Initialize the model input detector.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Layer to capture from (typically the root model, can be None)</p> <code>None</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier for this hook</p> <code>None</code> <code>save_input_ids</code> <code>bool</code> <p>Whether to save input_ids tensor</p> <code>True</code> <code>save_attention_mask</code> <code>bool</code> <p>Whether to save attention_mask tensor (excludes padding and special tokens)</p> <code>False</code> <code>special_token_ids</code> <code>Optional[List[int] | Set[int]]</code> <p>Optional list/set of special token IDs. If None, will extract from LanguageModel context.</p> <code>None</code> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def __init__(\n    self,\n    layer_signature: str | int | None = None,\n    hook_id: str | None = None,\n    save_input_ids: bool = True,\n    save_attention_mask: bool = False,\n    special_token_ids: Optional[List[int] | Set[int]] = None,\n):\n    \"\"\"\n    Initialize the model input detector.\n\n    Args:\n        layer_signature: Layer to capture from (typically the root model, can be None)\n        hook_id: Unique identifier for this hook\n        save_input_ids: Whether to save input_ids tensor\n        save_attention_mask: Whether to save attention_mask tensor (excludes padding and special tokens)\n        special_token_ids: Optional list/set of special token IDs. If None, will extract from LanguageModel context.\n    \"\"\"\n    super().__init__(hook_type=HookType.PRE_FORWARD, hook_id=hook_id, store=None, layer_signature=layer_signature)\n    self.save_input_ids = save_input_ids\n    self.save_attention_mask = save_attention_mask\n    self.special_token_ids = set(special_token_ids) if special_token_ids is not None else None\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.clear_captured","title":"clear_captured","text":"<pre><code>clear_captured()\n</code></pre> <p>Clear all captured inputs for current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def clear_captured(self) -&gt; None:\n    \"\"\"Clear all captured inputs for current batch.\"\"\"\n    keys_to_remove = [\"input_ids\", \"attention_mask\"]\n    for key in keys_to_remove:\n        self.tensor_metadata.pop(key, None)\n        self.metadata.pop(f\"{key}_shape\", None)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.get_captured_attention_mask","title":"get_captured_attention_mask","text":"<pre><code>get_captured_attention_mask()\n</code></pre> <p>Get the captured attention_mask from the current batch (excludes padding and special tokens).</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def get_captured_attention_mask(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured attention_mask from the current batch (excludes padding and special tokens).\"\"\"\n    return self.tensor_metadata.get(\"attention_mask\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.get_captured_input_ids","title":"get_captured_input_ids","text":"<pre><code>get_captured_input_ids()\n</code></pre> <p>Get the captured input_ids from the current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def get_captured_input_ids(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured input_ids from the current batch.\"\"\"\n    return self.tensor_metadata.get(\"input_ids\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Extract and store tokenized inputs.</p> <p>Note: For HuggingFace models called with **kwargs, the input tuple may be empty. In such cases, use set_inputs_from_encodings() to manually set inputs from the encodings dictionary returned by lm.inference.execute_inference().</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked (typically the root model)</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors/dicts to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output from the module (None for PRE_FORWARD hooks)</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def process_activations(\n    self, module: torch.nn.Module, input: HOOK_FUNCTION_INPUT, output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Extract and store tokenized inputs.\n\n    Note: For HuggingFace models called with **kwargs, the input tuple may be empty.\n    In such cases, use set_inputs_from_encodings() to manually set inputs from\n    the encodings dictionary returned by lm.inference.execute_inference().\n\n    Args:\n        module: The PyTorch module being hooked (typically the root model)\n        input: Tuple of input tensors/dicts to the module\n        output: Output from the module (None for PRE_FORWARD hooks)\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        if self.save_input_ids:\n            input_ids = self._extract_input_ids(input)\n            if input_ids is not None:\n                if input_ids.is_cuda:\n                    input_ids_cpu = input_ids.detach().to(\"cpu\", non_blocking=True)\n                else:\n                    input_ids_cpu = input_ids.detach()\n                self.tensor_metadata[\"input_ids\"] = input_ids_cpu\n                self.metadata[\"input_ids_shape\"] = tuple(input_ids_cpu.shape)\n\n        if self.save_attention_mask:\n            input_ids = self._extract_input_ids(input)\n            if input_ids is not None:\n                original_attention_mask = self._extract_attention_mask(input)\n                combined_mask = self._create_combined_attention_mask(input_ids, original_attention_mask, module)\n                if combined_mask.is_cuda:\n                    combined_mask_cpu = combined_mask.detach().to(\"cpu\", non_blocking=True)\n                else:\n                    combined_mask_cpu = combined_mask.detach()\n                self.tensor_metadata[\"attention_mask\"] = combined_mask_cpu\n                self.metadata[\"attention_mask_shape\"] = tuple(combined_mask_cpu.shape)\n\n    except Exception as e:\n        layer_sig = str(self.layer_signature) if self.layer_signature is not None else \"unknown\"\n        raise RuntimeError(\n            f\"Error extracting inputs in ModelInputDetector {self.id} (layer={layer_sig}): {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.set_inputs_from_encodings","title":"set_inputs_from_encodings","text":"<pre><code>set_inputs_from_encodings(encodings, module=None)\n</code></pre> <p>Manually set inputs from encodings dictionary.</p> <p>This is useful when the model is called with keyword arguments, as PyTorch's pre_forward hook doesn't receive kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>encodings</code> <code>Dict[str, Tensor]</code> <p>Dictionary of encoded inputs (e.g., from lm.inference.execute_inference() or lm.tokenize())</p> required <code>module</code> <code>Optional[Module]</code> <p>Optional module for extracting special token IDs. If None, will use DummyModule.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def set_inputs_from_encodings(\n    self, encodings: Dict[str, torch.Tensor], module: Optional[torch.nn.Module] = None\n) -&gt; None:\n    \"\"\"\n    Manually set inputs from encodings dictionary.\n\n    This is useful when the model is called with keyword arguments,\n    as PyTorch's pre_forward hook doesn't receive kwargs.\n\n    Args:\n        encodings: Dictionary of encoded inputs (e.g., from lm.inference.execute_inference() or lm.tokenize())\n        module: Optional module for extracting special token IDs. If None, will use DummyModule.\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        if self.save_input_ids and \"input_ids\" in encodings:\n            input_ids = encodings[\"input_ids\"]\n            self.tensor_metadata[\"input_ids\"] = input_ids.detach().to(\"cpu\")\n            self.metadata[\"input_ids_shape\"] = tuple(input_ids.shape)\n\n        if self.save_attention_mask and \"input_ids\" in encodings:\n            input_ids = encodings[\"input_ids\"]\n            if module is None:\n\n                class DummyModule:\n                    pass\n\n                module = DummyModule()\n\n            original_attention_mask = encodings.get(\"attention_mask\")\n            combined_mask = self._create_combined_attention_mask(input_ids, original_attention_mask, module)\n            self.tensor_metadata[\"attention_mask\"] = combined_mask.detach().to(\"cpu\")\n            self.metadata[\"attention_mask_shape\"] = tuple(combined_mask.shape)\n    except Exception as e:\n        raise RuntimeError(f\"Error setting inputs from encodings in ModelInputDetector {self.id}: {e}\") from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector","title":"mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector","text":"<pre><code>ModelOutputDetector(layer_signature=None, hook_id=None, save_output_logits=True, save_output_hidden_state=False)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>Detector hook that captures and saves model outputs.</p> <p>This detector is designed to be attached to the root model module and captures: - Model outputs (logits) from the model's forward pass - Hidden states (optional) from the model's forward pass</p> <p>Uses FORWARD hook to capture outputs after they are computed. Useful for saving model outputs for analysis or training.</p> <p>Initialize the model output detector.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Layer to capture from (typically the root model, can be None)</p> <code>None</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier for this hook</p> <code>None</code> <code>save_output_logits</code> <code>bool</code> <p>Whether to save output logits (if available)</p> <code>True</code> <code>save_output_hidden_state</code> <code>bool</code> <p>Whether to save last_hidden_state (if available)</p> <code>False</code> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def __init__(\n    self,\n    layer_signature: str | int | None = None,\n    hook_id: str | None = None,\n    save_output_logits: bool = True,\n    save_output_hidden_state: bool = False,\n):\n    \"\"\"\n    Initialize the model output detector.\n\n    Args:\n        layer_signature: Layer to capture from (typically the root model, can be None)\n        hook_id: Unique identifier for this hook\n        save_output_logits: Whether to save output logits (if available)\n        save_output_hidden_state: Whether to save last_hidden_state (if available)\n    \"\"\"\n    super().__init__(hook_type=HookType.FORWARD, hook_id=hook_id, store=None, layer_signature=layer_signature)\n    self.save_output_logits = save_output_logits\n    self.save_output_hidden_state = save_output_hidden_state\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.clear_captured","title":"clear_captured","text":"<pre><code>clear_captured()\n</code></pre> <p>Clear all captured outputs for current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def clear_captured(self) -&gt; None:\n    \"\"\"Clear all captured outputs for current batch.\"\"\"\n    keys_to_remove = [\"output_logits\", \"output_hidden_state\"]\n    for key in keys_to_remove:\n        self.tensor_metadata.pop(key, None)\n        self.metadata.pop(f\"{key}_shape\", None)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.get_captured_output_hidden_state","title":"get_captured_output_hidden_state","text":"<pre><code>get_captured_output_hidden_state()\n</code></pre> <p>Get the captured output hidden state from the current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def get_captured_output_hidden_state(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured output hidden state from the current batch.\"\"\"\n    return self.tensor_metadata.get(\"output_hidden_state\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.get_captured_output_logits","title":"get_captured_output_logits","text":"<pre><code>get_captured_output_logits()\n</code></pre> <p>Get the captured output logits from the current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def get_captured_output_logits(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured output logits from the current batch.\"\"\"\n    return self.tensor_metadata.get(\"output_logits\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Extract and store model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked (typically the root model)</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors/dicts to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output from the module</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def process_activations(\n    self, module: torch.nn.Module, input: HOOK_FUNCTION_INPUT, output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Extract and store model outputs.\n\n    Args:\n        module: The PyTorch module being hooked (typically the root model)\n        input: Tuple of input tensors/dicts to the module\n        output: Output from the module\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        # Extract and save outputs\n        logits, hidden_state = self._extract_output_tensor(output)\n\n        if self.save_output_logits and logits is not None:\n            if logits.is_cuda:\n                logits_cpu = logits.detach().to(\"cpu\", non_blocking=True)\n            else:\n                logits_cpu = logits.detach()\n            self.tensor_metadata[\"output_logits\"] = logits_cpu\n            self.metadata[\"output_logits_shape\"] = tuple(logits_cpu.shape)\n\n        if self.save_output_hidden_state and hidden_state is not None:\n            if hidden_state.is_cuda:\n                hidden_state_cpu = hidden_state.detach().to(\"cpu\", non_blocking=True)\n            else:\n                hidden_state_cpu = hidden_state.detach()\n            self.tensor_metadata[\"output_hidden_state\"] = hidden_state_cpu\n            self.metadata[\"output_hidden_state_shape\"] = tuple(hidden_state_cpu.shape)\n\n    except Exception as e:\n        layer_sig = str(self.layer_signature) if self.layer_signature is not None else \"unknown\"\n        raise RuntimeError(\n            f\"Error extracting outputs in ModelOutputDetector {self.id} (layer={layer_sig}): {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.function_controller.FunctionController","title":"mi_crow.hooks.implementations.function_controller.FunctionController","text":"<pre><code>FunctionController(layer_signature, function, hook_type=HookType.FORWARD, hook_id=None)\n</code></pre> <p>               Bases: <code>Controller</code></p> <p>A controller that applies a user-provided function to tensors during inference.</p> <p>This controller allows users to pass any function and apply it to activations. The function will be applied to: - Single tensors directly - All tensors in tuples/lists (default behavior)</p> Example <p>Initialize a function controller.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer to attach to</p> required <code>function</code> <code>Callable[[Tensor], Tensor]</code> <p>Function to apply to tensors. Must take a torch.Tensor and return a torch.Tensor</p> required <code>hook_type</code> <code>HookType | str</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If function is None or not callable</p> Source code in <code>src/mi_crow/hooks/implementations/function_controller.py</code> <pre><code>def __init__(\n    self,\n    layer_signature: str | int,\n    function: Callable[[torch.Tensor], torch.Tensor],\n    hook_type: HookType | str = HookType.FORWARD,\n    hook_id: str | None = None,\n):\n    \"\"\"\n    Initialize a function controller.\n\n    Args:\n        layer_signature: Layer to attach to\n        function: Function to apply to tensors. Must take a torch.Tensor and return a torch.Tensor\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)\n        hook_id: Unique identifier\n\n    Raises:\n        ValueError: If function is None or not callable\n    \"\"\"\n    if function is None:\n        raise ValueError(\"function cannot be None\")\n\n    if not callable(function):\n        raise ValueError(f\"function must be callable, got: {type(function)}\")\n\n    super().__init__(hook_type=hook_type, hook_id=hook_id, layer_signature=layer_signature)\n    self.function = function\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.function_controller.FunctionController--scale-activations-by-2","title":"Scale activations by 2","text":"<p>controller = FunctionController( ...     layer_signature=\"layer_0\", ...     function=lambda x: x * 2.0 ... )</p>"},{"location":"api/hooks/#mi_crow.hooks.implementations.function_controller.FunctionController.modify_activations","title":"modify_activations","text":"<pre><code>modify_activations(module, inputs, output)\n</code></pre> <p>Apply the user-provided function to activations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>'nn.Module'</code> <p>The PyTorch module being hooked</p> required <code>inputs</code> <code>Tensor | None</code> <p>Input tensor (None for forward hooks)</p> required <code>output</code> <code>Tensor | None</code> <p>Output tensor (None for pre_forward hooks)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>Modified tensor with function applied, or None if target tensor is None</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If function raises an exception when applied to tensor</p> Source code in <code>src/mi_crow/hooks/implementations/function_controller.py</code> <pre><code>def modify_activations(\n    self,\n    module: \"nn.Module\",\n    inputs: torch.Tensor | None,\n    output: torch.Tensor | None\n) -&gt; torch.Tensor | None:\n    \"\"\"\n    Apply the user-provided function to activations.\n\n    Args:\n        module: The PyTorch module being hooked\n        inputs: Input tensor (None for forward hooks)\n        output: Output tensor (None for pre_forward hooks)\n\n    Returns:\n        Modified tensor with function applied, or None if target tensor is None\n\n    Raises:\n        RuntimeError: If function raises an exception when applied to tensor\n    \"\"\"\n    target = output if self.hook_type == HookType.FORWARD else inputs\n\n    if target is None or not isinstance(target, torch.Tensor):\n        return target\n\n    try:\n        result = self.function(target)\n        if not isinstance(result, torch.Tensor):\n            raise TypeError(\n                f\"Function must return a torch.Tensor, got: {type(result)}\"\n            )\n        return result\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error applying function in FunctionController {self.id}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#utilities","title":"Utilities","text":""},{"location":"api/hooks/#mi_crow.hooks.utils","title":"mi_crow.hooks.utils","text":"<p>Utility functions for hook implementations.</p>"},{"location":"api/hooks/#mi_crow.hooks.utils.apply_modification_to_output","title":"apply_modification_to_output","text":"<pre><code>apply_modification_to_output(output, modified_tensor, target_device=None)\n</code></pre> <p>Apply a modified tensor to an output object in-place.</p> <p>Handles various output formats: - Plain tensors: modifies the tensor directly (in-place) - Tuples/lists of tensors: replaces first tensor - Objects with last_hidden_state attribute: sets last_hidden_state</p> <p>If target_device is provided, output tensors are moved to target_device first, ensuring consistency with the desired device (e.g., context.device). Otherwise, modified_tensor is moved to match output's current device.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output object to modify</p> required <code>modified_tensor</code> <code>Tensor</code> <p>Modified tensor to apply</p> required <code>target_device</code> <code>device | None</code> <p>Optional target device. If provided, output tensors are moved to this device before applying modification. If None, uses output's current device.</p> <code>None</code> Source code in <code>src/mi_crow/hooks/utils.py</code> <pre><code>def apply_modification_to_output(\n    output: HOOK_FUNCTION_OUTPUT,\n    modified_tensor: torch.Tensor,\n    target_device: torch.device | None = None\n) -&gt; None:\n    \"\"\"\n    Apply a modified tensor to an output object in-place.\n\n    Handles various output formats:\n    - Plain tensors: modifies the tensor directly (in-place)\n    - Tuples/lists of tensors: replaces first tensor\n    - Objects with last_hidden_state attribute: sets last_hidden_state\n\n    If target_device is provided, output tensors are moved to target_device first,\n    ensuring consistency with the desired device (e.g., context.device).\n    Otherwise, modified_tensor is moved to match output's current device.\n\n    Args:\n        output: Output object to modify\n        modified_tensor: Modified tensor to apply\n        target_device: Optional target device. If provided, output tensors are moved\n            to this device before applying modification. If None, uses output's current device.\n    \"\"\"\n    if output is None:\n        return\n\n    if isinstance(output, torch.Tensor):\n        if target_device is not None:\n            if output.device != target_device:\n                output = output.to(target_device)\n            if modified_tensor.device != target_device:\n                modified_tensor = modified_tensor.to(target_device)\n        else:\n            if modified_tensor.device != output.device:\n                modified_tensor = modified_tensor.to(output.device)\n        output.data.copy_(modified_tensor.data)\n        return\n\n    if isinstance(output, (tuple, list)):\n        for i, item in enumerate(output):\n            if isinstance(item, torch.Tensor):\n                if target_device is not None:\n                    if item.device != target_device:\n                        item = item.to(target_device)\n                        if isinstance(output, list):\n                            output[i] = item\n                    if modified_tensor.device != target_device or modified_tensor.dtype != item.dtype:\n                        modified_tensor = modified_tensor.to(device=target_device, dtype=item.dtype)\n                else:\n                    if modified_tensor.device != item.device or modified_tensor.dtype != item.dtype:\n                        modified_tensor = modified_tensor.to(device=item.device, dtype=item.dtype)\n                if isinstance(output, tuple):\n                    item.data.copy_(modified_tensor.data)\n                else:\n                    output[i] = modified_tensor\n                break\n        return\n\n    if hasattr(output, \"last_hidden_state\"):\n        original_tensor = output.last_hidden_state\n        if isinstance(original_tensor, torch.Tensor):\n            if target_device is not None:\n                if original_tensor.device != target_device:\n                    output.last_hidden_state = original_tensor.to(target_device)\n                    original_tensor = output.last_hidden_state\n                if modified_tensor.device != target_device:\n                    modified_tensor = modified_tensor.to(target_device)\n            else:\n                if modified_tensor.device != original_tensor.device:\n                    modified_tensor = modified_tensor.to(original_tensor.device)\n        output.last_hidden_state = modified_tensor\n        return\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.utils.extract_tensor_from_input","title":"extract_tensor_from_input","text":"<pre><code>extract_tensor_from_input(input)\n</code></pre> <p>Extract the first tensor from input sequence.</p> <p>Handles various input formats: - Direct tensor in first position - Tuple/list of tensors in first position - Empty or None inputs</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Input sequence (tuple/list of tensors)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>First tensor found, or None if no tensor found</p> Source code in <code>src/mi_crow/hooks/utils.py</code> <pre><code>def extract_tensor_from_input(input: HOOK_FUNCTION_INPUT) -&gt; torch.Tensor | None:\n    \"\"\"\n    Extract the first tensor from input sequence.\n\n    Handles various input formats:\n    - Direct tensor in first position\n    - Tuple/list of tensors in first position\n    - Empty or None inputs\n\n    Args:\n        input: Input sequence (tuple/list of tensors)\n\n    Returns:\n        First tensor found, or None if no tensor found\n    \"\"\"\n    if not input or len(input) == 0:\n        return None\n\n    first_item = input[0]\n    if isinstance(first_item, torch.Tensor):\n        return first_item\n\n    if isinstance(first_item, (tuple, list)):\n        for item in first_item:\n            if isinstance(item, torch.Tensor):\n                return item\n\n    return None\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.utils.extract_tensor_from_output","title":"extract_tensor_from_output","text":"<pre><code>extract_tensor_from_output(output)\n</code></pre> <p>Extract tensor from output (handles various output types).</p> <p>Handles various output formats: - Plain tensors - Tuples/lists of tensors (takes first tensor) - Objects with last_hidden_state attribute (e.g., HuggingFace outputs) - None outputs</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output from module (tensor, tuple, or object with attributes)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>First tensor found, or None if no tensor found</p> Source code in <code>src/mi_crow/hooks/utils.py</code> <pre><code>def extract_tensor_from_output(output: HOOK_FUNCTION_OUTPUT) -&gt; torch.Tensor | None:\n    \"\"\"\n    Extract tensor from output (handles various output types).\n\n    Handles various output formats:\n    - Plain tensors\n    - Tuples/lists of tensors (takes first tensor)\n    - Objects with last_hidden_state attribute (e.g., HuggingFace outputs)\n    - None outputs\n\n    Args:\n        output: Output from module (tensor, tuple, or object with attributes)\n\n    Returns:\n        First tensor found, or None if no tensor found\n    \"\"\"\n    if output is None:\n        return None\n\n    if isinstance(output, torch.Tensor):\n        return output\n\n    if isinstance(output, (tuple, list)):\n        for item in output:\n            if isinstance(item, torch.Tensor):\n                return item\n\n    # Try common HuggingFace output objects\n    if hasattr(output, \"last_hidden_state\"):\n        maybe = getattr(output, \"last_hidden_state\")\n        if isinstance(maybe, torch.Tensor):\n            return maybe\n\n    return None\n</code></pre>"},{"location":"api/language_model/","title":"Language Model API","text":"<p>Core language model functionality for loading models, running inference, and managing activations.</p>"},{"location":"api/language_model/#main-classes","title":"Main Classes","text":""},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel","title":"mi_crow.language_model.language_model.LanguageModel","text":"<pre><code>LanguageModel(model, tokenizer, store, model_id=None, device=None)\n</code></pre> <p>Fence-style language model wrapper.</p> <p>Provides a unified interface for working with language models, including: - Model initialization and configuration - Inference operations through the inference property - Hook management (detectors and controllers) - Model persistence - Activation tracking</p> <p>Initialize LanguageModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model module</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>HuggingFace tokenizer</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>model_id</code> <code>str | None</code> <p>Optional model identifier (auto-extracted if not provided)</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>Optional device string or torch.device (defaults to 'cpu' if None)</p> <code>None</code> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def __init__(\n        self,\n        model: nn.Module,\n        tokenizer: PreTrainedTokenizerBase,\n        store: Store,\n        model_id: str | None = None,\n        device: str | torch.device | None = None,\n):\n    \"\"\"\n    Initialize LanguageModel.\n\n    Args:\n        model: PyTorch model module\n        tokenizer: HuggingFace tokenizer\n        store: Store instance for persistence\n        model_id: Optional model identifier (auto-extracted if not provided)\n        device: Optional device string or torch.device (defaults to 'cpu' if None)\n    \"\"\"\n    self.context = LanguageModelContext(self)\n    self.context.model = model\n    self.context.tokenizer = tokenizer\n    self.context.model_id = initialize_model_id(model, model_id)\n    self.context.store = store\n    self.context.special_token_ids = _extract_special_token_ids(tokenizer)\n    self.context.device = normalize_device(device)\n    sync_model_to_context_device(self)\n\n    self.layers = LanguageModelLayers(self.context)\n    self.lm_tokenizer = LanguageModelTokenizer(self.context)\n    self.activations = LanguageModelActivations(self.context)\n    self.inference = InferenceEngine(self)\n\n    self._input_tracker: \"InputTracker | None\" = None\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Get the underlying PyTorch model.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.model_id","title":"model_id  <code>property</code>","text":"<pre><code>model_id\n</code></pre> <p>Get the model identifier.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.store","title":"store  <code>property</code> <code>writable</code>","text":"<pre><code>store\n</code></pre> <p>Get the store instance.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer\n</code></pre> <p>Get the tokenizer.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.clear_detectors","title":"clear_detectors","text":"<pre><code>clear_detectors()\n</code></pre> <p>Clear all accumulated metadata for registered detectors.</p> <p>This is useful when running multiple independent inference runs (e.g. separate <code>infer_texts</code> / <code>infer_dataset</code> calls) and you want to ensure that detector state does not leak between runs.</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def clear_detectors(self) -&gt; None:\n    \"\"\"\n    Clear all accumulated metadata for registered detectors.\n\n    This is useful when running multiple independent inference runs\n    (e.g. separate `infer_texts` / `infer_dataset` calls) and you want\n    to ensure that detector state does not leak between runs.\n    \"\"\"\n    detectors = self.layers.get_detectors()\n    for detector in detectors:\n        detector.metadata.clear()\n        detector.tensor_metadata.clear()\n\n        clear_captured = getattr(detector, \"clear_captured\", None)\n        if callable(clear_captured):\n            clear_captured()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(model_name, store, tokenizer_params=None, model_params=None, device=None)\n</code></pre> <p>Load a language model from HuggingFace Hub.</p> <p>Automatically loads model to GPU if device is \"cuda\" and CUDA is available. This prevents OOM errors by keeping the model on GPU instead of CPU RAM.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>HuggingFace model identifier</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>tokenizer_params</code> <code>dict</code> <p>Optional tokenizer parameters</p> <code>None</code> <code>model_params</code> <code>dict</code> <p>Optional model parameters</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>Target device (\"cuda\", \"cpu\", \"mps\"). If \"cuda\" and CUDA is available, model will be loaded directly to GPU using device_map=\"auto\" (via the HuggingFace factory helpers).</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>@classmethod\ndef from_huggingface(\n        cls,\n        model_name: str,\n        store: Store,\n        tokenizer_params: dict = None,\n        model_params: dict = None,\n        device: str | torch.device | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from HuggingFace Hub.\n\n    Automatically loads model to GPU if device is \"cuda\" and CUDA is available.\n    This prevents OOM errors by keeping the model on GPU instead of CPU RAM.\n\n    Args:\n        model_name: HuggingFace model identifier\n        store: Store instance for persistence\n        tokenizer_params: Optional tokenizer parameters\n        model_params: Optional model parameters\n        device: Target device (\"cuda\", \"cpu\", \"mps\"). If \"cuda\" and CUDA is available,\n            model will be loaded directly to GPU using device_map=\"auto\"\n            (via the HuggingFace factory helpers).\n\n    Returns:\n        LanguageModel instance\n    \"\"\"\n    return create_from_huggingface(cls, model_name, store, tokenizer_params, model_params, device)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(saved_path, store, model_id=None, device=None)\n</code></pre> <p>Load a language model from a saved file (created by save_model).</p> <p>Parameters:</p> Name Type Description Default <code>saved_path</code> <code>Path | str</code> <p>Path to the saved model file (.pt file)</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>model_id</code> <code>str | None</code> <p>Optional model identifier. If not provided, will use the model_id from saved metadata.      If provided, will be used to load the model architecture from HuggingFace.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>Optional device string or torch.device (defaults to 'cpu' if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the saved file doesn't exist</p> <code>ValueError</code> <p>If the saved file format is invalid or model_id is required but not provided</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>@classmethod\ndef from_local(\n        cls,\n        saved_path: Path | str,\n        store: Store,\n        model_id: str | None = None,\n        device: str | torch.device | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from a saved file (created by save_model).\n\n    Args:\n        saved_path: Path to the saved model file (.pt file)\n        store: Store instance for persistence\n        model_id: Optional model identifier. If not provided, will use the model_id from saved metadata.\n                 If provided, will be used to load the model architecture from HuggingFace.\n        device: Optional device string or torch.device (defaults to 'cpu' if None)\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        FileNotFoundError: If the saved file doesn't exist\n        ValueError: If the saved file format is invalid or model_id is required but not provided\n    \"\"\"\n    return load_model_from_saved_file(cls, saved_path, store, model_id, device)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.from_local_torch","title":"from_local_torch  <code>classmethod</code>","text":"<pre><code>from_local_torch(model_path, tokenizer_path, store, device=None)\n</code></pre> <p>Load a language model from local HuggingFace paths.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model directory or file</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to the tokenizer directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>device</code> <code>str | device | None</code> <p>Optional device string or torch.device (defaults to 'cpu' if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>@classmethod\ndef from_local_torch(\n        cls,\n        model_path: str,\n        tokenizer_path: str,\n        store: Store,\n        device: str | torch.device | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from local HuggingFace paths.\n\n    Args:\n        model_path: Path to the model directory or file\n        tokenizer_path: Path to the tokenizer directory or file\n        store: Store instance for persistence\n        device: Optional device string or torch.device (defaults to 'cpu' if None)\n\n    Returns:\n        LanguageModel instance\n    \"\"\"\n    return create_from_local_torch(cls, model_path, tokenizer_path, store, device)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.get_all_detector_metadata","title":"get_all_detector_metadata","text":"<pre><code>get_all_detector_metadata()\n</code></pre> <p>Get metadata from all registered detectors.</p> <p>Returns:</p> Type Description <code>tuple[dict[str, dict[str, Any]], dict[str, dict[str, Tensor]]]</code> <p>Tuple of (detectors_metadata, detectors_tensor_metadata)</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def get_all_detector_metadata(self) -&gt; tuple[dict[str, dict[str, Any]], dict[str, dict[str, Tensor]]]:\n    \"\"\"\n    Get metadata from all registered detectors.\n\n    Returns:\n        Tuple of (detectors_metadata, detectors_tensor_metadata)\n    \"\"\"\n    detectors = self.layers.get_detectors()\n    detectors_metadata: Dict[str, Dict[str, Any]] = defaultdict(dict)\n    detectors_tensor_metadata: Dict[str, Dict[str, torch.Tensor]] = defaultdict(dict)\n\n    for detector in detectors:\n        detectors_metadata[detector.layer_signature] = dict(detector.metadata)\n        detectors_tensor_metadata[detector.layer_signature] = dict(detector.tensor_metadata)\n\n    return detectors_metadata, detectors_tensor_metadata\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.get_input_tracker","title":"get_input_tracker","text":"<pre><code>get_input_tracker()\n</code></pre> <p>Get the input tracker instance if it exists.</p> <p>Returns:</p> Type Description <code>'InputTracker | None'</code> <p>InputTracker instance or None</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def get_input_tracker(self) -&gt; \"InputTracker | None\":\n    \"\"\"\n    Get the input tracker instance if it exists.\n\n    Returns:\n        InputTracker instance or None\n    \"\"\"\n    return self._input_tracker\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.save_detector_metadata","title":"save_detector_metadata","text":"<pre><code>save_detector_metadata(run_name, batch_idx, unified=False, clear_after_save=True)\n</code></pre> <p>Save detector metadata to store.</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>Name of the run</p> required <code>batch_idx</code> <code>int | None</code> <p>Batch index. Ignored when <code>unified</code> is True.</p> required <code>unified</code> <code>bool</code> <p>If True, save metadata in a single detectors directory for the whole run instead of per\u2011batch directories.</p> <code>False</code> <code>clear_after_save</code> <code>bool</code> <p>If True, clear detector metadata after saving to free memory. Defaults to True to prevent OOM errors when processing large batches.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path where metadata was saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is not set</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def save_detector_metadata(self, run_name: str, batch_idx: int | None, unified: bool = False, clear_after_save: bool = True) -&gt; str:\n    \"\"\"\n    Save detector metadata to store.\n\n    Args:\n        run_name: Name of the run\n        batch_idx: Batch index. Ignored when ``unified`` is True.\n        unified: If True, save metadata in a single detectors directory\n            for the whole run instead of per\u2011batch directories.\n        clear_after_save: If True, clear detector metadata after saving to free memory.\n            Defaults to True to prevent OOM errors when processing large batches.\n\n    Returns:\n        Path where metadata was saved\n\n    Raises:\n        ValueError: If store is not set\n    \"\"\"\n    if self.store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n    detectors_metadata, detectors_tensor_metadata = self.get_all_detector_metadata()\n\n    if unified:\n        result = self.store.put_run_detector_metadata(run_name, detectors_metadata, detectors_tensor_metadata)\n    else:\n        if batch_idx is None:\n            raise ValueError(\"batch_idx must be provided when unified is False\")\n        result = self.store.put_detector_metadata(run_name, batch_idx, detectors_metadata, detectors_tensor_metadata)\n\n    if clear_after_save:\n        for layer_signature in list(detectors_tensor_metadata.keys()):\n            detector_tensors = detectors_tensor_metadata[layer_signature]\n            for tensor_key in list(detector_tensors.keys()):\n                del detector_tensors[tensor_key]\n            del detectors_tensor_metadata[layer_signature]\n        detectors_metadata.clear()\n\n        detectors = self.layers.get_detectors()\n        for detector in detectors:\n            clear_captured = getattr(detector, \"clear_captured\", None)\n            if callable(clear_captured):\n                clear_captured()\n            for key in list(detector.tensor_metadata.keys()):\n                del detector.tensor_metadata[key]\n            detector.metadata.clear()\n\n        gc.collect()\n\n    return result\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.save_model","title":"save_model","text":"<pre><code>save_model(path=None)\n</code></pre> <p>Save the model and its metadata to the store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str | None</code> <p>Optional path to save the model. If None, defaults to {model_id}/model.pt   relative to the store base path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path where the model was saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is not set</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def save_model(self, path: Path | str | None = None) -&gt; Path:\n    \"\"\"\n    Save the model and its metadata to the store.\n\n    Args:\n        path: Optional path to save the model. If None, defaults to {model_id}/model.pt\n              relative to the store base path.\n\n    Returns:\n        Path where the model was saved\n\n    Raises:\n        ValueError: If store is not set\n    \"\"\"\n    return save_model(self, path)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.tokenize","title":"tokenize","text":"<pre><code>tokenize(texts, **kwargs)\n</code></pre> <p>Tokenize texts using the language model tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of text strings to tokenize</p> required <code>**kwargs</code> <code>Any</code> <p>Additional tokenizer arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tokenized encodings</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def tokenize(self, texts: Sequence[str], **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Tokenize texts using the language model tokenizer.\n\n    Args:\n        texts: Sequence of text strings to tokenize\n        **kwargs: Additional tokenizer arguments\n\n    Returns:\n        Tokenized encodings\n    \"\"\"\n    return self.lm_tokenizer.tokenize(texts, **kwargs)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.context.LanguageModelContext","title":"mi_crow.language_model.context.LanguageModelContext  <code>dataclass</code>","text":"<pre><code>LanguageModelContext(language_model, model_id=None, tokenizer_params=None, model_params=None, device='cpu', dtype=None, model=None, tokenizer=None, store=None, special_token_ids=None, _hook_registry=dict(), _hook_id_map=dict())\n</code></pre> <p>Shared context for LanguageModel and its components.</p>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers","title":"mi_crow.language_model.layers.LanguageModelLayers","text":"<pre><code>LanguageModelLayers(context)\n</code></pre> <p>Manages layer access and hook registration for LanguageModel.</p> <p>Initialize LanguageModelLayers.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>LanguageModelContext instance</p> required Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def __init__(\n        self,\n        context: \"LanguageModelContext\",\n):\n    \"\"\"\n    Initialize LanguageModelLayers.\n\n    Args:\n        context: LanguageModelContext instance\n    \"\"\"\n    self.context = context\n    self.name_to_layer: Dict[str, nn.Module] = {}\n    self.idx_to_layer: Dict[int, nn.Module] = {}\n    self._flatten_layer_names()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.disable_all_hooks","title":"disable_all_hooks","text":"<pre><code>disable_all_hooks()\n</code></pre> <p>Disable all registered hooks.</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def disable_all_hooks(self) -&gt; None:\n    \"\"\"Disable all registered hooks.\"\"\"\n    for _, _, hook in self.context._hook_id_map.values():\n        hook.disable()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.disable_hook","title":"disable_hook","text":"<pre><code>disable_hook(hook_id)\n</code></pre> <p>Disable a specific hook by ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook_id</code> <code>str</code> <p>Hook ID to disable</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hook was found and disabled, False otherwise</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def disable_hook(self, hook_id: str) -&gt; bool:\n    \"\"\"\n    Disable a specific hook by ID.\n\n    Args:\n        hook_id: Hook ID to disable\n\n    Returns:\n        True if hook was found and disabled, False otherwise\n    \"\"\"\n    if hook_id in self.context._hook_id_map:\n        _, _, hook = self.context._hook_id_map[hook_id]\n        hook.disable()\n        return True\n    return False\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.enable_all_hooks","title":"enable_all_hooks","text":"<pre><code>enable_all_hooks()\n</code></pre> <p>Enable all registered hooks.</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def enable_all_hooks(self) -&gt; None:\n    \"\"\"Enable all registered hooks.\"\"\"\n    for _, _, hook in self.context._hook_id_map.values():\n        hook.enable()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.enable_hook","title":"enable_hook","text":"<pre><code>enable_hook(hook_id)\n</code></pre> <p>Enable a specific hook by ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook_id</code> <code>str</code> <p>Hook ID to enable</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hook was found and enabled, False otherwise</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def enable_hook(self, hook_id: str) -&gt; bool:\n    \"\"\"\n    Enable a specific hook by ID.\n\n    Args:\n        hook_id: Hook ID to enable\n\n    Returns:\n        True if hook was found and enabled, False otherwise\n    \"\"\"\n    if hook_id in self.context._hook_id_map:\n        _, _, hook = self.context._hook_id_map[hook_id]\n        hook.enable()\n        return True\n    return False\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_controllers","title":"get_controllers","text":"<pre><code>get_controllers()\n</code></pre> <p>Get all registered Controller hooks.</p> <p>Returns:</p> Type Description <code>List[Controller]</code> <p>List of Controller instances</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_controllers(self) -&gt; List[Controller]:\n    \"\"\"\n    Get all registered Controller hooks.\n\n    Returns:\n        List of Controller instances\n    \"\"\"\n    return [hook for hook in self.get_hooks() if isinstance(hook, Controller)]\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_detectors","title":"get_detectors","text":"<pre><code>get_detectors()\n</code></pre> <p>Get all registered Detector hooks.</p> <p>Returns:</p> Type Description <code>List[Detector]</code> <p>List of Detector instances</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_detectors(self) -&gt; List[Detector]:\n    \"\"\"\n    Get all registered Detector hooks.\n\n    Returns:\n        List of Detector instances\n    \"\"\"\n    return [hook for hook in self.get_hooks() if isinstance(hook, Detector)]\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_hooks","title":"get_hooks","text":"<pre><code>get_hooks(layer_signature=None, hook_type=None)\n</code></pre> <p>Get registered hooks, optionally filtered by layer and/or type.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Optional layer to filter by</p> <code>None</code> <code>hook_type</code> <code>HookType | str | None</code> <p>Optional hook type to filter by (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Hook]</code> <p>List of Hook instances</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_hooks(\n        self,\n        layer_signature: str | int | None = None,\n        hook_type: HookType | str | None = None\n) -&gt; List[Hook]:\n    \"\"\"\n    Get registered hooks, optionally filtered by layer and/or type.\n\n    Args:\n        layer_signature: Optional layer to filter by\n        hook_type: Optional hook type to filter by (HookType.FORWARD or HookType.PRE_FORWARD)\n\n    Returns:\n        List of Hook instances\n    \"\"\"\n    # Normalize hook_type if string\n    normalized_hook_type = None\n    if hook_type is not None:\n        if isinstance(hook_type, str):\n            normalized_hook_type = HookType(hook_type)\n        else:\n            normalized_hook_type = hook_type\n\n    return self._get_hooks_from_registry(layer_signature, normalized_hook_type)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_layer_names","title":"get_layer_names","text":"<pre><code>get_layer_names()\n</code></pre> <p>Get all layer names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of layer names</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_layer_names(self) -&gt; List[str]:\n    \"\"\"\n    Get all layer names.\n\n    Returns:\n        List of layer names\n    \"\"\"\n    return list(self.name_to_layer.keys())\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.print_layer_names","title":"print_layer_names","text":"<pre><code>print_layer_names()\n</code></pre> <p>Print layer names with basic info.</p> <p>Useful for debugging and exploring model structure.</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def print_layer_names(self) -&gt; None:\n    \"\"\"\n    Print layer names with basic info.\n\n    Useful for debugging and exploring model structure.\n    \"\"\"\n    names = self.get_layer_names()\n    for name in names:\n        layer = self.name_to_layer[name]\n        weight_shape = getattr(layer, 'weight', None)\n        weight_info = weight_shape.shape if weight_shape is not None else 'No weight'\n        print(f\"{name}: {weight_info}\")\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.register_forward_hook_for_layer","title":"register_forward_hook_for_layer","text":"<pre><code>register_forward_hook_for_layer(layer_signature, hook, hook_args=None)\n</code></pre> <p>Register a forward hook directly on a layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer name or index</p> required <code>hook</code> <code>Callable</code> <p>Hook callable</p> required <code>hook_args</code> <code>dict</code> <p>Optional arguments for register_forward_hook</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Hook handle</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def register_forward_hook_for_layer(\n        self,\n        layer_signature: str | int,\n        hook: Callable,\n        hook_args: dict = None\n) -&gt; Any:\n    \"\"\"\n    Register a forward hook directly on a layer.\n\n    Args:\n        layer_signature: Layer name or index\n        hook: Hook callable\n        hook_args: Optional arguments for register_forward_hook\n\n    Returns:\n        Hook handle\n    \"\"\"\n    layer = self._resolve_layer(layer_signature)\n    return layer.register_forward_hook(hook, **(hook_args or {}))\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.register_hook","title":"register_hook","text":"<pre><code>register_hook(layer_signature, hook, hook_type=None)\n</code></pre> <p>Register a hook on a layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer name or index</p> required <code>hook</code> <code>Hook</code> <p>Hook instance to register</p> required <code>hook_type</code> <code>HookType | str | None</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD).        If None, uses hook.hook_type</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The hook's ID</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hook ID is not unique or if mixing hook types on same layer</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def register_hook(\n        self,\n        layer_signature: str | int,\n        hook: Hook,\n        hook_type: HookType | str | None = None\n) -&gt; str:\n    \"\"\"\n    Register a hook on a layer.\n\n    Args:\n        layer_signature: Layer name or index\n        hook: Hook instance to register\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD). \n                  If None, uses hook.hook_type\n\n    Returns:\n        The hook's ID\n\n    Raises:\n        ValueError: If hook ID is not unique or if mixing hook types on same layer\n    \"\"\"\n    layer = self._resolve_layer(layer_signature)\n\n    if hook_type is None:\n        hook_type = self._get_hook_type_from_hook(hook)\n    elif isinstance(hook_type, str):\n        hook_type = HookType(hook_type)\n\n    self._validate_hook_registration(layer_signature, hook)\n\n    hook.layer_signature = layer_signature\n    hook.set_context(self.context)\n\n    if layer_signature not in self.context._hook_registry:\n        self.context._hook_registry[layer_signature] = {}\n\n    if hook_type not in self.context._hook_registry[layer_signature]:\n        self.context._hook_registry[layer_signature][hook_type] = []\n\n    torch_hook_fn = hook.get_torch_hook()\n\n    if hook_type == HookType.PRE_FORWARD:\n        handle = layer.register_forward_pre_hook(torch_hook_fn)\n    else:\n        handle = layer.register_forward_hook(torch_hook_fn)\n\n    self.context._hook_registry[layer_signature][hook_type].append((hook, handle))\n    self.context._hook_id_map[hook.id] = (layer_signature, hook_type, hook)\n\n    return hook.id\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.register_pre_forward_hook_for_layer","title":"register_pre_forward_hook_for_layer","text":"<pre><code>register_pre_forward_hook_for_layer(layer_signature, hook, hook_args=None)\n</code></pre> <p>Register a pre-forward hook directly on a layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer name or index</p> required <code>hook</code> <code>Callable</code> <p>Hook callable</p> required <code>hook_args</code> <code>dict</code> <p>Optional arguments for register_forward_pre_hook</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Hook handle</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def register_pre_forward_hook_for_layer(\n        self,\n        layer_signature: str | int,\n        hook: Callable,\n        hook_args: dict = None\n) -&gt; Any:\n    \"\"\"\n    Register a pre-forward hook directly on a layer.\n\n    Args:\n        layer_signature: Layer name or index\n        hook: Hook callable\n        hook_args: Optional arguments for register_forward_pre_hook\n\n    Returns:\n        Hook handle\n    \"\"\"\n    layer = self._resolve_layer(layer_signature)\n    return layer.register_forward_pre_hook(hook, **(hook_args or {}))\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.unregister_hook","title":"unregister_hook","text":"<pre><code>unregister_hook(hook_or_id)\n</code></pre> <p>Unregister a hook by Hook instance or ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook_or_id</code> <code>Hook | str</code> <p>Hook instance or hook ID string</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hook was found and removed, False otherwise</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def unregister_hook(self, hook_or_id: Hook | str) -&gt; bool:\n    \"\"\"\n    Unregister a hook by Hook instance or ID.\n\n    Args:\n        hook_or_id: Hook instance or hook ID string\n\n    Returns:\n        True if hook was found and removed, False otherwise\n    \"\"\"\n    # Get hook ID\n    if isinstance(hook_or_id, Hook):\n        hook_id = hook_or_id.id\n    else:\n        hook_id = hook_or_id\n\n    # Look up hook\n    if hook_id not in self.context._hook_id_map:\n        return False\n\n    layer_signature, hook_type, hook = self.context._hook_id_map[hook_id]\n\n    if layer_signature not in self.context._hook_registry:\n        del self.context._hook_id_map[hook_id]\n        return True\n\n    hook_types = self.context._hook_registry[layer_signature]\n    if hook_type not in hook_types:\n        del self.context._hook_id_map[hook_id]\n        return True\n\n    hooks_list = hook_types[hook_type]\n    for i, (h, handle) in enumerate(hooks_list):\n        if h.id == hook_id:\n            handle.remove()\n            hooks_list.pop(i)\n            break\n\n    del self.context._hook_id_map[hook_id]\n    return True\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.tokenizer.LanguageModelTokenizer","title":"mi_crow.language_model.tokenizer.LanguageModelTokenizer","text":"<pre><code>LanguageModelTokenizer(context)\n</code></pre> <p>Handles tokenization for LanguageModel.</p> <p>Initialize LanguageModelTokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>LanguageModelContext instance</p> required Source code in <code>src/mi_crow/language_model/tokenizer.py</code> <pre><code>def __init__(\n        self,\n        context: \"LanguageModelContext\"\n):\n    \"\"\"\n    Initialize LanguageModelTokenizer.\n\n    Args:\n        context: LanguageModelContext instance\n    \"\"\"\n    self.context = context\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.tokenizer.LanguageModelTokenizer.split_to_tokens","title":"split_to_tokens","text":"<pre><code>split_to_tokens(text, add_special_tokens=False)\n</code></pre> <p>Split text into token strings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, Sequence[str]]</code> <p>Single string or sequence of strings to tokenize</p> required <code>add_special_tokens</code> <code>bool</code> <p>Whether to add special tokens (e.g., BOS, EOS)</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>For a single string: list of token strings</p> <code>Union[List[str], List[List[str]]]</code> <p>For a sequence of strings: list of lists of token strings</p> Source code in <code>src/mi_crow/language_model/tokenizer.py</code> <pre><code>def split_to_tokens(\n        self,\n        text: Union[str, Sequence[str]],\n        add_special_tokens: bool = False\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"\n    Split text into token strings.\n\n    Args:\n        text: Single string or sequence of strings to tokenize\n        add_special_tokens: Whether to add special tokens (e.g., BOS, EOS)\n\n    Returns:\n        For a single string: list of token strings\n        For a sequence of strings: list of lists of token strings\n    \"\"\"\n    if isinstance(text, str):\n        return self._split_single_text_to_tokens(text, add_special_tokens)\n\n    return [self._split_single_text_to_tokens(t, add_special_tokens) for t in text]\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.tokenizer.LanguageModelTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(texts, padding=False, pad_token='[PAD]', **kwargs)\n</code></pre> <p>Robust batch tokenization that works across tokenizer variants.</p> <p>Tries methods in order: - callable tokenizer (most HF tokenizers) - batch_encode_plus - encode_plus per item + tokenizer.pad to collate</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of text strings to tokenize</p> required <code>padding</code> <code>bool</code> <p>Whether to pad sequences</p> <code>False</code> <code>pad_token</code> <code>str</code> <p>Pad token string</p> <code>'[PAD]'</code> <code>**kwargs</code> <code>Any</code> <p>Additional tokenizer arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tokenized encodings</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokenizer is not initialized</p> <code>TypeError</code> <p>If tokenizer is not usable for batch tokenization</p> Source code in <code>src/mi_crow/language_model/tokenizer.py</code> <pre><code>def tokenize(\n        self,\n        texts: Sequence[str],\n        padding: bool = False,\n        pad_token: str = \"[PAD]\",\n        **kwargs: Any\n) -&gt; Any:\n    \"\"\"\n    Robust batch tokenization that works across tokenizer variants.\n\n    Tries methods in order:\n    - callable tokenizer (most HF tokenizers)\n    - batch_encode_plus\n    - encode_plus per item + tokenizer.pad to collate\n\n    Args:\n        texts: Sequence of text strings to tokenize\n        padding: Whether to pad sequences\n        pad_token: Pad token string\n        **kwargs: Additional tokenizer arguments\n\n    Returns:\n        Tokenized encodings\n\n    Raises:\n        ValueError: If tokenizer is not initialized\n        TypeError: If tokenizer is not usable for batch tokenization\n    \"\"\"\n    tokenizer = self.context.tokenizer\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be initialized before tokenization\")\n\n    model = self.context.model\n\n    if padding and pad_token and getattr(tokenizer, \"pad_token\", None) is None:\n        self._setup_pad_token(tokenizer, model)\n\n    kwargs[\"padding\"] = padding\n\n    # Try callable tokenizer first (most common case)\n    if callable(tokenizer):\n        try:\n            return tokenizer(texts, **kwargs)\n        except (TypeError, NotImplementedError):\n            pass\n\n    # Try batch_encode_plus\n    if hasattr(tokenizer, \"batch_encode_plus\"):\n        return tokenizer.batch_encode_plus(texts, **kwargs)\n\n    # Fallback to encode_plus per item\n    if hasattr(tokenizer, \"encode_plus\"):\n        encoded = [tokenizer.encode_plus(t, **kwargs) for t in texts]\n        if hasattr(tokenizer, \"pad\"):\n            rt = kwargs.get(\"return_tensors\") or \"pt\"\n            return tokenizer.pad(encoded, return_tensors=rt)\n        return encoded\n\n    raise TypeError(\"Tokenizer object on LanguageModel is not usable for batch tokenization\")\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.activations.LanguageModelActivations","title":"mi_crow.language_model.activations.LanguageModelActivations","text":"<pre><code>LanguageModelActivations(context)\n</code></pre> <p>Handles activation saving and processing for LanguageModel.</p> <p>Initialize LanguageModelActivations.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>LanguageModelContext instance</p> required Source code in <code>src/mi_crow/language_model/activations.py</code> <pre><code>def __init__(self, context: \"LanguageModelContext\"):  # noqa: F821\n    \"\"\"\n    Initialize LanguageModelActivations.\n\n    Args:\n        context: LanguageModelContext instance\n    \"\"\"\n    self.context = context\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.activations.LanguageModelActivations.save_activations","title":"save_activations","text":"<pre><code>save_activations(texts, layer_signature, run_name=None, batch_size=None, *, dtype=None, max_length=None, autocast=True, autocast_dtype=None, free_cuda_cache_every=0, verbose=False, save_in_batches=True, save_attention_mask=False, stop_after_last_layer=True)\n</code></pre> <p>Save activations from a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of text strings to process</p> required <code>layer_signature</code> <code>str | int | list[str | int]</code> <p>Layer signature (or list of signatures) to capture activations from</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name (generated if None)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Optional batch size for processing (if None, processes all at once)</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to convert activations to</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional max length for tokenization</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use autocast</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>free_cuda_cache_every</code> <code>int | None</code> <p>Clear CUDA cache every N batches (0 or None to disable)</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>save_attention_mask</code> <code>bool</code> <p>Whether to also save attention masks (automatically attaches ModelInputDetector)</p> <code>False</code> <code>stop_after_last_layer</code> <code>bool</code> <p>Whether to stop model forward pass after the last requested layer to save memory and time. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Run name used for saving</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model or store is not initialized</p> Source code in <code>src/mi_crow/language_model/activations.py</code> <pre><code>def save_activations(\n    self,\n    texts: Sequence[str],\n    layer_signature: str | int | list[str | int],\n    run_name: str | None = None,\n    batch_size: int | None = None,\n    *,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    free_cuda_cache_every: int | None = 0,\n    verbose: bool = False,\n    save_in_batches: bool = True,\n    save_attention_mask: bool = False,\n    stop_after_last_layer: bool = True,\n) -&gt; str:\n    \"\"\"\n    Save activations from a list of texts.\n\n    Args:\n        texts: Sequence of text strings to process\n        layer_signature: Layer signature (or list of signatures) to capture activations from\n        run_name: Optional run name (generated if None)\n        batch_size: Optional batch size for processing (if None, processes all at once)\n        dtype: Optional dtype to convert activations to\n        max_length: Optional max length for tokenization\n        autocast: Whether to use autocast\n        autocast_dtype: Optional dtype for autocast\n        free_cuda_cache_every: Clear CUDA cache every N batches (0 or None to disable)\n        verbose: Whether to log progress\n        save_attention_mask: Whether to also save attention masks (automatically attaches ModelInputDetector)\n        stop_after_last_layer: Whether to stop model forward pass after the last requested layer\n            to save memory and time. Defaults to True.\n\n    Returns:\n        Run name used for saving\n\n    Raises:\n        ValueError: If model or store is not initialized\n    \"\"\"\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    model, store = self._validate_save_prerequisites()\n\n    device = torch.device(self.context.device)\n    device_type = str(device.type)\n\n    if batch_size is None:\n        batch_size = len(texts)\n\n    options = {\n        \"dtype\": str(dtype) if dtype is not None else None,\n        \"max_length\": max_length,\n        \"batch_size\": int(batch_size),\n        \"stop_after_last_layer\": stop_after_last_layer,\n    }\n\n    run_name, meta, layer_sig_list = self._prepare_save_metadata(layer_signature, None, run_name, options)\n\n    if verbose:\n        logger.info(\n            f\"Starting save_activations: run={run_name}, layers={layer_sig_list}, \"\n            f\"batch_size={batch_size}, device={device_type}\"\n        )\n\n    self._save_run_metadata(store, run_name, meta, verbose)\n\n    hook_ids, attention_mask_hook_id = self._setup_activation_hooks(\n        layer_sig_list, run_name, save_attention_mask, dtype=dtype\n    )\n\n    batch_counter = 0\n    # Stop after last hooked layer if requested\n    stop_after = layer_sig_list[-1] if (layer_sig_list and stop_after_last_layer) else None\n\n    try:\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch_texts = texts[i : i + batch_size]\n                batch_index = i // batch_size\n\n                self._process_batch(\n                    batch_texts,\n                    run_name,\n                    batch_index,\n                    max_length,\n                    autocast,\n                    autocast_dtype,\n                    dtype,\n                    verbose,\n                    save_in_batches=save_in_batches,\n                    stop_after_layer=stop_after,\n                )\n                batch_counter += 1\n                self._manage_cuda_cache(batch_counter, free_cuda_cache_every, device_type, verbose)\n    finally:\n        self._teardown_activation_hooks(hook_ids, attention_mask_hook_id)\n        if verbose:\n            logger.info(f\"Completed save_activations: run={run_name}, batches_saved={batch_counter}\")\n\n    return run_name\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.activations.LanguageModelActivations.save_activations_dataset","title":"save_activations_dataset","text":"<pre><code>save_activations_dataset(dataset, layer_signature, run_name=None, batch_size=32, *, dtype=None, max_length=None, autocast=True, autocast_dtype=None, free_cuda_cache_every=None, verbose=False, save_in_batches=True, save_attention_mask=False, stop_after_last_layer=True)\n</code></pre> <p>Save activations from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>BaseDataset</code> <p>Dataset to process</p> required <code>layer_signature</code> <code>str | int | list[str | int]</code> <p>Layer signature (or list of signatures) to capture activations from</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name (generated if None)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>32</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to convert activations to</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional max length for tokenization</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use autocast</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>free_cuda_cache_every</code> <code>int | None</code> <p>Clear CUDA cache every N batches (None to auto-detect, 0 to disable)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>save_attention_mask</code> <code>bool</code> <p>Whether to also save attention masks (automatically attaches ModelInputDetector)</p> <code>False</code> <code>stop_after_last_layer</code> <code>bool</code> <p>Whether to stop model forward pass after the last requested layer to save memory and time. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Run name used for saving</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model or store is not initialized</p> Source code in <code>src/mi_crow/language_model/activations.py</code> <pre><code>def save_activations_dataset(\n    self,\n    dataset: BaseDataset,\n    layer_signature: str | int | list[str | int],\n    run_name: str | None = None,\n    batch_size: int = 32,\n    *,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    free_cuda_cache_every: int | None = None,\n    verbose: bool = False,\n    save_in_batches: bool = True,\n    save_attention_mask: bool = False,\n    stop_after_last_layer: bool = True,\n) -&gt; str:\n    \"\"\"\n    Save activations from a dataset.\n\n    Args:\n        dataset: Dataset to process\n        layer_signature: Layer signature (or list of signatures) to capture activations from\n        run_name: Optional run name (generated if None)\n        batch_size: Batch size for processing\n        dtype: Optional dtype to convert activations to\n        max_length: Optional max length for tokenization\n        autocast: Whether to use autocast\n        autocast_dtype: Optional dtype for autocast\n        free_cuda_cache_every: Clear CUDA cache every N batches (None to auto-detect, 0 to disable)\n        verbose: Whether to log progress\n        save_attention_mask: Whether to also save attention masks (automatically attaches ModelInputDetector)\n        stop_after_last_layer: Whether to stop model forward pass after the last requested layer\n            to save memory and time. Defaults to True.\n\n    Returns:\n        Run name used for saving\n\n    Raises:\n        ValueError: If model or store is not initialized\n    \"\"\"\n    model, store = self._validate_save_prerequisites()\n\n    device = torch.device(self.context.device)\n    device_type = str(device.type)\n\n    if free_cuda_cache_every is None:\n        free_cuda_cache_every = 5 if device_type == \"cuda\" else 0\n\n    options = {\n        \"dtype\": str(dtype) if dtype is not None else None,\n        \"max_length\": max_length,\n        \"batch_size\": int(batch_size),\n        \"stop_after_last_layer\": stop_after_last_layer,\n    }\n\n    run_name, meta, layer_sig_list = self._prepare_save_metadata(layer_signature, dataset, run_name, options)\n\n    if verbose:\n        logger.info(\n            f\"Starting save_activations_dataset: run={run_name}, layers={layer_sig_list}, \"\n            f\"batch_size={batch_size}, device={device_type}\"\n        )\n\n    self._save_run_metadata(store, run_name, meta, verbose)\n\n    hook_ids, attention_mask_hook_id = self._setup_activation_hooks(\n        layer_sig_list, run_name, save_attention_mask, dtype=dtype\n    )\n\n    batch_counter = 0\n    # Stop after last hooked layer if requested\n    stop_after = layer_sig_list[-1] if (layer_sig_list and stop_after_last_layer) else None\n\n    try:\n        with torch.inference_mode():\n            for batch_index, batch in enumerate(dataset.iter_batches(batch_size)):\n                texts = dataset.extract_texts_from_batch(batch)\n                self._process_batch(\n                    texts,\n                    run_name,\n                    batch_index,\n                    max_length,\n                    autocast,\n                    autocast_dtype,\n                    dtype,\n                    verbose,\n                    save_in_batches=save_in_batches,\n                    stop_after_layer=stop_after,\n                )\n                batch_counter += 1\n\n                self._manage_cuda_cache(batch_counter, free_cuda_cache_every, device_type, verbose)\n    finally:\n        self._teardown_activation_hooks(hook_ids, attention_mask_hook_id)\n        if verbose:\n            logger.info(f\"Completed save_activations_dataset: run={run_name}, batches_saved={batch_counter}\")\n\n    return run_name\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine","title":"mi_crow.language_model.inference.InferenceEngine","text":"<pre><code>InferenceEngine(language_model)\n</code></pre> <p>Handles inference operations for LanguageModel.</p> <p>Initialize inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>'LanguageModel'</code> <p>LanguageModel instance</p> required Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def __init__(self, language_model: \"LanguageModel\"):\n    \"\"\"\n    Initialize inference engine.\n\n    Args:\n        language_model: LanguageModel instance\n    \"\"\"\n    self.lm = language_model\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.execute_inference","title":"execute_inference","text":"<pre><code>execute_inference(texts, tok_kwargs=None, autocast=True, autocast_dtype=None, with_controllers=True, stop_after_layer=None)\n</code></pre> <p>Execute inference on texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of input texts</p> required <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>stop_after_layer</code> <code>str | int | None</code> <p>Optional layer signature (name or index) after which the forward pass should be stopped early</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Any, Dict[str, Tensor]]</code> <p>Tuple of (model_output, encodings)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If texts is empty or tokenizer is not initialized</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def execute_inference(\n        self,\n        texts: Sequence[str],\n        tok_kwargs: Dict | None = None,\n        autocast: bool = True,\n        autocast_dtype: torch.dtype | None = None,\n        with_controllers: bool = True,\n        stop_after_layer: str | int | None = None,\n) -&gt; tuple[Any, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Execute inference on texts.\n\n    Args:\n        texts: Sequence of input texts\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        stop_after_layer: Optional layer signature (name or index) after which\n            the forward pass should be stopped early\n\n    Returns:\n        Tuple of (model_output, encodings)\n\n    Raises:\n        ValueError: If texts is empty or tokenizer is not initialized\n    \"\"\"\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    if self.lm.tokenizer is None:\n        raise ValueError(\"Tokenizer must be initialized before running inference\")\n\n    tok_kwargs = self._prepare_tokenizer_kwargs(tok_kwargs)\n    logger.debug(f\"[DEBUG] About to tokenize {len(texts)} texts...\")\n    enc = self.lm.tokenize(texts, **tok_kwargs)\n    logger.debug(f\"[DEBUG] Tokenization completed, shape: {enc['input_ids'].shape if isinstance(enc, dict) else 'N/A'}\")\n\n    device = torch.device(self.lm.context.device)\n    device_type = str(device.type)\n\n    sync_model_to_context_device(self.lm)\n\n    enc = move_tensors_to_device(enc, device)\n\n    self.lm.model.eval()\n\n    self._setup_trackers(texts)\n    self._setup_model_input_detectors(enc)\n\n    controllers_to_restore = self._prepare_controllers(with_controllers)\n\n    hook_handle = None\n    try:\n        if stop_after_layer is not None:\n            # Register a temporary forward hook that stops the forward pass\n            def _early_stop_hook(module: nn.Module, inputs: tuple, output: Any):\n                raise _EarlyStopInference(output)\n\n            hook_handle = self.lm.layers.register_forward_hook_for_layer(\n                stop_after_layer, _early_stop_hook\n            )\n\n        output = self._run_model_forward(enc, autocast, device_type, autocast_dtype)\n        return output, enc\n    finally:\n        if hook_handle is not None:\n            try:\n                hook_handle.remove()\n            except Exception:\n                pass\n        self._restore_controllers(controllers_to_restore)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.extract_logits","title":"extract_logits","text":"<pre><code>extract_logits(output)\n</code></pre> <p>Extract logits tensor from model output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>Model output</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Logits tensor</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def extract_logits(self, output: Any) -&gt; torch.Tensor:\n    \"\"\"\n    Extract logits tensor from model output.\n\n    Args:\n        output: Model output\n\n    Returns:\n        Logits tensor\n    \"\"\"\n    return extract_logits_from_output(output)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.infer_dataset","title":"infer_dataset","text":"<pre><code>infer_dataset(dataset, run_name=None, batch_size=32, tok_kwargs=None, autocast=True, autocast_dtype=None, with_controllers=True, free_cuda_cache_every=0, clear_detectors_before=False, verbose=False, stop_after_layer=None, save_in_batches=True)\n</code></pre> <p>Run inference on whole dataset with metadata saving.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'BaseDataset'</code> <p>Dataset to process</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name (generated if None)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>32</code> <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>free_cuda_cache_every</code> <code>int | None</code> <p>Clear CUDA cache every N batches (0 or None to disable)</p> <code>0</code> <code>clear_detectors_before</code> <code>bool</code> <p>If True, clears all detector state before running</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>stop_after_layer</code> <code>str | int | None</code> <p>Optional layer signature (name or index) after which the forward pass should be stopped early</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Run name used for saving</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model or store is not initialized</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def infer_dataset(\n    self,\n    dataset: \"BaseDataset\",\n    run_name: str | None = None,\n    batch_size: int = 32,\n    tok_kwargs: Dict | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    with_controllers: bool = True,\n    free_cuda_cache_every: int | None = 0,\n    clear_detectors_before: bool = False,\n    verbose: bool = False,\n    stop_after_layer: str | int | None = None,\n    save_in_batches: bool = True,\n) -&gt; str:\n    \"\"\"\n    Run inference on whole dataset with metadata saving.\n\n    Args:\n        dataset: Dataset to process\n        run_name: Optional run name (generated if None)\n        batch_size: Batch size for processing\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        free_cuda_cache_every: Clear CUDA cache every N batches (0 or None to disable)\n        clear_detectors_before: If True, clears all detector state before running\n        verbose: Whether to log progress\n        stop_after_layer: Optional layer signature (name or index) after which\n            the forward pass should be stopped early\n\n    Returns:\n        Run name used for saving\n\n    Raises:\n        ValueError: If model or store is not initialized\n    \"\"\"\n    if clear_detectors_before:\n        self.lm.clear_detectors()\n\n    model: nn.Module | None = self.lm.model\n    if model is None:\n        raise ValueError(\"Model must be initialized before running\")\n\n    store = self.lm.store\n    if store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n    device = torch.device(self.lm.context.device)\n    device_type = str(device.type)\n\n    options = {\n        \"max_length\": tok_kwargs.get(\"max_length\") if tok_kwargs else None,\n        \"batch_size\": int(batch_size),\n    }\n\n    run_name, meta = self._prepare_run_metadata(dataset=dataset, run_name=run_name, options=options)\n\n    if verbose:\n        logger.info(\n            f\"Starting infer_dataset: run={run_name}, \"\n            f\"batch_size={batch_size}, device={device_type}\"\n        )\n\n    self._save_run_metadata(store, run_name, meta, verbose)\n\n    batch_counter = 0\n\n    with torch.inference_mode():\n        for batch_index, batch in enumerate(dataset.iter_batches(batch_size)):\n            if not batch:\n                continue\n\n            texts = dataset.extract_texts_from_batch(batch)\n\n            self.execute_inference(\n                texts,\n                tok_kwargs=tok_kwargs,\n                autocast=autocast,\n                autocast_dtype=autocast_dtype,\n                with_controllers=with_controllers,\n                stop_after_layer=stop_after_layer,\n            )\n\n            self.lm.save_detector_metadata(run_name, batch_index, unified=not save_in_batches)\n\n            batch_counter += 1\n\n            if device_type == \"cuda\" and free_cuda_cache_every and free_cuda_cache_every &gt; 0:\n                if (batch_counter % free_cuda_cache_every) == 0:\n                    torch.cuda.empty_cache()\n                    if verbose:\n                        logger.info(\"Emptied CUDA cache\")\n\n            if verbose:\n                logger.info(f\"Saved batch {batch_index} for run={run_name}\")\n\n    if verbose:\n        logger.info(f\"Completed infer_dataset: run={run_name}, batches_saved={batch_counter}\")\n\n    return run_name\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.infer_texts","title":"infer_texts","text":"<pre><code>infer_texts(texts, run_name=None, batch_size=None, tok_kwargs=None, autocast=True, autocast_dtype=None, with_controllers=True, clear_detectors_before=False, verbose=False, stop_after_layer=None, save_in_batches=True)\n</code></pre> <p>Run inference on list of strings with optional metadata saving.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of input texts</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name for saving metadata (if None, no metadata saved)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Optional batch size for processing (if None, processes all at once)</p> <code>None</code> <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>clear_detectors_before</code> <code>bool</code> <p>If True, clears all detector state before running</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>stop_after_layer</code> <code>str | int | None</code> <p>Optional layer signature (name or index) after which the forward pass should be stopped early</p> <code>None</code> <code>save_in_batches</code> <code>bool</code> <p>If True, save detector metadata in per\u2011batch directories. If False, aggregate all detector metadata for the run under a single detectors directory.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Any, Dict[str, Tensor]] | tuple[List[Any], List[Dict[str, Tensor]]]</code> <p>If batch_size is None or &gt;= len(texts): Tuple of (model_output, encodings)</p> <code>tuple[Any, Dict[str, Tensor]] | tuple[List[Any], List[Dict[str, Tensor]]]</code> <p>If batch_size &lt; len(texts): Tuple of (list of outputs, list of encodings)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If texts is empty or tokenizer is not initialized</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def infer_texts(\n    self,\n    texts: Sequence[str],\n    run_name: str | None = None,\n    batch_size: int | None = None,\n    tok_kwargs: Dict | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    with_controllers: bool = True,\n    clear_detectors_before: bool = False,\n    verbose: bool = False,\n    stop_after_layer: str | int | None = None,\n    save_in_batches: bool = True,\n) -&gt; tuple[Any, Dict[str, torch.Tensor]] | tuple[List[Any], List[Dict[str, torch.Tensor]]]:\n    \"\"\"\n    Run inference on list of strings with optional metadata saving.\n\n    Args:\n        texts: Sequence of input texts\n        run_name: Optional run name for saving metadata (if None, no metadata saved)\n        batch_size: Optional batch size for processing (if None, processes all at once)\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        clear_detectors_before: If True, clears all detector state before running\n        verbose: Whether to log progress\n        stop_after_layer: Optional layer signature (name or index) after which\n            the forward pass should be stopped early\n        save_in_batches: If True, save detector metadata in per\u2011batch\n            directories. If False, aggregate all detector metadata for\n            the run under a single detectors directory.\n\n    Returns:\n        If batch_size is None or &gt;= len(texts): Tuple of (model_output, encodings)\n        If batch_size &lt; len(texts): Tuple of (list of outputs, list of encodings)\n\n    Raises:\n        ValueError: If texts is empty or tokenizer is not initialized\n    \"\"\"\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    if self.lm.tokenizer is None:\n        raise ValueError(\"Tokenizer must be initialized before running inference\")\n\n    if clear_detectors_before:\n        self.lm.clear_detectors()\n\n    store = self.lm.store\n    if run_name is not None and store is None:\n        raise ValueError(\"Store must be provided to save metadata\")\n\n    if batch_size is None or batch_size &gt;= len(texts):\n        output, enc = self.execute_inference(\n            texts,\n            tok_kwargs=tok_kwargs,\n            autocast=autocast,\n            autocast_dtype=autocast_dtype,\n            with_controllers=with_controllers,\n            stop_after_layer=stop_after_layer,\n        )\n\n        if run_name is not None:\n            options = {\n                \"batch_size\": len(texts),\n                \"max_length\": tok_kwargs.get(\"max_length\") if tok_kwargs else None,\n            }\n            _, meta = self._prepare_run_metadata(dataset=None, run_name=run_name, options=options)\n            self._save_run_metadata(store, run_name, meta, verbose)\n            self.lm.save_detector_metadata(run_name, 0, unified=not save_in_batches)\n\n        return output, enc\n\n    all_outputs = []\n    all_encodings = []\n    batch_counter = 0\n\n    if run_name is not None:\n        options = {\n            \"batch_size\": batch_size,\n            \"max_length\": tok_kwargs.get(\"max_length\") if tok_kwargs else None,\n        }\n        _, meta = self._prepare_run_metadata(dataset=None, run_name=run_name, options=options)\n        self._save_run_metadata(store, run_name, meta, verbose)\n\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i + batch_size]\n        output, enc = self.execute_inference(\n            batch_texts,\n            tok_kwargs=tok_kwargs,\n            autocast=autocast,\n            autocast_dtype=autocast_dtype,\n            with_controllers=with_controllers,\n            stop_after_layer=stop_after_layer,\n        )\n\n        all_outputs.append(output)\n        all_encodings.append(enc)\n\n        if run_name is not None:\n            self.lm.save_detector_metadata(run_name, batch_counter, unified=not save_in_batches)\n            if verbose:\n                logger.info(f\"Saved batch {batch_counter} for run={run_name}\")\n\n        batch_counter += 1\n\n    return all_outputs, all_encodings\n</code></pre>"},{"location":"api/language_model/#utilities","title":"Utilities","text":""},{"location":"api/language_model/#mi_crow.language_model.initialization","title":"mi_crow.language_model.initialization","text":"<p>Model initialization and factory methods.</p>"},{"location":"api/language_model/#mi_crow.language_model.initialization.create_from_huggingface","title":"create_from_huggingface","text":"<pre><code>create_from_huggingface(cls, model_name, store, tokenizer_params=None, model_params=None, device=None)\n</code></pre> <p>Load a language model from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type['LanguageModel']</code> <p>LanguageModel class</p> required <code>model_name</code> <code>str</code> <p>HuggingFace model identifier</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>tokenizer_params</code> <code>dict | None</code> <p>Optional tokenizer parameters</p> <code>None</code> <code>model_params</code> <code>dict | None</code> <p>Optional model parameters</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>Target device (\"cuda\", \"cpu\", \"mps\"). Model will be moved to this device after loading.</p> <code>None</code> <p>Returns:     LanguageModel instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_name is invalid</p> <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/mi_crow/language_model/initialization.py</code> <pre><code>def create_from_huggingface(\n        cls: type[\"LanguageModel\"],\n        model_name: str,\n        store: Store,\n        tokenizer_params: dict | None = None,\n        model_params: dict | None = None,\n        device: str | torch.device | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from HuggingFace Hub.\n\n    Args:\n        cls: LanguageModel class\n        model_name: HuggingFace model identifier\n        store: Store instance for persistence\n        tokenizer_params: Optional tokenizer parameters\n        model_params: Optional model parameters\n        device: Target device (\"cuda\", \"cpu\", \"mps\"). Model will be moved to this device\n            after loading.\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        ValueError: If model_name is invalid\n        RuntimeError: If model loading fails\n    \"\"\"\n    if not model_name or not isinstance(model_name, str) or not model_name.strip():\n        raise ValueError(f\"model_name must be a non-empty string, got: {model_name!r}\")\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    if tokenizer_params is None:\n        tokenizer_params = {}\n    if model_params is None:\n        model_params = {}\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_params)\n        model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load model '{model_name}' from HuggingFace. Error: {e}\"\n        ) from e\n\n    return cls(model, tokenizer, store, device=device)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.initialization.create_from_local_torch","title":"create_from_local_torch","text":"<pre><code>create_from_local_torch(cls, model_path, tokenizer_path, store, device=None)\n</code></pre> <p>Load a language model from local HuggingFace paths.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type['LanguageModel']</code> <p>LanguageModel class</p> required <code>model_path</code> <code>str</code> <p>Path to the model directory or file</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to the tokenizer directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>device</code> <code>str | device | None</code> <p>Optional device string or torch.device (defaults to 'cpu' if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If model or tokenizer paths don't exist</p> <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/mi_crow/language_model/initialization.py</code> <pre><code>def create_from_local_torch(\n        cls: type[\"LanguageModel\"],\n        model_path: str,\n        tokenizer_path: str,\n        store: Store,\n        device: str | torch.device | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from local HuggingFace paths.\n\n    Args:\n        cls: LanguageModel class\n        model_path: Path to the model directory or file\n        tokenizer_path: Path to the tokenizer directory or file\n        store: Store instance for persistence\n        device: Optional device string or torch.device (defaults to 'cpu' if None)\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        FileNotFoundError: If model or tokenizer paths don't exist\n        RuntimeError: If model loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    model_path_obj = Path(model_path)\n    tokenizer_path_obj = Path(tokenizer_path)\n\n    if not model_path_obj.exists():\n        raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n\n    if not tokenizer_path_obj.exists():\n        raise FileNotFoundError(f\"Tokenizer path does not exist: {tokenizer_path}\")\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load model from local paths. \"\n            f\"model_path={model_path!r}, tokenizer_path={tokenizer_path!r}. Error: {e}\"\n        ) from e\n\n    return cls(model, tokenizer, store, device=device)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.initialization.initialize_model_id","title":"initialize_model_id","text":"<pre><code>initialize_model_id(model, provided_model_id=None)\n</code></pre> <p>Initialize model ID for LanguageModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model module</p> required <code>provided_model_id</code> <code>str | None</code> <p>Optional model ID provided by user</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Model ID string</p> Source code in <code>src/mi_crow/language_model/initialization.py</code> <pre><code>def initialize_model_id(\n        model: nn.Module,\n        provided_model_id: str | None = None\n) -&gt; str:\n    \"\"\"\n    Initialize model ID for LanguageModel.\n\n    Args:\n        model: PyTorch model module\n        provided_model_id: Optional model ID provided by user\n\n    Returns:\n        Model ID string\n    \"\"\"\n    return extract_model_id(model, provided_model_id)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.persistence","title":"mi_crow.language_model.persistence","text":"<p>Model persistence (save/load) operations.</p>"},{"location":"api/language_model/#mi_crow.language_model.persistence.load_model_from_saved_file","title":"load_model_from_saved_file","text":"<pre><code>load_model_from_saved_file(cls, saved_path, store, model_id=None, device=None)\n</code></pre> <p>Load a language model from a saved file (created by save_model).</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type['LanguageModel']</code> <p>LanguageModel class</p> required <code>saved_path</code> <code>Path | str</code> <p>Path to the saved model file (.pt file)</p> required <code>store</code> <code>'Store'</code> <p>Store instance for persistence</p> required <code>model_id</code> <code>str | None</code> <p>Optional model identifier. If not provided, will use the model_id from saved metadata.      If provided, will be used to load the model architecture from HuggingFace.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>Optional device string or torch.device (defaults to 'cpu' if None)</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the saved file doesn't exist</p> <code>ValueError</code> <p>If the saved file format is invalid or model_id is required but not provided</p> <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/mi_crow/language_model/persistence.py</code> <pre><code>def load_model_from_saved_file(\n        cls: type[\"LanguageModel\"],\n        saved_path: Path | str,\n        store: \"Store\",\n        model_id: str | None = None,\n        device: str | torch.device | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from a saved file (created by save_model).\n\n    Args:\n        cls: LanguageModel class\n        saved_path: Path to the saved model file (.pt file)\n        store: Store instance for persistence\n        model_id: Optional model identifier. If not provided, will use the model_id from saved metadata.\n                 If provided, will be used to load the model architecture from HuggingFace.\n        device: Optional device string or torch.device (defaults to 'cpu' if None)\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        FileNotFoundError: If the saved file doesn't exist\n        ValueError: If the saved file format is invalid or model_id is required but not provided\n        RuntimeError: If model loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    saved_path = Path(saved_path)\n    if not saved_path.exists():\n        raise FileNotFoundError(f\"Saved model file not found: {saved_path}\")\n\n    # Load the saved payload\n    try:\n        payload = torch.load(saved_path, map_location='cpu')\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load model file {saved_path}. Error: {e}\"\n        ) from e\n\n    # Validate payload structure\n    if \"model_state_dict\" not in payload:\n        raise ValueError(f\"Invalid saved model format: missing 'model_state_dict' key in {saved_path}\")\n    if \"metadata\" not in payload:\n        raise ValueError(f\"Invalid saved model format: missing 'metadata' key in {saved_path}\")\n\n    model_state_dict = payload[\"model_state_dict\"]\n    metadata_dict = payload[\"metadata\"]\n\n    # Get model_id from metadata or use provided one\n    saved_model_id = metadata_dict.get(\"model_id\")\n    if model_id is None:\n        if saved_model_id is None:\n            raise ValueError(\n                f\"model_id not found in saved metadata and not provided. \"\n                f\"Please provide model_id parameter.\"\n            )\n        model_id = saved_model_id\n\n    # Load model and tokenizer from HuggingFace using model_id\n    # This assumes model_id is a valid HuggingFace model name\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        model = AutoModelForCausalLM.from_pretrained(model_id)\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to load model '{model_id}' from HuggingFace. \"\n            f\"Error: {e}. \"\n            f\"Please ensure model_id is a valid HuggingFace model name.\"\n        ) from e\n\n    # Load the saved state dict into the model\n    try:\n        model.load_state_dict(model_state_dict)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load state dict into model '{model_id}'. Error: {e}\"\n        ) from e\n\n    # Create LanguageModel instance\n    lm = cls(model, tokenizer, store, model_id=model_id, device=device)\n\n    # Note: Hooks are not automatically restored as they require hook instances\n    # The hook metadata is available in metadata_dict[\"hooks\"] if needed\n\n    from mi_crow.utils import get_logger\n    logger = get_logger(__name__)\n    logger.info(f\"Loaded model from {saved_path} (model_id: {model_id})\")\n\n    return lm\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.persistence.save_model","title":"save_model","text":"<pre><code>save_model(language_model, path=None)\n</code></pre> <p>Save the model and its metadata to the store.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>'LanguageModel'</code> <p>LanguageModel instance to save</p> required <code>path</code> <code>Path | str | None</code> <p>Optional path to save the model. If None, defaults to {model_id}/model.pt   relative to the store base path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path where the model was saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is not set</p> <code>OSError</code> <p>If file operations fail</p> Source code in <code>src/mi_crow/language_model/persistence.py</code> <pre><code>def save_model(\n        language_model: \"LanguageModel\",\n        path: Path | str | None = None\n) -&gt; Path:\n    \"\"\"\n    Save the model and its metadata to the store.\n\n    Args:\n        language_model: LanguageModel instance to save\n        path: Optional path to save the model. If None, defaults to {model_id}/model.pt\n              relative to the store base path.\n\n    Returns:\n        Path where the model was saved\n\n    Raises:\n        ValueError: If store is not set\n        OSError: If file operations fail\n    \"\"\"\n    if language_model.store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n    # Determine save path\n    if path is None:\n        save_path = Path(language_model.store.base_path) / language_model.model_id / \"model.pt\"\n    else:\n        save_path = Path(path)\n        # If path is relative, make it relative to store base path\n        if not save_path.is_absolute():\n            save_path = Path(language_model.store.base_path) / save_path\n\n    # Ensure parent directory exists\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Collect hooks information\n    hooks_info = collect_hooks_metadata(language_model.context)\n\n    # Save model state dict\n    model_state_dict = language_model.model.state_dict()\n\n    # Create metadata\n    metadata = ModelMetadata(\n        model_id=language_model.model_id,\n        hooks=hooks_info,\n        model_path=str(save_path)\n    )\n\n    # Save everything in a single file\n    payload = {\n        \"model_state_dict\": model_state_dict,\n        \"metadata\": asdict(metadata),\n    }\n\n    try:\n        torch.save(payload, save_path)\n    except OSError as e:\n        raise OSError(\n            f\"Failed to save model to {save_path}. Error: {e}\"\n        ) from e\n\n    from mi_crow.utils import get_logger\n    logger = get_logger(__name__)\n    logger.info(f\"Saved model to {save_path}\")\n\n    return save_path\n</code></pre>"},{"location":"api/sae/","title":"Sparse Autoencoder (SAE) API","text":"<p>Sparse Autoencoders, training, concepts, and related modules for mechanistic interpretability.</p>"},{"location":"api/sae/#core-sae-classes","title":"Core SAE Classes","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae","title":"mi_crow.mechanistic.sae.sae.Sae","text":"<pre><code>Sae(n_latents, n_inputs, hook_id=None, device='cpu', store=None, *args, **kwargs)\n</code></pre> <p>               Bases: <code>Controller</code>, <code>Detector</code>, <code>ABC</code></p> Source code in <code>src/mi_crow/mechanistic/sae/sae.py</code> <pre><code>def __init__(\n        self,\n        n_latents: int,\n        n_inputs: int,\n        hook_id: str | None = None,\n        device: str = 'cpu',\n        store: Store | None = None,\n        *args: Any,\n        **kwargs: Any\n) -&gt; None:\n    # Initialize both Controller and Detector\n    Controller.__init__(self, hook_type=HookType.FORWARD, hook_id=hook_id)\n    Detector.__init__(self, hook_type=HookType.FORWARD, hook_id=hook_id, store=store)\n\n    self._autoencoder_context = AutoencoderContext(\n        autoencoder=self,\n        n_latents=n_latents,\n        n_inputs=n_inputs\n    )\n    self._autoencoder_context.device = device\n    self.sae_engine: OvercompleteSAE = self._initialize_sae_engine()\n    self.concepts = AutoencoderConcepts(self._autoencoder_context)\n\n    # Text tracking flag\n    self._text_tracking_enabled: bool = False\n\n    # Training component\n    self.trainer = SaeTrainer(self)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae.context","title":"context  <code>property</code> <code>writable</code>","text":"<pre><code>context\n</code></pre> <p>Get the AutoencoderContext associated with this SAE.</p>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae.process_activations","title":"process_activations  <code>abstractmethod</code>","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Process activations to save neuron activations in metadata.</p> <p>This implements the Detector interface. It extracts activations, encodes them to get neuron activations (latents), and saves metadata for each item in the batch individually, including nonzero latent indices and activations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required Source code in <code>src/mi_crow/mechanistic/sae/sae.py</code> <pre><code>@abc.abstractmethod\ndef process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Process activations to save neuron activations in metadata.\n\n    This implements the Detector interface. It extracts activations, encodes them\n    to get neuron activations (latents), and saves metadata for each item in the batch\n    individually, including nonzero latent indices and activations.\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n    \"\"\"\n    raise NotImplementedError(\"process_activations method not implemented.\")\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae.set_context","title":"set_context","text":"<pre><code>set_context(context)\n</code></pre> <p>Set the LanguageModelContext for this hook and sync to AutoencoderContext.</p> <p>When the hook is registered, this method is called with the LanguageModelContext. It automatically syncs relevant values to the AutoencoderContext, including device.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>The LanguageModelContext instance from the LanguageModel</p> required Source code in <code>src/mi_crow/mechanistic/sae/sae.py</code> <pre><code>def set_context(self, context: \"LanguageModelContext\") -&gt; None:\n    \"\"\"Set the LanguageModelContext for this hook and sync to AutoencoderContext.\n\n    When the hook is registered, this method is called with the LanguageModelContext.\n    It automatically syncs relevant values to the AutoencoderContext, including device.\n\n    Args:\n        context: The LanguageModelContext instance from the LanguageModel\n    \"\"\"\n    Hook.set_context(self, context)\n    self._context = context\n    if context is not None:\n        self._autoencoder_context.lm = context.language_model\n        if context.model_id is not None:\n            self._autoencoder_context.model_id = context.model_id\n        if context.store is not None and self._autoencoder_context.store is None:\n            self._autoencoder_context.store = context.store\n        if self.layer_signature is not None:\n            self._autoencoder_context.lm_layer_signature = self.layer_signature\n        if context.device is not None:\n            self._autoencoder_context.device = context.device\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae","title":"mi_crow.mechanistic.sae.modules.topk_sae.TopKSae","text":"<pre><code>TopKSae(n_latents, n_inputs, hook_id=None, device='cpu', store=None, *args, **kwargs)\n</code></pre> <p>               Bases: <code>Sae</code></p> <p>Initialize TopK SAE.</p> <p>Parameters:</p> Name Type Description Default <code>n_latents</code> <code>int</code> <p>Number of latent dimensions (concepts)</p> required <code>n_inputs</code> <code>int</code> <p>Number of input dimensions</p> required <code>hook_id</code> <code>str | None</code> <p>Optional hook identifier</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to run on ('cpu', 'cuda', 'mps')</p> <code>'cpu'</code> <code>store</code> <code>Store | None</code> <p>Optional store instance</p> <code>None</code> Note <p>The <code>k</code> parameter must be provided in TopKSaeTrainingConfig during training. For loaded models, <code>k</code> is restored from saved metadata. A temporary default k=1 is used for engine initialization and will be overridden with the actual k value from config during training.</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def __init__(\n        self,\n        n_latents: int,\n        n_inputs: int,\n        hook_id: str | None = None,\n        device: str = 'cpu',\n        store: Store | None = None,\n        *args: Any,\n        **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize TopK SAE.\n\n    Args:\n        n_latents: Number of latent dimensions (concepts)\n        n_inputs: Number of input dimensions\n        hook_id: Optional hook identifier\n        device: Device to run on ('cpu', 'cuda', 'mps')\n        store: Optional store instance\n\n    Note:\n        The `k` parameter must be provided in TopKSaeTrainingConfig during training.\n        For loaded models, `k` is restored from saved metadata.\n        A temporary default k=1 is used for engine initialization and will be\n        overridden with the actual k value from config during training.\n    \"\"\"\n    super().__init__(n_latents, n_inputs, hook_id, device, store, *args, **kwargs)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.decode","title":"decode","text":"<pre><code>decode(x)\n</code></pre> <p>Decode latents using sae_engine.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Encoded tensor of shape [batch_size, n_latents]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed tensor of shape [batch_size, n_inputs]</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latents using sae_engine.\n\n    Args:\n        x: Encoded tensor of shape [batch_size, n_latents]\n\n    Returns:\n        Reconstructed tensor of shape [batch_size, n_inputs]\n    \"\"\"\n    return self.sae_engine.decode(x)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.encode","title":"encode","text":"<pre><code>encode(x)\n</code></pre> <p>Encode input using sae_engine.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, n_inputs]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded latents (TopK sparse activations)</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Encode input using sae_engine.\n\n    Args:\n        x: Input tensor of shape [batch_size, n_inputs]\n\n    Returns:\n        Encoded latents (TopK sparse activations)\n    \"\"\"\n    # Overcomplete TopKSAE encode returns (pre_codes, codes)\n    _, codes = self.sae_engine.encode(x)\n    return codes\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass using sae_engine.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, n_inputs]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed tensor of shape [batch_size, n_inputs]</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using sae_engine.\n\n    Args:\n        x: Input tensor of shape [batch_size, n_inputs]\n\n    Returns:\n        Reconstructed tensor of shape [batch_size, n_inputs]\n    \"\"\"\n    # Overcomplete TopKSAE forward returns (pre_codes, codes, x_reconstructed)\n    _, _, x_reconstructed = self.sae_engine.forward(x)\n    return x_reconstructed\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.load","title":"load  <code>staticmethod</code>","text":"<pre><code>load(path)\n</code></pre> <p>Load TopKSAE from saved file using overcomplete's load method + our metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to saved model file</p> required <p>Returns:</p> Type Description <code>TopKSae</code> <p>Loaded TopKSAE instance</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>@staticmethod\ndef load(path: Path) -&gt; \"TopKSae\":\n    \"\"\"\n    Load TopKSAE from saved file using overcomplete's load method + our metadata.\n\n    Args:\n        path: Path to saved model file\n\n    Returns:\n        Loaded TopKSAE instance\n    \"\"\"\n    p = Path(path)\n\n    # Load payload\n    if torch.cuda.is_available():\n        map_location = 'cuda'\n    elif torch.backends.mps.is_available():\n        map_location = 'mps'\n    else:\n        map_location = 'cpu'\n    payload = torch.load(p, map_location=map_location)\n\n    # Extract our metadata\n    if \"mi_crow_metadata\" not in payload:\n        raise ValueError(f\"Invalid TopKSAE save format: missing 'mi_crow_metadata' key in {p}\")\n\n    mi_crow_meta = payload[\"mi_crow_metadata\"]\n    n_latents = int(mi_crow_meta[\"n_latents\"])\n    n_inputs = int(mi_crow_meta[\"n_inputs\"])\n    k = int(mi_crow_meta[\"k\"])\n    device = mi_crow_meta.get(\"device\", \"cpu\")\n    layer_signature = mi_crow_meta.get(\"layer_signature\")\n    model_id = mi_crow_meta.get(\"model_id\")\n    concepts_state = mi_crow_meta.get(\"concepts_state\", {})\n\n    # Create TopKSAE instance\n    topk_sae = TopKSae(\n        n_latents=n_latents,\n        n_inputs=n_inputs,\n        device=device\n    )\n\n    topk_sae.sae_engine = topk_sae._initialize_sae_engine(k=k)\n\n    # Load overcomplete model state dict\n    if \"sae_state_dict\" in payload:\n        topk_sae.sae_engine.load_state_dict(payload[\"sae_state_dict\"])\n    elif \"model\" in payload:\n        # Backward compatibility with old format\n        topk_sae.sae_engine.load_state_dict(payload[\"model\"])\n    else:\n        # Assume payload is the state dict itself (backward compatibility)\n        topk_sae.sae_engine.load_state_dict(payload)\n\n    # Load concepts state\n    if concepts_state:\n        device = topk_sae.context.device\n        if isinstance(device, str):\n            device = torch.device(device)\n        if \"multiplication\" in concepts_state:\n            topk_sae.concepts.multiplication.data = concepts_state[\"multiplication\"].to(device)\n        if \"bias\" in concepts_state:\n            topk_sae.concepts.bias.data = concepts_state[\"bias\"].to(device)\n\n    # Note: Top texts loading was removed as serialization methods were removed\n    # Top texts should be exported/imported separately if needed\n\n    # Set context metadata\n    topk_sae.context.lm_layer_signature = layer_signature\n    topk_sae.context.model_id = model_id\n\n    params_str = f\"n_latents={n_latents}, n_inputs={n_inputs}, k={k}\"\n    logger.info(f\"\\nLoaded TopKSAE from {p}\\n{params_str}\")\n\n    return topk_sae\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.modify_activations","title":"modify_activations","text":"<pre><code>modify_activations(module, inputs, output)\n</code></pre> <p>Modify activations using TopKSAE (Controller hook interface).</p> <p>Extracts tensor from inputs/output, applies SAE forward pass, and optionally applies concept manipulation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>inputs</code> <code>Tensor | None</code> <p>Tuple of inputs to the module</p> required <code>output</code> <code>Tensor | None</code> <p>Output from the module (None for pre_forward hooks)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>Modified activations with same shape as input</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def modify_activations(\n        self,\n        module: \"torch.nn.Module\",\n        inputs: torch.Tensor | None,\n        output: torch.Tensor | None\n) -&gt; torch.Tensor | None:\n    \"\"\"\n    Modify activations using TopKSAE (Controller hook interface).\n\n    Extracts tensor from inputs/output, applies SAE forward pass,\n    and optionally applies concept manipulation.\n\n    Args:\n        module: The PyTorch module being hooked\n        inputs: Tuple of inputs to the module\n        output: Output from the module (None for pre_forward hooks)\n\n    Returns:\n        Modified activations with same shape as input\n    \"\"\"\n    # Extract tensor from output/inputs, handling objects with last_hidden_state\n    if self.hook_type == HookType.FORWARD:\n        if isinstance(output, torch.Tensor):\n            tensor = output\n        elif hasattr(output, \"last_hidden_state\") and isinstance(output.last_hidden_state, torch.Tensor):\n            tensor = output.last_hidden_state\n        elif isinstance(output, (tuple, list)):\n            # Try to find first tensor in tuple/list\n            tensor = next((item for item in output if isinstance(item, torch.Tensor)), None)\n        else:\n            tensor = None\n    else:\n        tensor = inputs[0] if len(inputs) &gt; 0 and isinstance(inputs[0], torch.Tensor) else None\n\n    if tensor is None or not isinstance(tensor, torch.Tensor):\n        return output if self.hook_type == HookType.FORWARD else inputs\n\n    original_shape = tensor.shape\n\n    # Flatten to 2D for SAE processing: (batch, seq_len, hidden) -&gt; (batch * seq_len, hidden)\n    # or keep as 2D if already 2D: (batch, hidden)\n    if len(original_shape) &gt; 2:\n        batch_size, seq_len = original_shape[:2]\n        tensor_flat = tensor.reshape(-1, original_shape[-1])\n    else:\n        batch_size = original_shape[0]\n        seq_len = 1\n        tensor_flat = tensor\n\n    # Get full activations (pre_codes) and sparse codes\n    # Overcomplete TopKSAE encode returns (pre_codes, codes)\n    pre_codes, codes = self.sae_engine.encode(tensor_flat)\n\n    # Save SAE activations (pre_codes) as 3D tensor: (batch, seq, n_latents)\n    latents_cpu = pre_codes.detach().cpu()\n    latents_3d = latents_cpu.reshape(batch_size, seq_len, -1)\n\n    # Save to tensor_metadata\n    self.tensor_metadata['neurons'] = latents_3d\n    self.tensor_metadata['activations'] = latents_3d\n\n    # Process each item in the batch individually for metadata\n    batch_items = []\n    n_items = latents_cpu.shape[0]\n    for item_idx in range(n_items):\n        item_latents = latents_cpu[item_idx]  # [n_latents]\n\n        # Find nonzero indices for this item\n        nonzero_mask = item_latents != 0\n        nonzero_indices = torch.nonzero(nonzero_mask, as_tuple=False).flatten().tolist()\n\n        # Create map of nonzero indices to activations\n        activations_map = {\n            int(idx): float(item_latents[idx].item())\n            for idx in nonzero_indices\n        }\n\n        # Create item metadata\n        item_metadata = {\n            \"nonzero_indices\": nonzero_indices,\n            \"activations\": activations_map\n        }\n        batch_items.append(item_metadata)\n\n    # Save batch items metadata\n    self.metadata['batch_items'] = batch_items\n\n    # Use sparse codes for reconstruction\n    latents = codes\n\n    # Update top texts if text tracking is enabled\n    if self._text_tracking_enabled and self.context.lm is not None:\n        input_tracker = self.context.lm.get_input_tracker()\n        if input_tracker is not None:\n            texts = input_tracker.get_current_texts()\n            if texts:\n                # Use pre_codes (full activations) for text tracking\n                self.concepts.update_top_texts_from_latents(\n                    latents_cpu,\n                    texts,\n                    original_shape\n                )\n\n    # Apply concept manipulation if parameters are set\n    # Check if multiplication or bias differ from defaults (ones)\n    if not torch.allclose(self.concepts.multiplication, torch.ones_like(self.concepts.multiplication)) or \\\n            not torch.allclose(self.concepts.bias, torch.ones_like(self.concepts.bias)):\n        # Apply manipulation: latents = latents * multiplication + bias\n        latents = latents * self.concepts.multiplication + self.concepts.bias\n\n    # Decode to get reconstruction\n    reconstructed = self.decode(latents)\n\n    # Reshape back to original shape\n    if len(original_shape) &gt; 2:\n        reconstructed = reconstructed.reshape(original_shape)\n\n    # Return in appropriate format\n    if self.hook_type == HookType.FORWARD:\n        if isinstance(output, torch.Tensor):\n            return reconstructed\n        elif isinstance(output, (tuple, list)):\n            # Replace first tensor in tuple/list\n            result = list(output)\n            for i, item in enumerate(result):\n                if isinstance(item, torch.Tensor):\n                    result[i] = reconstructed\n                    break\n            return tuple(result) if isinstance(output, tuple) else result\n        else:\n            # For objects with attributes, try to set last_hidden_state\n            if hasattr(output, \"last_hidden_state\"):\n                output.last_hidden_state = reconstructed\n            return output\n    else:  # PRE_FORWARD\n        # Return modified inputs tuple\n        result = list(inputs)\n        if len(result) &gt; 0:\n            result[0] = reconstructed\n        return tuple(result)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Process activations (Detector interface).</p> <p>Metadata saving is handled in modify_activations to avoid duplicate work. This method is kept for interface compatibility but does nothing since modify_activations already saves the metadata when called.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Process activations (Detector interface).\n\n    Metadata saving is handled in modify_activations to avoid duplicate work.\n    This method is kept for interface compatibility but does nothing since\n    modify_activations already saves the metadata when called.\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n    \"\"\"\n    # Metadata saving is done in modify_activations to avoid duplicate encoding\n    pass\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.save","title":"save","text":"<pre><code>save(name, path=None, k=None)\n</code></pre> <p>Save model using overcomplete's state dict + our metadata.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name</p> required <code>path</code> <code>str | Path | None</code> <p>Directory path to save to (defaults to current directory)</p> <code>None</code> <code>k</code> <code>int | None</code> <p>Top-K value to save (if None, attempts to get from engine or raises error)</p> <code>None</code> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def save(self, name: str, path: str | Path | None = None, k: int | None = None) -&gt; None:\n    \"\"\"\n    Save model using overcomplete's state dict + our metadata.\n\n    Args:\n        name: Model name\n        path: Directory path to save to (defaults to current directory)\n        k: Top-K value to save (if None, attempts to get from engine or raises error)\n    \"\"\"\n    if path is None:\n        path = Path.cwd()\n    save_dir = Path(path)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    save_path = save_dir / f\"{name}.pt\"\n\n    # Save overcomplete model state dict\n    sae_state_dict = self.sae_engine.state_dict()\n\n    # Get k value - prefer parameter, then try to get from engine\n    if k is None:\n        if hasattr(self.sae_engine, 'top_k'):\n            k = self.sae_engine.top_k\n        else:\n            raise ValueError(\n                \"k parameter must be provided to save() method. \"\n                \"The engine does not expose top_k attribute.\"\n            )\n\n    mi_crow_metadata = {\n        \"concepts_state\": {\n            'multiplication': self.concepts.multiplication.data,\n            'bias': self.concepts.bias.data,\n        },\n        \"n_latents\": self.context.n_latents,\n        \"n_inputs\": self.context.n_inputs,\n        \"k\": k,\n        \"device\": self.context.device,\n        \"layer_signature\": self.context.lm_layer_signature,\n        \"model_id\": self.context.model_id,\n    }\n\n    payload = {\n        \"sae_state_dict\": sae_state_dict,\n        \"mi_crow_metadata\": mi_crow_metadata,\n    }\n\n    torch.save(payload, save_path)\n    logger.info(f\"Saved TopKSAE to {save_path}\")\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.train","title":"train","text":"<pre><code>train(store, run_id, layer_signature, config=None, training_run_id=None)\n</code></pre> <p>Train TopKSAE using activations from a Store.</p> <p>This method delegates to the SaeTrainer composite class. The SAE engine will be reinitialized with the k value from config.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>Store instance containing activations</p> required <code>run_id</code> <code>str</code> <p>Run ID to train on</p> required <code>config</code> <code>TopKSaeTrainingConfig | None</code> <p>Training configuration (must include k parameter)</p> <code>None</code> <code>training_run_id</code> <code>str | None</code> <p>Optional training run ID</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with keys: - \"history\": Training history dictionary - \"training_run_id\": Training run ID where outputs were saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If config is None or config.k is not set</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def train(\n        self,\n        store: Store,\n        run_id: str,\n        layer_signature: str | int,\n        config: TopKSaeTrainingConfig | None = None,\n        training_run_id: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Train TopKSAE using activations from a Store.\n\n    This method delegates to the SaeTrainer composite class.\n    The SAE engine will be reinitialized with the k value from config.\n\n    Args:\n        store: Store instance containing activations\n        run_id: Run ID to train on\n        config: Training configuration (must include k parameter)\n        training_run_id: Optional training run ID\n\n    Returns:\n        Dictionary with keys:\n            - \"history\": Training history dictionary\n            - \"training_run_id\": Training run ID where outputs were saved\n\n    Raises:\n        ValueError: If config is None or config.k is not set\n    \"\"\"\n    if config is None:\n        config = TopKSaeTrainingConfig()\n\n    # Ensure k is set in config\n    if not hasattr(config, 'k') or config.k is None:\n        raise ValueError(\n            \"TopKSaeTrainingConfig must have k parameter set. \"\n            \"Example: TopKSaeTrainingConfig(k=10, epochs=100, ...)\"\n        )\n\n    # Reinitialize engine with k from config\n    logger.info(f\"Initializing SAE engine with k={config.k}\")\n    self.sae_engine = self._initialize_sae_engine(k=config.k)\n    if hasattr(config, 'device') and config.device:\n        device = torch.device(config.device)\n        self.sae_engine.to(device)\n        self.context.device = str(device)\n\n    return self.trainer.train(store, run_id, layer_signature, config, training_run_id)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.autoencoder_context.AutoencoderContext","title":"mi_crow.mechanistic.sae.autoencoder_context.AutoencoderContext  <code>dataclass</code>","text":"<pre><code>AutoencoderContext(autoencoder, n_latents, n_inputs, lm=None, lm_layer_signature=None, model_id=None, device='cpu', experiment_name=None, run_id=None, text_tracking_enabled=False, text_tracking_k=5, text_tracking_negative=False, store=None, tied=False, bias_init=0.0, init_method='kaiming')\n</code></pre> <p>Shared context for Autoencoder and its nested components.</p>"},{"location":"api/sae/#training","title":"Training","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.sae_trainer.SaeTrainer","title":"mi_crow.mechanistic.sae.sae_trainer.SaeTrainer","text":"<pre><code>SaeTrainer(sae)\n</code></pre> <p>Composite trainer class for SAE models using overcomplete's training functions.</p> <p>This trainer handles training of any SAE that has a sae_engine attribute compatible with overcomplete's train_sae functions.</p> <p>Initialize SaeTrainer.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <code>Sae</code> <p>The SAE instance to train</p> required Source code in <code>src/mi_crow/mechanistic/sae/sae_trainer.py</code> <pre><code>def __init__(self, sae: \"Sae\") -&gt; None:\n    \"\"\"\n    Initialize SaeTrainer.\n\n    Args:\n        sae: The SAE instance to train\n    \"\"\"\n    self.sae = sae\n    self.logger = get_logger(__name__)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae_trainer.SaeTrainingConfig","title":"mi_crow.mechanistic.sae.sae_trainer.SaeTrainingConfig  <code>dataclass</code>","text":"<pre><code>SaeTrainingConfig(epochs=1, batch_size=1024, lr=0.001, l1_lambda=0.0, device='cpu', dtype=None, max_batches_per_epoch=None, verbose=False, use_amp=True, amp_dtype=None, grad_accum_steps=1, clip_grad=1.0, monitoring=1, scheduler=None, max_nan_fallbacks=5, use_wandb=False, wandb_project=None, wandb_entity=None, wandb_name=None, wandb_tags=None, wandb_config=None, wandb_mode='online', wandb_slow_metrics_frequency=50, wandb_api_key=None, memory_efficient=False, snapshot_every_n_epochs=None, snapshot_base_path=None)\n</code></pre> <p>Configuration for SAE training (compatible with overcomplete.train_sae).</p>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSaeTrainingConfig","title":"mi_crow.mechanistic.sae.modules.topk_sae.TopKSaeTrainingConfig  <code>dataclass</code>","text":"<pre><code>TopKSaeTrainingConfig(epochs=1, batch_size=1024, lr=0.001, l1_lambda=0.0, device='cpu', dtype=None, max_batches_per_epoch=None, verbose=False, use_amp=True, amp_dtype=None, grad_accum_steps=1, clip_grad=1.0, monitoring=1, scheduler=None, max_nan_fallbacks=5, use_wandb=False, wandb_project=None, wandb_entity=None, wandb_name=None, wandb_tags=None, wandb_config=None, wandb_mode='online', wandb_slow_metrics_frequency=50, wandb_api_key=None, memory_efficient=False, snapshot_every_n_epochs=None, snapshot_base_path=None, k=10)\n</code></pre> <p>               Bases: <code>SaeTrainingConfig</code></p> <p>Training configuration for TopK SAE models.</p> <p>This class extends SaeTrainingConfig to provide a type-safe configuration interface specifically for TopK SAE models. It adds the <code>k</code> parameter which specifies how many top activations to keep during encoding.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of top activations to keep (required for TopK SAE training)</p> <code>10</code> Note <p>All other parameters are inherited from SaeTrainingConfig.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>Number of top activations to keep during TopK encoding</p> Example <p>config = TopKSaeTrainingConfig( ...     k=10, ...     epochs=100, ...     batch_size=1024, ...     lr=1e-3, ...     l1_lambda=1e-4 ... )</p>"},{"location":"api/sae/#concepts","title":"Concepts","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts","title":"mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts","text":"<pre><code>AutoencoderConcepts(context)\n</code></pre> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def __init__(\n        self,\n        context: AutoencoderContext\n):\n    self.context = context\n    self._n_size = context.n_latents\n    self.dictionary: ConceptDictionary | None = None\n\n    self.multiplication = nn.Parameter(torch.ones(self._n_size))\n    self.bias = nn.Parameter(torch.ones(self._n_size))\n\n    self._text_heaps_positive: list[TextHeap] | None = None\n    self._text_heaps_negative: list[TextHeap] | None = None\n    self._text_tracking_k: int = 5\n    self._text_tracking_negative: bool = False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.enable_text_tracking","title":"enable_text_tracking","text":"<pre><code>enable_text_tracking()\n</code></pre> <p>Enable text tracking using context parameters.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def enable_text_tracking(self):\n    \"\"\"Enable text tracking using context parameters.\"\"\"\n    if self.context.lm is None:\n        raise ValueError(\"LanguageModel must be set in context to enable tracking\")\n\n    # Store tracking parameters\n    self._text_tracking_k = self.context.text_tracking_k\n    self._text_tracking_negative = self.context.text_tracking_negative\n\n    # Ensure InputTracker singleton exists on LanguageModel and enable it\n    input_tracker = self.context.lm._ensure_input_tracker()\n    input_tracker.enable()\n\n    # Enable text tracking on the SAE instance\n    if hasattr(self.context.autoencoder, '_text_tracking_enabled'):\n        self.context.autoencoder._text_tracking_enabled = True\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.export_bottom_texts_to_json","title":"export_bottom_texts_to_json","text":"<pre><code>export_bottom_texts_to_json(filepath)\n</code></pre> <p>Export bottom texts (negative activations) to JSON file.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def export_bottom_texts_to_json(self, filepath: Path | str) -&gt; Path:\n    \"\"\"Export bottom texts (negative activations) to JSON file.\"\"\"\n    if not self._text_tracking_negative or self._text_heaps_negative is None:\n        raise ValueError(\"No bottom texts available. Enable negative text tracking and run inference first.\")\n\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    all_texts = self.get_all_bottom_texts()\n    export_data = {}\n\n    for neuron_idx, neuron_texts in enumerate(all_texts):\n        export_data[neuron_idx] = [\n            {\n                \"text\": nt.text,\n                \"score\": nt.score,\n                \"token_str\": nt.token_str,\n                \"token_idx\": nt.token_idx\n            }\n            for nt in neuron_texts\n        ]\n\n    with filepath.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(export_data, f, ensure_ascii=False, indent=2)\n\n    return filepath\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.export_top_texts_to_json","title":"export_top_texts_to_json","text":"<pre><code>export_top_texts_to_json(filepath)\n</code></pre> <p>Export top texts (positive activations) to JSON file.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def export_top_texts_to_json(self, filepath: Path | str) -&gt; Path:\n    \"\"\"Export top texts (positive activations) to JSON file.\"\"\"\n    if self._text_heaps_positive is None:\n        raise ValueError(\"No top texts available. Enable text tracking and run inference first.\")\n\n    filepath = Path(filepath)\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    all_texts = self.get_all_top_texts()\n    export_data = {}\n\n    for neuron_idx, neuron_texts in enumerate(all_texts):\n        export_data[neuron_idx] = [\n            {\n                \"text\": nt.text,\n                \"score\": nt.score,\n                \"token_str\": nt.token_str,\n                \"token_idx\": nt.token_idx\n            }\n            for nt in neuron_texts\n        ]\n\n    with filepath.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(export_data, f, ensure_ascii=False, indent=2)\n\n    return filepath\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.generate_concepts_with_llm","title":"generate_concepts_with_llm","text":"<pre><code>generate_concepts_with_llm(llm_provider=None)\n</code></pre> <p>Generate concepts using LLM based on current top texts</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def generate_concepts_with_llm(self, llm_provider: str | None = None):\n    \"\"\"Generate concepts using LLM based on current top texts\"\"\"\n    if self._text_heaps_positive is None:\n        raise ValueError(\"No top texts available. Enable text tracking and run inference first.\")\n\n    from mi_crow.mechanistic.sae.concepts.concept_dictionary import ConceptDictionary\n    neuron_texts = self.get_all_top_texts()\n\n    self.dictionary = ConceptDictionary.from_llm(\n        neuron_texts=neuron_texts,\n        n_size=self._n_size,\n        store=self.dictionary.store if self.dictionary else None,\n        llm_provider=llm_provider\n    )\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.get_all_bottom_texts","title":"get_all_bottom_texts","text":"<pre><code>get_all_bottom_texts()\n</code></pre> <p>Get bottom texts for all neurons (negative activations).</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def get_all_bottom_texts(self) -&gt; list[list[NeuronText]]:\n    \"\"\"Get bottom texts for all neurons (negative activations).\"\"\"\n    if not self._text_tracking_negative or self._text_heaps_negative is None:\n        return []\n    return [self.get_bottom_texts_for_neuron(i) for i in range(len(self._text_heaps_negative))]\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.get_all_top_texts","title":"get_all_top_texts","text":"<pre><code>get_all_top_texts()\n</code></pre> <p>Get top texts for all neurons (positive activations).</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def get_all_top_texts(self) -&gt; list[list[NeuronText]]:\n    \"\"\"Get top texts for all neurons (positive activations).\"\"\"\n    if self._text_heaps_positive is None:\n        return []\n    return [self.get_top_texts_for_neuron(i) for i in range(len(self._text_heaps_positive))]\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.get_bottom_texts_for_neuron","title":"get_bottom_texts_for_neuron","text":"<pre><code>get_bottom_texts_for_neuron(neuron_idx, top_m=None)\n</code></pre> <p>Get bottom texts for a specific neuron (negative activations).</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def get_bottom_texts_for_neuron(self, neuron_idx: int, top_m: int | None = None) -&gt; list[NeuronText]:\n    \"\"\"Get bottom texts for a specific neuron (negative activations).\"\"\"\n    if not self._text_tracking_negative:\n        return []\n    if self._text_heaps_negative is None or neuron_idx &lt; 0 or neuron_idx &gt;= len(self._text_heaps_negative):\n        return []\n    heap = self._text_heaps_negative[neuron_idx]\n    items = heap.get_items()\n    items_sorted = sorted(items, key=lambda s_t: s_t[0], reverse=False)\n    if top_m is not None:\n        items_sorted = items_sorted[: top_m]\n\n    neuron_texts = []\n    for score, text, token_idx in items_sorted:\n        token_str = self._decode_token(text, token_idx)\n        neuron_texts.append(NeuronText(score=score, text=text, token_idx=token_idx, token_str=token_str))\n    return neuron_texts\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.get_top_texts_for_neuron","title":"get_top_texts_for_neuron","text":"<pre><code>get_top_texts_for_neuron(neuron_idx, top_m=None)\n</code></pre> <p>Get top texts for a specific neuron (positive activations).</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def get_top_texts_for_neuron(self, neuron_idx: int, top_m: int | None = None) -&gt; list[NeuronText]:\n    \"\"\"Get top texts for a specific neuron (positive activations).\"\"\"\n    if self._text_heaps_positive is None or neuron_idx &lt; 0 or neuron_idx &gt;= len(self._text_heaps_positive):\n        return []\n    heap = self._text_heaps_positive[neuron_idx]\n    items = heap.get_items()\n    items_sorted = sorted(items, key=lambda s_t: s_t[0], reverse=True)\n    if top_m is not None:\n        items_sorted = items_sorted[: top_m]\n\n    neuron_texts = []\n    for score, text, token_idx in items_sorted:\n        token_str = self._decode_token(text, token_idx)\n        neuron_texts.append(NeuronText(score=score, text=text, token_idx=token_idx, token_str=token_str))\n    return neuron_texts\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.reset_top_texts","title":"reset_top_texts","text":"<pre><code>reset_top_texts()\n</code></pre> <p>Reset all tracked top texts.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def reset_top_texts(self) -&gt; None:\n    \"\"\"Reset all tracked top texts.\"\"\"\n    self._text_heaps_positive = None\n    self._text_heaps_negative = None\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.update_top_texts_from_latents","title":"update_top_texts_from_latents","text":"<pre><code>update_top_texts_from_latents(latents, texts, original_shape=None)\n</code></pre> <p>Update top texts heaps from latents and texts.</p> <p>Optimized version that: - Only processes active neurons (non-zero activations) - Vectorizes argmax/argmin operations - Eliminates per-neuron tensor slicing</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Latent activations tensor, shape [B*T, n_latents] or [B, n_latents] (already flattened)</p> required <code>texts</code> <code>Sequence[str]</code> <p>List of texts corresponding to the batch</p> required <code>original_shape</code> <code>tuple[int, ...] | None</code> <p>Original shape before flattening, e.g., (B, T, D) or (B, D)</p> <code>None</code> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def update_top_texts_from_latents(\n        self,\n        latents: torch.Tensor,\n        texts: Sequence[str],\n        original_shape: tuple[int, ...] | None = None\n) -&gt; None:\n    \"\"\"\n    Update top texts heaps from latents and texts.\n\n    Optimized version that:\n    - Only processes active neurons (non-zero activations)\n    - Vectorizes argmax/argmin operations\n    - Eliminates per-neuron tensor slicing\n\n    Args:\n        latents: Latent activations tensor, shape [B*T, n_latents] or [B, n_latents] (already flattened)\n        texts: List of texts corresponding to the batch\n        original_shape: Original shape before flattening, e.g., (B, T, D) or (B, D)\n    \"\"\"\n    if not texts:\n        return\n\n    n_neurons = latents.shape[-1]\n    self._ensure_heaps(n_neurons)\n\n    # Calculate batch and token dimensions\n    original_B = len(texts)\n    BT = latents.shape[0]  # Total positions (B*T if 3D original, or B if 2D original)\n\n    # Determine if original was 3D or 2D\n    if original_shape is not None and len(original_shape) == 3:\n        # Original was [B, T, D], latents are [B*T, n_latents]\n        B, T, _ = original_shape\n        # Verify batch size matches\n        if B != original_B:\n            logger.warning(f\"Batch size mismatch: original_shape has B={B}, but {original_B} texts provided\")\n            # Use the actual number of texts as batch size\n            B = original_B\n            T = BT // B if B &gt; 0 else 1\n    else:\n        # Original was [B, D], latents are [B, n_latents]\n        B = original_B\n        T = 1\n\n    # OPTIMIZATION 1: Find active neurons (have any non-zero activation across batch)\n    # Shape: [n_neurons] - boolean mask\n    active_neurons_mask = (latents.abs().sum(dim=0) &gt; 0)\n    active_neuron_indices = torch.nonzero(active_neurons_mask, as_tuple=False).flatten().tolist()\n\n    if not active_neuron_indices:\n        return  # No active neurons, skip\n\n    # OPTIMIZATION 2: Vectorize argmax/argmin for all neurons at once\n    if original_shape is not None and len(original_shape) == 3:\n        # Reshape to [B, T, n_neurons]\n        latents_3d = latents.view(B, T, n_neurons)\n        # For each text, find max/min across tokens for each neuron\n        # Shape: [B, n_neurons] - max activation per text per neuron\n        max_activations, max_token_indices_3d = latents_3d.max(dim=1)  # [B, n_neurons]\n        min_activations, min_token_indices_3d = latents_3d.min(dim=1)  # [B, n_neurons]\n        # max_token_indices_3d is already the token index (0 to T-1)\n        max_token_indices = max_token_indices_3d\n        min_token_indices = min_token_indices_3d\n    else:\n        # Shape: [B, n_neurons]\n        latents_2d = latents.view(B, n_neurons)\n        max_activations = latents_2d  # [B, n_neurons]\n        max_token_indices = torch.zeros(B, n_neurons, dtype=torch.long, device=latents.device)\n        min_activations = latents_2d\n        min_token_indices = torch.zeros(B, n_neurons, dtype=torch.long, device=latents.device)\n\n    # Convert to numpy for faster CPU access (already on CPU from l1_sae.py)\n    max_activations_np = max_activations.cpu().numpy()\n    min_activations_np = min_activations.cpu().numpy()\n    max_token_indices_np = max_token_indices.cpu().numpy()\n    min_token_indices_np = min_token_indices.cpu().numpy()\n\n    # OPTIMIZATION 3: Only process active neurons\n    for j in active_neuron_indices:\n        heap_positive = self._text_heaps_positive[j]\n        heap_negative = self._text_heaps_negative[j] if self._text_tracking_negative else None\n\n        # OPTIMIZATION 4: Batch process all texts for this neuron\n        for batch_idx in range(original_B):\n            if batch_idx &gt;= len(texts):\n                continue\n\n            text = texts[batch_idx]\n\n            # Use pre-computed max/min (no tensor slicing needed!)\n            max_score_positive = float(max_activations_np[batch_idx, j])\n            token_idx_positive = int(max_token_indices_np[batch_idx, j])\n\n            if max_score_positive &gt; 0.0:\n                heap_positive.update(text, max_score_positive, token_idx_positive)\n\n            if self._text_tracking_negative and heap_negative is not None:\n                min_score_negative = float(min_activations_np[batch_idx, j])\n                if min_score_negative != 0.0:\n                    token_idx_negative = int(min_token_indices_np[batch_idx, j])\n                    heap_negative.update(text, min_score_negative, token_idx_negative, adjusted_score=-min_score_negative)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.concept_dictionary.ConceptDictionary","title":"mi_crow.mechanistic.sae.concepts.concept_dictionary.ConceptDictionary","text":"<pre><code>ConceptDictionary(n_size, store=None)\n</code></pre> Source code in <code>src/mi_crow/mechanistic/sae/concepts/concept_dictionary.py</code> <pre><code>def __init__(\n        self,\n        n_size: int,\n        store: Store | None = None\n) -&gt; None:\n    self.n_size = n_size\n    self.concepts_map: Dict[int, Concept] = {}\n    self.store = store\n    self._directory: Path | None = None\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.concept_models","title":"mi_crow.mechanistic.sae.concepts.concept_models","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker","title":"mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker","text":"<pre><code>InputTracker(language_model)\n</code></pre> <p>Simple listener that saves input texts before tokenization.</p> <p>This is a singleton per LanguageModel instance. It's used as a listener during inference to capture texts before they are tokenized. SAE hooks can then access these texts to track top activating texts for their neurons.</p> <p>Initialize InputTracker.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>LanguageModel</code> <p>Language model instance</p> required Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def __init__(\n        self,\n        language_model: \"LanguageModel\",\n) -&gt; None:\n    \"\"\"\n    Initialize InputTracker.\n\n    Args:\n        language_model: Language model instance\n    \"\"\"\n    self.language_model = language_model\n\n    # Flag to control whether to save inputs\n    self._enabled: bool = False\n\n    # Runtime state - only stores texts\n    self._current_texts: list[str] = []\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.enabled","title":"enabled  <code>property</code>","text":"<pre><code>enabled\n</code></pre> <p>Whether input tracking is enabled.</p>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.disable","title":"disable","text":"<pre><code>disable()\n</code></pre> <p>Disable input tracking.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def disable(self) -&gt; None:\n    \"\"\"Disable input tracking.\"\"\"\n    self._enabled = False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.enable","title":"enable","text":"<pre><code>enable()\n</code></pre> <p>Enable input tracking.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def enable(self) -&gt; None:\n    \"\"\"Enable input tracking.\"\"\"\n    self._enabled = True\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.get_current_texts","title":"get_current_texts","text":"<pre><code>get_current_texts()\n</code></pre> <p>Get the current batch of texts.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def get_current_texts(self) -&gt; list[str]:\n    \"\"\"Get the current batch of texts.\"\"\"\n    return self._current_texts.copy()\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset stored texts.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset stored texts.\"\"\"\n    self._current_texts.clear()\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.set_current_texts","title":"set_current_texts","text":"<pre><code>set_current_texts(texts)\n</code></pre> <p>Set the current batch of texts being processed.</p> <p>This is called by LanguageModel._inference() before tokenization if tracking is enabled.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def set_current_texts(self, texts: Sequence[str]) -&gt; None:\n    \"\"\"\n    Set the current batch of texts being processed.\n\n    This is called by LanguageModel._inference() before tokenization\n    if tracking is enabled.\n    \"\"\"\n    if self._enabled:\n        self._current_texts = list(texts)\n</code></pre>"},{"location":"api/sae/#training-utilities","title":"Training Utilities","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger","title":"mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger","text":"<pre><code>WandbLogger(config, run_id)\n</code></pre> <p>Handles wandb logging for SAE training.</p> <p>Encapsulates all wandb-related operations including initialization, metric logging, and summary updates.</p> <p>Initialize WandbLogger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SaeTrainingConfig</code> <p>Training configuration</p> required <code>run_id</code> <code>str</code> <p>Training run identifier</p> required Source code in <code>src/mi_crow/mechanistic/sae/training/wandb_logger.py</code> <pre><code>def __init__(self, config: SaeTrainingConfig, run_id: str):\n    \"\"\"\n    Initialize WandbLogger.\n\n    Args:\n        config: Training configuration\n        run_id: Training run identifier\n    \"\"\"\n    self.config = config\n    self.run_id = run_id\n    self.wandb_run: Optional[Any] = None\n    self._initialized = False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize wandb run if enabled in config.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if wandb was successfully initialized, False otherwise</p> Source code in <code>src/mi_crow/mechanistic/sae/training/wandb_logger.py</code> <pre><code>def initialize(self) -&gt; bool:\n    \"\"\"\n    Initialize wandb run if enabled in config.\n\n    Returns:\n        True if wandb was successfully initialized, False otherwise\n    \"\"\"\n    if not self.config.use_wandb:\n        return False\n\n    try:\n        import wandb\n    except ImportError:\n        logger.warning(\"[WandbLogger] wandb not installed, skipping wandb logging\")\n        logger.warning(\"[WandbLogger] Install with: pip install wandb\")\n        return False\n\n    try:\n        wandb_project = self.config.wandb_project or \"sae-training\"\n        wandb_name = self.config.wandb_name or self.run_id\n        wandb_mode = self.config.wandb_mode.lower() if self.config.wandb_mode else \"online\"\n\n        self.wandb_run = wandb.init(\n            project=wandb_project,\n            entity=self.config.wandb_entity,\n            name=wandb_name,\n            mode=wandb_mode,\n            config=self._build_wandb_config(),\n            tags=self.config.wandb_tags or [],\n        )\n        self._initialized = True\n        return True\n    except Exception as e:\n        logger.warning(f\"[WandbLogger] Unexpected error initializing wandb: {e}\")\n        logger.warning(\"[WandbLogger] Continuing training without wandb logging\")\n        return False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(history, verbose=False)\n</code></pre> <p>Log training metrics to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>dict[str, list[float | None]]</code> <p>Dictionary with training history (loss, r2, l1, l0, etc.)</p> required <code>verbose</code> <code>bool</code> <p>Whether to log verbose information</p> <code>False</code> Source code in <code>src/mi_crow/mechanistic/sae/training/wandb_logger.py</code> <pre><code>def log_metrics(\n        self,\n        history: dict[str, list[float | None]],\n        verbose: bool = False\n) -&gt; None:\n    \"\"\"\n    Log training metrics to wandb.\n\n    Args:\n        history: Dictionary with training history (loss, r2, l1, l0, etc.)\n        verbose: Whether to log verbose information\n    \"\"\"\n    if not self._initialized or self.wandb_run is None:\n        return\n\n    try:\n        num_epochs = len(history.get(\"loss\", []))\n        slow_metrics_freq = self.config.wandb_slow_metrics_frequency\n\n        # Helper to get last known value for slow metrics\n        def get_last_known_value(values: list[float | None], idx: int) -&gt; float:\n            \"\"\"Get the last non-None value up to idx, or 0.0 if none found.\"\"\"\n            for i in range(idx, -1, -1):\n                if i &lt; len(values) and values[i] is not None:\n                    return values[i]\n            return 0.0\n\n        # Log metrics for each epoch\n        for epoch in range(1, num_epochs + 1):\n            epoch_idx = epoch - 1\n            should_log_slow = (epoch % slow_metrics_freq == 0) or (epoch == num_epochs)\n\n            metrics = self._build_epoch_metrics(history, epoch_idx, should_log_slow, get_last_known_value)\n            self.wandb_run.log(metrics)\n\n        # Log final summary metrics\n        self._log_summary_metrics(history, get_last_known_value)\n\n        if verbose:\n            self._log_wandb_url()\n\n    except Exception as e:\n        logger.warning(f\"[WandbLogger] Failed to log metrics to wandb: {e}\")\n</code></pre>"},{"location":"api/store/","title":"Store API","text":"<p>Persistence layer for activations, models, and runs.</p>"},{"location":"api/store/#mi_crow.store","title":"mi_crow.store","text":""},{"location":"api/store/#mi_crow.store.LocalStore","title":"LocalStore","text":"<pre><code>LocalStore(base_path='', runs_prefix='runs', dataset_prefix='datasets', model_prefix='models')\n</code></pre> <p>               Bases: <code>Store</code></p> <p>Local filesystem implementation of Store interface.</p> <p>Initialize LocalStore.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path | str</code> <p>Base directory path for the store</p> <code>''</code> <code>runs_prefix</code> <code>str</code> <p>Prefix for runs directory</p> <code>'runs'</code> <code>dataset_prefix</code> <code>str</code> <p>Prefix for datasets directory</p> <code>'datasets'</code> <code>model_prefix</code> <code>str</code> <p>Prefix for models directory</p> <code>'models'</code> Source code in <code>src/mi_crow/store/local_store.py</code> <pre><code>def __init__(\n        self,\n        base_path: Path | str = '',\n        runs_prefix: str = \"runs\",\n        dataset_prefix: str = \"datasets\",\n        model_prefix: str = \"models\",\n):\n    \"\"\"Initialize LocalStore.\n\n    Args:\n        base_path: Base directory path for the store\n        runs_prefix: Prefix for runs directory\n        dataset_prefix: Prefix for datasets directory\n        model_prefix: Prefix for models directory\n    \"\"\"\n    super().__init__(base_path, runs_prefix, dataset_prefix, model_prefix)\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store","title":"Store","text":"<pre><code>Store(base_path='', runs_prefix='runs', dataset_prefix='datasets', model_prefix='models')\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract store optimized for tensor batches grouped by run_id.</p> <p>This interface intentionally excludes generic bytes/JSON APIs. Implementations should focus on efficient safetensors-backed IO.</p> <p>The store organizes data hierarchically: - Runs: Top-level grouping by run_id - Batches: Within each run, data is organized by batch_index - Layers: Within each batch, tensors are organized by layer_signature - Keys: Within each layer, tensors are identified by key (e.g., \"activations\")</p> <p>Initialize Store.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path | str</code> <p>Base directory path for the store</p> <code>''</code> <code>runs_prefix</code> <code>str</code> <p>Prefix for runs directory (default: \"runs\")</p> <code>'runs'</code> <code>dataset_prefix</code> <code>str</code> <p>Prefix for datasets directory (default: \"datasets\")</p> <code>'datasets'</code> <code>model_prefix</code> <code>str</code> <p>Prefix for models directory (default: \"models\")</p> <code>'models'</code> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>def __init__(\n        self,\n        base_path: Path | str = \"\",\n        runs_prefix: str = \"runs\",\n        dataset_prefix: str = \"datasets\",\n        model_prefix: str = \"models\",\n):\n    \"\"\"Initialize Store.\n\n    Args:\n        base_path: Base directory path for the store\n        runs_prefix: Prefix for runs directory (default: \"runs\")\n        dataset_prefix: Prefix for datasets directory (default: \"datasets\")\n        model_prefix: Prefix for models directory (default: \"models\")\n    \"\"\"\n    self.runs_prefix = runs_prefix\n    self.dataset_prefix = dataset_prefix\n    self.model_prefix = model_prefix\n    self.base_path = Path(base_path)\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.get_detector_metadata","title":"get_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>get_detector_metadata(run_id, batch_index)\n</code></pre> <p>Load detector metadata with separate JSON and tensor store.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>tuple[Dict[str, Any], TensorMetadata]</code> <p>Tuple of (metadata dict, tensor_metadata dict). Returns empty dicts if not found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata format is invalid</p> <code>JSONDecodeError</code> <p>If metadata file exists but contains invalid JSON</p> <code>OSError</code> <p>If tensor files exist but cannot be loaded</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_detector_metadata(\n        self,\n        run_id: str,\n        batch_index: int\n) -&gt; tuple[Dict[str, Any], TensorMetadata]:\n    \"\"\"Load detector metadata with separate JSON and tensor store.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index\n\n    Returns:\n        Tuple of (metadata dict, tensor_metadata dict). Returns empty dicts if not found.\n\n    Raises:\n        ValueError: If parameters are invalid or metadata format is invalid\n        json.JSONDecodeError: If metadata file exists but contains invalid JSON\n        OSError: If tensor files exist but cannot be loaded\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.get_detector_metadata_by_layer_by_key","title":"get_detector_metadata_by_layer_by_key  <code>abstractmethod</code>","text":"<pre><code>get_detector_metadata_by_layer_by_key(run_id, batch_index, layer, key)\n</code></pre> <p>Get a specific tensor from detector metadata by layer and key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index</p> required <code>layer</code> <code>str</code> <p>Layer signature</p> required <code>key</code> <code>str</code> <p>Tensor key (e.g., \"activations\")</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The requested tensor</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>FileNotFoundError</code> <p>If the tensor doesn't exist</p> <code>OSError</code> <p>If tensor file exists but cannot be loaded</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_detector_metadata_by_layer_by_key(\n        self,\n        run_id: str,\n        batch_index: int,\n        layer: str,\n        key: str\n) -&gt; torch.Tensor:\n    \"\"\"Get a specific tensor from detector metadata by layer and key.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index\n        layer: Layer signature\n        key: Tensor key (e.g., \"activations\")\n\n    Returns:\n        The requested tensor\n\n    Raises:\n        ValueError: If parameters are invalid\n        FileNotFoundError: If the tensor doesn't exist\n        OSError: If tensor file exists but cannot be loaded\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.get_run_metadata","title":"get_run_metadata  <code>abstractmethod</code>","text":"<pre><code>get_run_metadata(run_id)\n</code></pre> <p>Load metadata for a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Metadata dictionary, or empty dict if not found</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_id is invalid</p> <code>JSONDecodeError</code> <p>If metadata file exists but contains invalid JSON</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_run_metadata(self, run_id: str) -&gt; Dict[str, Any]:\n    \"\"\"Load metadata for a run.\n\n    Args:\n        run_id: Run identifier\n\n    Returns:\n        Metadata dictionary, or empty dict if not found\n\n    Raises:\n        ValueError: If run_id is invalid\n        json.JSONDecodeError: If metadata file exists but contains invalid JSON\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.iter_run_batch_range","title":"iter_run_batch_range","text":"<pre><code>iter_run_batch_range(run_id, *, start=0, stop=None, step=1)\n</code></pre> <p>Iterate run batches for indices in range(start, stop, step).</p> <p>If stop is None, it will be set to max(list_run_batches(run_id)) + 1 (or 0 if none). Raises ValueError if step == 0 or start &lt; 0.</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>def iter_run_batch_range(\n        self,\n        run_id: str,\n        *,\n        start: int = 0,\n        stop: int | None = None,\n        step: int = 1,\n) -&gt; Iterator[List[torch.Tensor] | Dict[str, torch.Tensor]]:\n    \"\"\"Iterate run batches for indices in range(start, stop, step).\n\n    If stop is None, it will be set to max(list_run_batches(run_id)) + 1 (or 0 if none).\n    Raises ValueError if step == 0 or start &lt; 0.\n    \"\"\"\n    if step == 0:\n        raise ValueError(\"step must not be 0\")\n    if start &lt; 0:\n        raise ValueError(\"start must be &gt;= 0\")\n    indices = self.list_run_batches(run_id)\n    if not indices:\n        return\n    max_idx = max(indices)\n    if stop is None:\n        stop = max_idx + 1\n    for idx in range(start, stop, step):\n        try:\n            yield self.get_run_batch(run_id, idx)\n        except FileNotFoundError:\n            continue\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.put_detector_metadata","title":"put_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>put_detector_metadata(run_id, batch_index, metadata, tensor_metadata)\n</code></pre> <p>Save detector metadata with separate JSON and tensor store.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index (must be non-negative)</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>JSON-serializable metadata dictionary (aggregated from all detectors)</p> required <code>tensor_metadata</code> <code>TensorMetadata</code> <p>Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor            (from all detectors)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path key used for store (e.g., \"runs/{run_id}/batch_{batch_index}\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata is not JSON-serializable</p> <code>OSError</code> <p>If file system operations fail</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_detector_metadata(\n        self,\n        run_id: str,\n        batch_index: int,\n        metadata: Dict[str, Any],\n        tensor_metadata: TensorMetadata\n) -&gt; str:\n    \"\"\"Save detector metadata with separate JSON and tensor store.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index (must be non-negative)\n        metadata: JSON-serializable metadata dictionary (aggregated from all detectors)\n        tensor_metadata: Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor\n                       (from all detectors)\n\n    Returns:\n        Full path key used for store (e.g., \"runs/{run_id}/batch_{batch_index}\")\n\n    Raises:\n        ValueError: If parameters are invalid or metadata is not JSON-serializable\n        OSError: If file system operations fail\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.put_run_detector_metadata","title":"put_run_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>put_run_detector_metadata(run_id, metadata, tensor_metadata)\n</code></pre> <p>Save detector metadata for a whole run in a unified location.</p> <p>This differs from <code>put_detector_metadata</code> which organises data per-batch under <code>runs/{run_id}/batch_{batch_index}</code>.</p> <p><code>put_run_detector_metadata</code> instead stores everything under <code>runs/{run_id}/detectors</code>. Implementations are expected to support being called multiple times for the same <code>run_id</code> and append / aggregate new metadata rather than overwrite it.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>JSON-serialisable metadata dictionary aggregated from all detectors for the current chunk / batch.</p> required <code>tensor_metadata</code> <code>TensorMetadata</code> <p>Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor (from all detectors).</p> required <p>Returns:</p> Type Description <code>str</code> <p>String path/key where metadata was stored</p> <code>str</code> <p>(e.g. <code>runs/{run_id}/detectors</code>).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata is not JSON\u2011serialisable.</p> <code>OSError</code> <p>If file system operations fail.</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_run_detector_metadata(\n        self,\n        run_id: str,\n        metadata: Dict[str, Any],\n        tensor_metadata: TensorMetadata,\n) -&gt; str:\n    \"\"\"\n    Save detector metadata for a whole run in a unified location.\n\n    This differs from ``put_detector_metadata`` which organises data\n    per-batch under ``runs/{run_id}/batch_{batch_index}``.\n\n    ``put_run_detector_metadata`` instead stores everything under\n    ``runs/{run_id}/detectors``. Implementations are expected to\n    support being called multiple times for the same ``run_id`` and\n    append / aggregate new metadata rather than overwrite it.\n\n    Args:\n        run_id: Run identifier\n        metadata: JSON-serialisable metadata dictionary aggregated\n            from all detectors for the current chunk / batch.\n        tensor_metadata: Dictionary mapping layer_signature to dict\n            of tensor_key -&gt; tensor (from all detectors).\n\n    Returns:\n        String path/key where metadata was stored\n        (e.g. ``runs/{run_id}/detectors``).\n\n    Raises:\n        ValueError: If parameters are invalid or metadata is not\n            JSON\u2011serialisable.\n        OSError: If file system operations fail.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.put_run_metadata","title":"put_run_metadata  <code>abstractmethod</code>","text":"<pre><code>put_run_metadata(run_id, meta)\n</code></pre> <p>Persist metadata for a run (e.g., dataset/model identifiers).</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>meta</code> <code>Dict[str, Any]</code> <p>Metadata dictionary to save (must be JSON-serializable)</p> required <p>Returns:</p> Type Description <code>str</code> <p>String path/key where metadata was stored (e.g., \"runs/{run_id}/meta.json\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_id is invalid or meta is not JSON-serializable</p> <code>OSError</code> <p>If file system operations fail</p> Note <p>Implementations should store JSON at a stable location, e.g., runs/{run_id}/meta.json.</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_run_metadata(self, run_id: str, meta: Dict[str, Any]) -&gt; str:\n    \"\"\"Persist metadata for a run (e.g., dataset/model identifiers).\n\n    Args:\n        run_id: Run identifier\n        meta: Metadata dictionary to save (must be JSON-serializable)\n\n    Returns:\n        String path/key where metadata was stored (e.g., \"runs/{run_id}/meta.json\")\n\n    Raises:\n        ValueError: If run_id is invalid or meta is not JSON-serializable\n        OSError: If file system operations fail\n\n    Note:\n        Implementations should store JSON at a stable location, e.g., runs/{run_id}/meta.json.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"experiments/","title":"Experiments","text":"<p>This section provides detailed walkthroughs of sample experiments demonstrating real-world usage of mi-crow.</p>"},{"location":"experiments/#overview","title":"Overview","text":"<p>Experiments showcase complete workflows from data collection through analysis, using real models and datasets. They demonstrate best practices and provide templates for your own research.</p>"},{"location":"experiments/#available-experiments","title":"Available Experiments","text":""},{"location":"experiments/#verify-sae-training","title":"Verify SAE Training","text":"<p>Complete workflow for training and validating SAE models on the Bielik model using TinyStories dataset.</p> <p>What it covers: - Saving activations from a production model - Training SAEs with proper hyperparameters - Validating training success - Concept discovery and naming - Analysis and visualization</p> <p>Time required: Several hours (depending on hardware)</p> <p>Prerequisites: - Access to Bielik model or similar - Sufficient GPU memory - Understanding of basic SAE concepts</p>"},{"location":"experiments/#slurm-sae-pipeline","title":"SLURM SAE Pipeline","text":"<p>Distributed training setup for large-scale SAE training on cluster environments.</p> <p>What it covers: - SLURM job configuration - Distributed activation saving - Large-scale SAE training - Resource management</p> <p>Time required: Days (cluster-dependent)</p> <p>Prerequisites: - Access to SLURM cluster - Understanding of cluster computing - Large-scale dataset</p>"},{"location":"experiments/#experiment-structure","title":"Experiment Structure","text":"<p>Each experiment typically includes:</p> <ol> <li>Setup: Environment and dependencies</li> <li>Data Collection: Saving activations</li> <li>Training: SAE model training</li> <li>Validation: Verifying results</li> <li>Analysis: Understanding outcomes</li> <li>Documentation: Recording findings</li> </ol>"},{"location":"experiments/#running-experiments","title":"Running Experiments","text":""},{"location":"experiments/#prerequisites","title":"Prerequisites","text":"<pre><code># Install dependencies\npip install -e .\n\n# Or with uv\nuv sync\n</code></pre>"},{"location":"experiments/#basic-workflow","title":"Basic Workflow","text":"<pre><code># 1. Navigate to experiment directory\ncd experiments/verify_sae_training\n\n# 2. Review README\ncat README.md\n\n# 3. Run scripts in order\npython 01_save_activations.py\npython 02_train_sae.py\n\n# 4. Open analysis notebooks\njupyter notebook 03_analyze_training.ipynb\n</code></pre>"},{"location":"experiments/#customization","title":"Customization","text":"<p>Experiments are designed to be customizable:</p> <ul> <li>Modify model names</li> <li>Adjust hyperparameters</li> <li>Change dataset sources</li> <li>Adapt to your hardware</li> </ul>"},{"location":"experiments/#experiment-outputs","title":"Experiment Outputs","text":"<p>Experiments produce:</p> <ul> <li>Saved activations: Organized in store</li> <li>Trained models: SAE checkpoints</li> <li>Analysis results: Visualizations and metrics</li> <li>Documentation: Findings and observations</li> </ul>"},{"location":"experiments/#best-practices","title":"Best Practices","text":"<ol> <li>Start small: Test with limited data first</li> <li>Monitor resources: Watch memory and compute usage</li> <li>Document changes: Record any modifications</li> <li>Save checkpoints: Don't lose progress</li> <li>Validate results: Verify outputs make sense</li> </ol>"},{"location":"experiments/#contributing-experiments","title":"Contributing Experiments","text":"<p>If you create a new experiment:</p> <ol> <li>Create directory in <code>experiments/</code></li> <li>Include README with description</li> <li>Provide runnable scripts/notebooks</li> <li>Document setup and requirements</li> <li>Share findings and observations</li> </ol>"},{"location":"experiments/#next-steps","title":"Next Steps","text":"<ul> <li>Verify SAE Training - Start with this experiment</li> <li>User Guide - Learn fundamentals first</li> <li>Examples - Try examples before experiments</li> </ul>"},{"location":"experiments/slurm-pipeline/","title":"SLURM SAE Pipeline","text":"<p>This experiment demonstrates distributed training setup for large-scale SAE training on cluster environments using SLURM.</p>"},{"location":"experiments/slurm-pipeline/#overview","title":"Overview","text":"<p>This pipeline shows how to: - Configure SLURM jobs for activation saving - Set up distributed SAE training on clusters - Manage resources and job dependencies - Handle large-scale datasets efficiently</p>"},{"location":"experiments/slurm-pipeline/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to SLURM cluster</li> <li>Understanding of cluster computing</li> <li>Large-scale dataset</li> <li>Sufficient cluster resources</li> </ul>"},{"location":"experiments/slurm-pipeline/#experiment-structure","title":"Experiment Structure","text":"<pre><code>slurm_sae_pipeline/\n\u251c\u2500\u2500 01_save_activations.py      # Activation saving script\n\u251c\u2500\u2500 02_train_sae.py              # SAE training script\n\u251c\u2500\u2500 submit_save_activations.sh   # SLURM submission script for activations\n\u251c\u2500\u2500 submit_train_sae.sh         # SLURM submission script for training\n\u2514\u2500\u2500 README.md                    # Pipeline documentation\n</code></pre>"},{"location":"experiments/slurm-pipeline/#configuration","title":"Configuration","text":""},{"location":"experiments/slurm-pipeline/#slurm-job-configuration","title":"SLURM Job Configuration","text":"<p>The submission scripts configure: - Resources: GPU allocation, memory, time limits - Dependencies: Job ordering (train after save) - Environment: Python environment setup - Output: Logging and error handling</p>"},{"location":"experiments/slurm-pipeline/#mi-crow-configuration","title":"mi-crow Configuration","text":"<p>The Python scripts use standard mi-crow APIs: - <code>lm.activations.save()</code> for distributed activation saving - <code>SaeTrainer.train()</code> for SAE training - Store configuration for cluster filesystems</p>"},{"location":"experiments/slurm-pipeline/#workflow","title":"Workflow","text":"<ol> <li>Submit activation saving job: Uses <code>submit_save_activations.sh</code></li> <li>Wait for completion: Activation saving must complete first</li> <li>Submit training job: Uses <code>submit_train_sae.sh</code> (depends on step 1)</li> <li>Monitor jobs: Track progress through SLURM</li> </ol>"},{"location":"experiments/slurm-pipeline/#key-features","title":"Key Features","text":"<ul> <li>Distributed processing: Handles large datasets across cluster nodes</li> <li>Resource management: Proper GPU and memory allocation</li> <li>Job dependencies: Ensures correct execution order</li> <li>Error handling: Robust failure recovery</li> </ul>"},{"location":"experiments/slurm-pipeline/#customization","title":"Customization","text":"<p>Adapt the pipeline for your cluster: - Modify resource requests in submission scripts - Adjust batch sizes for available memory - Configure store paths for cluster filesystem - Set appropriate time limits</p>"},{"location":"experiments/slurm-pipeline/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training SAE Models - SAE training guide</li> <li>Saving Activations - Activation saving guide</li> <li>Best Practices - Performance optimization</li> </ul>"},{"location":"experiments/verify-sae-training/","title":"Verify SAE Training Experiment","text":"<p>This experiment demonstrates a complete workflow for training and validating SAE models on the Bielik model using the TinyStories dataset.</p>"},{"location":"experiments/verify-sae-training/#overview","title":"Overview","text":"<p>This experiment walks through: 1. Saving activations from a production model 2. Training SAEs with proper hyperparameters 3. Validating training success 4. Concept discovery and naming 5. Analysis and visualization</p>"},{"location":"experiments/verify-sae-training/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>PyTorch</li> <li>Required packages: <code>mi_crow</code>, <code>torch</code>, <code>transformers</code>, <code>datasets</code>, <code>overcomplete</code>, <code>matplotlib</code>, <code>seaborn</code></li> <li>Access to Bielik model or similar</li> <li>Sufficient GPU memory (or use CPU for smaller experiments)</li> </ul>"},{"location":"experiments/verify-sae-training/#experiment-structure","title":"Experiment Structure","text":"<pre><code>verify_sae_training/\n\u251c\u2500\u2500 01_save_activations.py      # Step 1: Save activations from dataset\n\u251c\u2500\u2500 02_train_sae.py              # Step 2: Train SAE model\n\u251c\u2500\u2500 03_analyze_training.ipynb   # Step 3: Analyze training metrics and verify learning\n\u251c\u2500\u2500 04_name_sae_concepts.ipynb  # Step 4: Export top texts for each neuron\n\u251c\u2500\u2500 05_show_concepts.ipynb       # Step 5: Display and explore concepts\n\u251c\u2500\u2500 observations.md              # Findings and observations\n\u2514\u2500\u2500 README.md                    # Experiment documentation\n</code></pre>"},{"location":"experiments/verify-sae-training/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"experiments/verify-sae-training/#step-1-save-activations","title":"Step 1: Save Activations","text":"<p>File: <code>01_save_activations.py</code></p> <p>This script: - Loads the Bielik model - Uses resid_mid layer (post_attention_layernorm) at layer 16 - Loads TinyStories dataset - Saves activations from the specified layer - Stores run ID in <code>store/run_id.txt</code></p> <p>Configuration: - Model: <code>speakleash/Bielik-1.5B-v3.0-Instruct</code> - Dataset: <code>roneneldan/TinyStories</code> (train split) - Layer: <code>llamaforcausallm_model_layers_16_post_attention_layernorm</code> - Store location: <code>experiments/verify_sae_training/store/</code></p> <p>To change the layer: Edit <code>LAYER_SIGNATURE</code> in the script (e.g., use <code>_0_</code> for first layer, <code>_31_</code> for last layer).</p> <p>Run: <pre><code>cd experiments/verify_sae_training\npython 01_save_activations.py\n</code></pre></p>"},{"location":"experiments/verify-sae-training/#step-2-train-sae","title":"Step 2: Train SAE","text":"<p>File: <code>02_train_sae.py</code></p> <p>This script: - Loads the saved activations - Creates a TopKSAE model - Trains the SAE on the activations - Saves the trained model to <code>store/sae_model/topk_sae.pt</code> - Saves training history to <code>store/training_history.json</code></p> <p>Configuration: - <code>N_LATENTS_MULTIPLIER</code>: Overcompleteness factor (default: 4x) - <code>TOP_K</code>: Sparsity parameter (default: 8) - <code>EPOCHS</code>: Number of training epochs (default: 100) - <code>BATCH_SIZE_TRAIN</code>: Training batch size (default: 1024)</p> <p>Run: <pre><code>python 02_train_sae.py\n</code></pre></p>"},{"location":"experiments/verify-sae-training/#step-3-analyze-training","title":"Step 3: Analyze Training","text":"<p>File: <code>03_analyze_training.ipynb</code></p> <p>This notebook demonstrates: - Accessing training history from <code>store/training_history.json</code> - Visualizing training metrics (loss, R\u00b2, L0, dead features) - Validating SAE training success using mi-crow APIs - Analyzing reconstruction quality</p> <p>Key validation checks: - Loss should decrease over time - R\u00b2 should increase (better reconstruction) - L0 should match expected TopK sparsity - Dead features should be minimal - Weight variance should be significant</p>"},{"location":"experiments/verify-sae-training/#step-4-export-top-texts","title":"Step 4: Export Top Texts","text":"<p>File: <code>04_name_sae_concepts.ipynb</code></p> <p>This notebook demonstrates: - Loading trained SAE using mi-crow APIs - Attaching SAE to language model with <code>lm.attach_sae()</code> - Enabling text tracking with <code>sae.concepts.enable_text_tracking()</code> - Collecting top texts using mi-crow's concept discovery features - Exporting results to JSON format</p> <p>Output: JSON file mapping neuron indices to top activating text snippets.</p>"},{"location":"experiments/verify-sae-training/#step-5-show-concepts","title":"Step 5: Show Concepts","text":"<p>File: <code>05_show_concepts.ipynb</code></p> <p>This notebook demonstrates: - Loading exported top texts - Using mi-crow APIs to access concept data - Analyzing neuron activation patterns - Exploring concept relationships</p>"},{"location":"experiments/verify-sae-training/#expected-outputs","title":"Expected Outputs","text":"<p>After running all steps, you'll have:</p> <ul> <li><code>store/run_id.txt</code> - Run ID for the activation saving run</li> <li><code>store/runs/&lt;run_id&gt;/</code> - Saved activations</li> <li><code>store/sae_model/topk_sae.pt</code> - Trained SAE model</li> <li><code>store/training_history.json</code> - Training metrics</li> <li><code>store/top_texts.json</code> - Exported top texts for each neuron</li> </ul>"},{"location":"experiments/verify-sae-training/#analysis-and-validation","title":"Analysis and Validation","text":""},{"location":"experiments/verify-sae-training/#training-metrics","title":"Training Metrics","text":"<p>Check that: - Loss decreases over epochs - R\u00b2 score improves (target: &gt; 0.9) - L0 matches expected TopK - Dead features &lt; 10% of total</p>"},{"location":"experiments/verify-sae-training/#concept-quality","title":"Concept Quality","text":"<p>Verify that: - Top texts show coherent patterns - Neurons detect meaningful concepts - Concepts are interpretable - Multiple neurons may detect similar concepts (redundancy is normal)</p>"},{"location":"experiments/verify-sae-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"experiments/verify-sae-training/#layer-signature-not-found","title":"Layer Signature Not Found","text":"<p>If you get an error about layer signature: 1. Run <code>01_save_activations.py</code> first to see available layers 2. Copy one of the layer names 3. Set <code>LAYER_SIGNATURE</code> in <code>01_save_activations.py</code></p>"},{"location":"experiments/verify-sae-training/#out-of-memory","title":"Out of Memory","text":"<p>If you run out of memory: - Reduce <code>DATA_LIMIT</code> in <code>01_save_activations.py</code> - Reduce <code>BATCH_SIZE_SAVE</code> in <code>01_save_activations.py</code> - Reduce <code>BATCH_SIZE_TRAIN</code> in <code>02_train_sae.py</code> - Use CPU instead of GPU (set <code>DEVICE = \"cpu\"</code>)</p>"},{"location":"experiments/verify-sae-training/#missing-dependencies","title":"Missing Dependencies","text":"<p>Install missing packages: <pre><code>pip install torch transformers datasets overcomplete matplotlib seaborn\n</code></pre></p>"},{"location":"experiments/verify-sae-training/#notes","title":"Notes","text":"<ul> <li>The experiment uses a relatively small dataset (1000 samples) for quick testing</li> <li>For production use, increase <code>DATA_LIMIT</code> and training epochs</li> <li>See <code>observations.md</code> for findings and missing functionality notes</li> </ul>"},{"location":"experiments/verify-sae-training/#related-documentation","title":"Related Documentation","text":"<ul> <li>Training SAE Models - Detailed training guide</li> <li>Concept Discovery - Concept discovery workflow</li> <li>Best Practices - General best practices</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"experiments/verify-sae-training/#next-steps","title":"Next Steps","text":"<p>After completing this experiment: - Try different layers - Experiment with hyperparameters - Scale up to larger datasets - Create your own experiments</p>"},{"location":"guide/","title":"User Guide","text":"<p>Welcome to the mi-crow user guide! This comprehensive guide will help you understand and use the mi-crow library for mechanistic interpretability research.</p>"},{"location":"guide/#what-is-mi-crow","title":"What is mi-crow?","text":"<p>mi-crow is a Python package for explaining and steering LLM behavior using Sparse Autoencoders (SAE) and concepts. It provides a complete toolkit for:</p> <ul> <li>Activation Analysis: Save and analyze model activations from any layer</li> <li>SAE Training: Train sparse autoencoders to discover interpretable features</li> <li>Concept Discovery: Identify and name concepts learned by SAE neurons</li> <li>Model Steering: Manipulate model behavior through concept-based interventions</li> <li>Hook System: Flexible system for intercepting and modifying activations</li> </ul>"},{"location":"guide/#what-is-mechanistic-interpretability","title":"What is Mechanistic Interpretability?","text":"<p>Mechanistic interpretability is the study of understanding how neural networks work by reverse-engineering their internal computations. In the context of language models, this means:</p> <ul> <li>Understanding what features the model learns at different layers</li> <li>Identifying how these features combine to produce outputs</li> <li>Discovering interpretable concepts that correspond to human-understandable ideas</li> <li>Using this understanding to control and improve model behavior</li> </ul>"},{"location":"guide/#library-capabilities","title":"Library Capabilities","text":"<p>mi-crow provides a modular architecture for mechanistic interpretability research:</p> <ul> <li>Language Model Wrapper: Easy loading and inference with HuggingFace models</li> <li>Sparse Autoencoders: Train and use SAEs to discover interpretable features</li> <li>Hooks System: Powerful framework for observing and modifying activations</li> <li>Store: Hierarchical storage for activations, models, and metadata</li> <li>Datasets: Flexible data loading from HuggingFace or local files</li> </ul>"},{"location":"guide/#getting-started","title":"Getting Started","text":"<ol> <li>Installation - Set up mi-crow and its dependencies</li> <li>Quick Start - Run your first example in minutes</li> <li>Core Concepts - Understand the fundamental ideas</li> <li>Hooks System - Learn about the powerful hooks framework</li> <li>Workflows - Step-by-step guides for common tasks</li> </ol>"},{"location":"guide/#documentation-structure","title":"Documentation Structure","text":""},{"location":"guide/#core-documentation","title":"Core Documentation","text":"<ul> <li>Installation &amp; Setup - Installation and environment configuration</li> <li>Quick Start - Get up and running quickly</li> <li>Core Concepts - Fundamental concepts and architecture</li> </ul>"},{"location":"guide/#hooks-system","title":"Hooks System","text":"<p>The hooks system is the foundation of mi-crow's interpretability capabilities:</p> <ul> <li>Hooks Overview - Introduction to the hooks system</li> <li>Hooks Fundamentals - Core concepts and lifecycle</li> <li>Detector Hooks - Observing activations without modification</li> <li>Controller Hooks - Modifying activations during inference</li> <li>Hook Registration - Managing hooks on layers</li> <li>Advanced Hooks - Advanced patterns and best practices</li> </ul>"},{"location":"guide/#workflows","title":"Workflows","text":"<p>Step-by-step guides for common tasks:</p> <ul> <li>Workflows Overview - When to use each workflow</li> <li>Saving Activations - Collect activation data</li> <li>Training SAE Models - Train sparse autoencoders</li> <li>Concept Discovery - Find interpretable concepts</li> <li>Concept Manipulation - Control model behavior</li> <li>Activation Control - Direct activation manipulation</li> </ul>"},{"location":"guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>Best Practices - Tips for effective research</li> <li>Troubleshooting - Common issues and solutions</li> <li>Examples - Example notebooks and learning path</li> <li>Experiments - Detailed experiment walkthroughs</li> </ul>"},{"location":"guide/#next-steps","title":"Next Steps","text":"<p>If you're new to mi-crow, we recommend following this path:</p> <ol> <li>Start with Installation to set up your environment</li> <li>Run through the Quick Start tutorial</li> <li>Read Core Concepts to understand the fundamentals</li> <li>Explore the Hooks System - it's central to everything</li> <li>Try a Workflow that matches your research goals</li> <li>Check out Examples for more detailed code</li> </ol> <p>For API reference, see the API Documentation.</p>"},{"location":"guide/best-practices/","title":"Best Practices","text":"<p>This guide covers best practices for using mi-crow effectively in your research.</p>"},{"location":"guide/best-practices/#model-selection-for-experimentation","title":"Model Selection for Experimentation","text":""},{"location":"guide/best-practices/#start-small","title":"Start Small","text":"<ul> <li>Use small models (e.g., <code>sshleifer/tiny-gpt2</code>) for initial experiments</li> <li>Faster iteration and lower memory requirements</li> <li>Easier to understand and debug</li> </ul>"},{"location":"guide/best-practices/#scale-gradually","title":"Scale Gradually","text":"<pre><code>import torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Start with tiny model\nlm = LanguageModel.from_huggingface(\"sshleifer/tiny-gpt2\", store=store, device=device)\n\n# Then move to small\nlm = LanguageModel.from_huggingface(\"gpt2\", store=store, device=device)\n\n# Finally use larger models\nlm = LanguageModel.from_huggingface(\"gpt2-large\", store=store, device=device)\n</code></pre>"},{"location":"guide/best-practices/#consider-your-goals","title":"Consider Your Goals","text":"<ul> <li>Quick prototyping: Use tiny/small models</li> <li>Production research: Use appropriately sized models</li> <li>Memory constraints: Start small, scale if needed</li> </ul>"},{"location":"guide/best-practices/#memory-management","title":"Memory Management","text":""},{"location":"guide/best-practices/#activation-saving","title":"Activation Saving","text":"<pre><code># Use appropriate batch sizes\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=4,  # Start small\n    sample_limit=100\n)\n</code></pre>"},{"location":"guide/best-practices/#sae-training","title":"SAE Training","text":"<pre><code># Monitor memory during training\nconfig = SaeTrainingConfig(\n    batch_size=128,  # Adjust based on available memory\n    epochs=100\n)\n</code></pre>"},{"location":"guide/best-practices/#hook-management","title":"Hook Management","text":"<pre><code># Always unregister hooks\ntry:\n    hook_id = lm.layers.register_hook(\"layer_0\", detector)\n    # ... use hook ...\nfinally:\n    lm.layers.unregister_hook(hook_id)  # Critical!\n</code></pre>"},{"location":"guide/best-practices/#move-to-cpu","title":"Move to CPU","text":"<pre><code># Move large tensors to CPU\nactivations = detector.get_captured()\nactivations_cpu = activations.detach().cpu()  # Saves GPU memory\n</code></pre>"},{"location":"guide/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guide/best-practices/#batch-processing","title":"Batch Processing","text":"<pre><code># Process in larger batches when possible\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=32,  # Larger = faster (if memory allows)\n    sample_limit=1000\n)\n</code></pre>"},{"location":"guide/best-practices/#device-selection","title":"Device Selection","text":"<pre><code># Use GPU when available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsae = TopKSae(n_latents=4096, n_inputs=768, k=32, device=device)\n</code></pre>"},{"location":"guide/best-practices/#efficient-data-loading","title":"Efficient Data Loading","text":"<pre><code># Use appropriate dataset sizes\ndataset = TextDataset(texts=texts[:1000])  # Limit for testing\n# Then scale up for full experiments\n</code></pre>"},{"location":"guide/best-practices/#experiment-organization","title":"Experiment Organization","text":""},{"location":"guide/best-practices/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Use descriptive names\nrun_id = f\"gpt2_layer0_attn_{timestamp}\"\nsae_id = f\"sae_gpt2_layer0_4x_k32\"\n</code></pre>"},{"location":"guide/best-practices/#store-organization","title":"Store Organization","text":"<pre><code>store/\n\u251c\u2500\u2500 activations/\n\u2502   \u2514\u2500\u2500 &lt;model&gt;_&lt;layer&gt;_&lt;date&gt;/\n\u251c\u2500\u2500 runs/\n\u2502   \u2514\u2500\u2500 &lt;sae_training&gt;_&lt;date&gt;/\n\u2514\u2500\u2500 sae_models/\n    \u2514\u2500\u2500 &lt;sae_id&gt;/\n</code></pre>"},{"location":"guide/best-practices/#version-control","title":"Version Control","text":"<ul> <li>Track configurations in code</li> <li>Save metadata with experiments</li> <li>Document hyperparameters</li> </ul>"},{"location":"guide/best-practices/#logging","title":"Logging","text":"<pre><code># Use wandb for tracking\nconfig = SaeTrainingConfig(\n    use_wandb=True,\n    wandb_project=\"sae-experiments\",\n    wandb_run_name=\"gpt2-layer0-4x\"\n)\n</code></pre>"},{"location":"guide/best-practices/#debugging-tips","title":"Debugging Tips","text":""},{"location":"guide/best-practices/#verify-layer-names","title":"Verify Layer Names","text":"<pre><code># Always check available layers\nlayers = lm.layers.list_layers()\nprint(f\"Available layers: {layers}\")\n\n# Use exact names\nlayer_name = layers[0]  # Don't guess!\n</code></pre>"},{"location":"guide/best-practices/#check-activations","title":"Check Activations","text":"<pre><code># Verify activations were saved\nfrom pathlib import Path\nrun_path = Path(f\"store/activations/{run_id}\")\nassert run_path.exists(), f\"Run {run_id} not found\"\n\n# Check metadata\nimport json\nwith open(run_path / \"meta.json\") as f:\n    meta = json.load(f)\n    print(f\"Samples: {meta['sample_count']}\")\n</code></pre>"},{"location":"guide/best-practices/#validate-sae-training","title":"Validate SAE Training","text":"<pre><code># Check training metrics\nprint(f\"Final loss: {history['loss'][-1]}\")\nprint(f\"Final R\u00b2: {history['r2'][-1]}\")\nprint(f\"Dead features: {history['dead_features'][-1]}\")\n\n# Verify weights learned\nweight_var = sae.encoder.weight.var().item()\nassert weight_var &gt; 0.01, \"Weights may not have learned!\"\n</code></pre>"},{"location":"guide/best-practices/#test-hooks","title":"Test Hooks","text":"<pre><code># Verify hook is registered\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\nassert hook_id in lm.layers.context._hook_id_map\n\n# Check hook executes\noutputs, encodings = lm.inference.execute_inference([\"test\"])\nactivations = detector.get_captured()\nassert activations is not None, \"Hook didn't execute!\"\n</code></pre>"},{"location":"guide/best-practices/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"guide/best-practices/#forgetting-to-cleanup","title":"Forgetting to Cleanup","text":"<pre><code># \u274c Wrong - hook never unregistered\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\n# ... use hook ...\n# Forgot to unregister!\n\n# \u2705 Correct - always cleanup\ntry:\n    hook_id = lm.layers.register_hook(\"layer_0\", detector)\n    # ... use hook ...\nfinally:\n    lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/best-practices/#wrong-layer-names","title":"Wrong Layer Names","text":"<pre><code># \u274c Wrong - guessing layer name\nhook_id = lm.layers.register_hook(\"layer_0\", detector)  # May not exist!\n\n# \u2705 Correct - check first\nlayers = lm.layers.list_layers()\nlayer_name = layers[0]  # Use actual name\nhook_id = lm.layers.register_hook(layer_name, detector)\n</code></pre>"},{"location":"guide/best-practices/#memory-leaks","title":"Memory Leaks","text":"<pre><code># \u274c Wrong - accumulating without clearing\ndef process_activations(self, module, input, output):\n    self.all_activations.append(output)  # Never cleared!\n\n# \u2705 Correct - clear periodically\ndef process_activations(self, module, input, output):\n    if len(self.all_activations) &gt; 100:\n        self.all_activations.clear()\n    self.all_activations.append(output.detach().cpu())\n</code></pre>"},{"location":"guide/best-practices/#in-place-modification","title":"In-place Modification","text":"<pre><code># \u274c Wrong - modifies in place\ndef modify_fn(x):\n    x *= 2.0  # In-place modification\n    return x\n\n# \u2705 Correct - return new tensor\ndef modify_fn(x):\n    return x * 2.0  # Creates new tensor\n</code></pre>"},{"location":"guide/best-practices/#code-organization","title":"Code Organization","text":""},{"location":"guide/best-practices/#reusable-functions","title":"Reusable Functions","text":"<pre><code>def create_sae(n_latents, n_inputs, k, device):\n    \"\"\"Create and return SAE.\"\"\"\n    return TopKSae(n_latents=n_latents, n_inputs=n_inputs, k=k, device=device)\n\ndef train_sae(sae, store, run_id, layer_signature, config):\n    \"\"\"Train SAE and return history.\"\"\"\n    trainer = SaeTrainer(sae)\n    return trainer.train(store, run_id, layer_signature, config)\n</code></pre>"},{"location":"guide/best-practices/#configuration-management","title":"Configuration Management","text":"<pre><code># Use dataclasses or config files\n@dataclass\nclass ExperimentConfig:\n    model_name: str\n    layer_name: str\n    n_latents: int\n    k: int\n    epochs: int\n    batch_size: int\n\nconfig = ExperimentConfig(\n    model_name=\"gpt2\",\n    layer_name=\"transformer.h.0.attn.c_attn\",\n    n_latents=4096,\n    k=32,\n    epochs=100,\n    batch_size=256\n)\n</code></pre>"},{"location":"guide/best-practices/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    run_id = lm.activations.save(\n        layer_signature=layer_name,\n        dataset=dataset,\n        sample_limit=1000\n    )\nexcept ValueError as e:\n    print(f\"Layer not found: {e}\")\n    # Handle error\nexcept RuntimeError as e:\n    print(f\"Memory error: {e}\")\n    # Reduce batch size and retry\n</code></pre>"},{"location":"guide/best-practices/#documentation","title":"Documentation","text":""},{"location":"guide/best-practices/#document-experiments","title":"Document Experiments","text":"<pre><code># Save experiment metadata\nexperiment_meta = {\n    \"model\": \"gpt2\",\n    \"layer\": \"transformer.h.0.attn.c_attn\",\n    \"sae_config\": {\n        \"n_latents\": 4096,\n        \"n_inputs\": 768,\n        \"k\": 32\n    },\n    \"training_config\": {\n        \"epochs\": 100,\n        \"batch_size\": 256,\n        \"lr\": 1e-3\n    },\n    \"results\": {\n        \"final_loss\": history['loss'][-1],\n        \"final_r2\": history['r2'][-1]\n    }\n}\n\nimport json\nwith open(\"experiment_meta.json\", \"w\") as f:\n    json.dump(experiment_meta, f, indent=2)\n</code></pre>"},{"location":"guide/best-practices/#comment-code","title":"Comment Code","text":"<pre><code># Save activations from attention layer\n# Using batch size 4 to fit in GPU memory\nrun_id = lm.activations.save(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    dataset=dataset,\n    batch_size=4,  # Limited by GPU memory\n    sample_limit=1000\n)\n</code></pre>"},{"location":"guide/best-practices/#testing","title":"Testing","text":""},{"location":"guide/best-practices/#start-with-small-examples","title":"Start with Small Examples","text":"<pre><code># Test with minimal data first\ntest_texts = [\"Hello, world!\"] * 10\ntest_dataset = TextDataset(texts=test_texts)\n\n# Verify workflow works\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=test_dataset,\n    sample_limit=10\n)\n</code></pre>"},{"location":"guide/best-practices/#validate-each-step","title":"Validate Each Step","text":"<pre><code># Verify activations saved\nassert Path(f\"store/activations/{run_id}\").exists()\n\n# Verify SAE trains\nhistory = trainer.train(...)\nassert history['loss'][-1] &lt; history['loss'][0]  # Loss decreased\n\n# Verify concepts discovered\ntop_texts = sae.concepts.get_top_texts()\nassert len(top_texts) &gt; 0  # Found some concepts\n</code></pre>"},{"location":"guide/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>Troubleshooting - Common issues and solutions</li> <li>Examples - Example code patterns</li> <li>Hooks: Advanced - Advanced patterns</li> </ul>"},{"location":"guide/core-concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts and architecture of mi-crow. Understanding these concepts will help you use the library effectively.</p>"},{"location":"guide/core-concepts/#language-models-in-mi-crow","title":"Language Models in mi-crow","text":"<p>mi-crow provides a unified interface for working with language models through the <code>LanguageModel</code> class. It wraps PyTorch models (typically from HuggingFace) and provides:</p> <ul> <li>Inference: Run forward passes with <code>forwards()</code> and generation with <code>generate()</code></li> <li>Layer Access: Inspect and manipulate individual layers</li> <li>Activation Saving: Collect activations from any layer</li> <li>Hook Integration: Attach hooks for observation and control</li> </ul>"},{"location":"guide/core-concepts/#model-loading","title":"Model Loading","text":"<pre><code>from mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\nimport torch\n\nstore = LocalStore(base_path=\"./store\")\n\n# Use GPU when available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlm = LanguageModel.from_huggingface(\n    \"gpt2\",  # Or any HuggingFace model\n    store=store,\n    device=device,\n)\n</code></pre> <p>The model is automatically tokenized and ready for inference. You can access layers through <code>lm.layers</code> and run inference with <code>lm.inference.execute_inference()</code>.</p>"},{"location":"guide/core-concepts/#sparse-autoencoders-sae","title":"Sparse Autoencoders (SAE)","text":"<p>Sparse Autoencoders are the core interpretability tool in mi-crow. They learn to represent model activations using a sparse set of interpretable features.</p>"},{"location":"guide/core-concepts/#what-are-saes","title":"What are SAEs?","text":"<p>An SAE is a neural network that: 1. Takes dense activations from a model layer as input 2. Encodes them into a sparse latent representation 3. Decodes back to reconstruct the original activations</p> <p>The sparsity constraint encourages the SAE to learn discrete, interpretable features (neurons) that correspond to meaningful concepts.</p>"},{"location":"guide/core-concepts/#why-use-saes","title":"Why Use SAEs?","text":"<ul> <li>Interpretability: Each SAE neuron often corresponds to a human-understandable concept</li> <li>Feature Discovery: Automatically discover what features the model uses</li> <li>Control: Manipulate model behavior by amplifying or suppressing specific neurons</li> <li>Analysis: Understand which features are important for different tasks</li> </ul>"},{"location":"guide/core-concepts/#sae-architecture","title":"SAE Architecture","text":"<p>mi-crow supports TopK SAEs, which enforce sparsity by keeping only the top-K most active neurons:</p> <pre><code>from mi_crow.mechanistic.sae import TopKSae\n\nsae = TopKSae(\n    n_latents=4096,  # Number of SAE neurons (overcomplete)\n    n_inputs=768,    # Size of input activations\n    k=32,            # Top-K sparsity (only 32 neurons active at once)\n    device=\"cuda\"\n)\n</code></pre> <p>The overcomplete ratio (<code>n_latents / n_inputs</code>) determines how many features the SAE can learn. Higher ratios allow more fine-grained feature discovery.</p>"},{"location":"guide/core-concepts/#concepts-and-interpretability","title":"Concepts and Interpretability","text":"<p>Concepts are human-interpretable meanings associated with SAE neurons. The concept discovery process involves:</p> <ol> <li>Training an SAE on model activations</li> <li>Collecting top texts that activate each neuron</li> <li>Manual curation to name concepts based on patterns</li> <li>Using concepts to understand and control model behavior</li> </ol>"},{"location":"guide/core-concepts/#concept-discovery","title":"Concept Discovery","text":"<p>After training an SAE, you can discover concepts by:</p> <pre><code># Enable text tracking during inference\nsae.concepts.enable_text_tracking(top_k=10)\n\n# Run inference on a dataset\noutputs, encodings = lm.inference.execute_inference(dataset_texts)\n\n# Get top activating texts for each neuron\ntop_texts = sae.concepts.get_top_texts()\n</code></pre> <p>Each neuron's top texts reveal what patterns it detects, allowing you to assign meaningful names like \"family relationships\" or \"scientific terminology\".</p>"},{"location":"guide/core-concepts/#concept-manipulation","title":"Concept Manipulation","text":"<p>Once concepts are identified, you can manipulate model behavior:</p> <pre><code># Amplify a concept (neuron 42)\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)\n\n# Suppress a concept\nsae.concepts.manipulate_concept(neuron_idx=42, scale=0.5)\n\n# Run inference with modified behavior\noutputs, encodings = lm.inference.execute_inference([\"Your prompt here\"])\n</code></pre>"},{"location":"guide/core-concepts/#hooks-system-overview","title":"Hooks System Overview","text":"<p>The hooks system is the foundation of mi-crow's interpretability capabilities. It allows you to intercept and process activations during model inference.</p>"},{"location":"guide/core-concepts/#what-are-hooks","title":"What are Hooks?","text":"<p>Hooks are callbacks that execute at specific points during a model's forward pass. They can:</p> <ul> <li>Observe activations without modification (Detectors)</li> <li>Modify activations to change model behavior (Controllers)</li> </ul>"},{"location":"guide/core-concepts/#hook-types","title":"Hook Types","text":"<ul> <li>Detectors: Collect data, save activations, track statistics</li> <li>Controllers: Modify inputs or outputs to steer model behavior</li> </ul>"},{"location":"guide/core-concepts/#why-hooks-matter","title":"Why Hooks Matter","text":"<p>Hooks enable: - Non-invasive inspection: Observe model internals without changing code - Flexible control: Modify behavior at any layer - Composable interventions: Combine multiple hooks for complex experiments - SAE integration: SAEs work as both detectors and controllers</p> <p>For detailed information about hooks, see the Hooks System Guide.</p>"},{"location":"guide/core-concepts/#store-architecture","title":"Store Architecture","text":"<p>The Store provides a hierarchical persistence layer for:</p> <ul> <li>Activations: Saved layer activations organized by run</li> <li>Models: Trained SAE models and checkpoints</li> <li>Metadata: Training history, configurations, and run information</li> </ul>"},{"location":"guide/core-concepts/#store-structure","title":"Store Structure","text":"<pre><code>store/\n\u251c\u2500\u2500 activations/\n\u2502   \u2514\u2500\u2500 &lt;run_id&gt;/\n\u2502       \u251c\u2500\u2500 batch_0/\n\u2502       \u2502   \u2514\u2500\u2500 &lt;layer_name&gt;/\n\u2502       \u2502       \u2514\u2500\u2500 activations.safetensors\n\u2502       \u2514\u2500\u2500 meta.json\n\u251c\u2500\u2500 runs/\n\u2502   \u2514\u2500\u2500 &lt;run_id&gt;/\n\u2502       \u2514\u2500\u2500 training_history.json\n\u2514\u2500\u2500 sae_models/\n    \u2514\u2500\u2500 &lt;sae_id&gt;/\n        \u2514\u2500\u2500 model.pt\n</code></pre>"},{"location":"guide/core-concepts/#localstore","title":"LocalStore","text":"<p>The default implementation uses the local filesystem:</p> <pre><code>from mi_crow.store import LocalStore\n\nstore = LocalStore(base_path=\"./store\")\n</code></pre> <p>All operations automatically organize data in this hierarchical structure, making it easy to manage large-scale experiments.</p>"},{"location":"guide/core-concepts/#datasets-and-data-loading","title":"Datasets and Data Loading","text":"<p>mi-crow provides flexible dataset loading for:</p> <ul> <li>HuggingFace datasets: Direct integration with HF datasets</li> <li>Local files: Load from text files or custom formats</li> <li>In-memory data: Use Python lists directly</li> </ul>"},{"location":"guide/core-concepts/#textdataset","title":"TextDataset","text":"<p>For simple text data:</p> <pre><code>from mi_crow.datasets import TextDataset\n\ndataset = TextDataset(texts=[\"Text 1\", \"Text 2\", \"Text 3\"])\n</code></pre>"},{"location":"guide/core-concepts/#huggingface-integration","title":"HuggingFace Integration","text":"<pre><code>from mi_crow.datasets import HuggingFaceDataset\n\ndataset = HuggingFaceDataset(\n    name=\"wikitext\",\n    split=\"train\",\n    text_field=\"text\"\n)\n</code></pre> <p>Datasets are automatically batched and tokenized when used with <code>lm.activations.save()</code> or during inference.</p>"},{"location":"guide/core-concepts/#putting-it-all-together","title":"Putting It All Together","text":"<p>The typical mi-crow workflow combines these concepts:</p> <ol> <li>Load a model and create a store</li> <li>Save activations from a layer using hooks (detectors)</li> <li>Train an SAE on the activations</li> <li>Discover concepts by analyzing neuron activations</li> <li>Manipulate concepts to control model behavior (controllers)</li> <li>Analyze results using the store's organized data</li> </ol> <p>Each component is designed to work seamlessly with the others, providing a complete toolkit for mechanistic interpretability research.</p>"},{"location":"guide/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Hooks System - Deep dive into the hooks framework</li> <li>Saving Activations - Detailed activation collection guide</li> <li>Training SAE Models - SAE training best practices</li> <li>Concept Discovery - Finding interpretable concepts</li> </ul>"},{"location":"guide/examples/","title":"Examples","text":"<p>This guide provides an overview of the example notebooks in the <code>examples/</code> directory and a recommended learning path.</p>"},{"location":"guide/examples/#overview","title":"Overview","text":"<p>The examples directory contains Jupyter notebooks demonstrating mi-crow functionality. Each example builds on previous ones, creating a complete learning path from basic usage to advanced techniques.</p>"},{"location":"guide/examples/#recommended-learning-path","title":"Recommended Learning Path","text":""},{"location":"guide/examples/#beginner-path","title":"Beginner Path","text":"<ol> <li>01_train_sae_model.ipynb - Train your first SAE</li> <li>02_attach_sae_and_save_texts.ipynb - Discover concepts</li> <li>03_load_concepts.ipynb - Manipulate concepts</li> </ol>"},{"location":"guide/examples/#intermediate-path","title":"Intermediate Path","text":"<ol> <li>04_save_inputs_and_outputs.ipynb - Advanced data collection</li> <li>08_inference_with_hooks.ipynb - Direct hook usage</li> <li>09_activations_two_modes.ipynb - Activation analysis</li> </ol>"},{"location":"guide/examples/#advanced-path","title":"Advanced Path","text":"<ol> <li>05_special_token_mask.ipynb - Special token handling</li> <li>06_save_activations_with_attention_masks.ipynb - Attention masks</li> <li>07_save_activations_and_attention_masks.ipynb - Combined techniques</li> </ol>"},{"location":"guide/examples/#example-notebooks","title":"Example Notebooks","text":""},{"location":"guide/examples/#01_train_sae_modelipynb","title":"01_train_sae_model.ipynb","text":"<p>Purpose: Train a Sparse Autoencoder (SAE) on model activations</p> <p>What you'll learn: - Load a language model - Save activations from a specific layer - Train an SAE to learn interpretable features - Save the trained SAE for future use</p> <p>Key concepts: - Activation saving - SAE architecture - Training configuration - Model persistence</p> <p>Output files: - <code>outputs/sae_model.pt</code> - <code>outputs/training_metadata.json</code> - <code>store/activations/&lt;run_id&gt;/</code></p> <p>Related guides: - Saving Activations - Training SAE Models</p>"},{"location":"guide/examples/#02_attach_sae_and_save_textsipynb","title":"02_attach_sae_and_save_texts.ipynb","text":"<p>Purpose: Collect top activating texts for each SAE neuron</p> <p>What you'll learn: - Load a trained SAE model - Enable automatic text tracking during inference - Run inference to collect neuron-text associations - Export top texts for manual concept curation</p> <p>Key concepts: - SAE attachment - Text tracking - Concept discovery - Data export</p> <p>Output files: - <code>outputs/top_texts.json</code> - <code>outputs/attachment_metadata.json</code></p> <p>Related guides: - Concept Discovery - Hooks: Detectors</p>"},{"location":"guide/examples/#03_load_conceptsipynb","title":"03_load_concepts.ipynb","text":"<p>Purpose: Control model behavior using learned concepts</p> <p>What you'll learn:</p>"},{"location":"guide/examples/#part-1-sae-level-manipulation","title":"Part 1: SAE-Level Manipulation","text":"<ul> <li>Load curated concepts (neuron \u2192 concept name mappings)</li> <li>Use <code>manipulate_concept()</code> to amplify/suppress specific SAE neurons</li> <li>Compare model behavior with different concept strengths</li> </ul>"},{"location":"guide/examples/#part-2-activation-control","title":"Part 2: Activation Control","text":"<ul> <li>Create custom activation controllers</li> <li>Amplify or suppress layer activations during inference</li> <li>Enable/disable controllers dynamically</li> <li>Use <code>with_controllers</code> parameter for A/B testing</li> <li>Control model generation in real-time</li> </ul> <p>Key concepts: - Concept manipulation - Activation controllers - Dynamic control - A/B testing</p> <p>Output files: - Modified model outputs - Comparison results</p> <p>Related guides: - Concept Manipulation - Activation Control - Hooks: Controllers</p>"},{"location":"guide/examples/#04_save_inputs_and_outputsipynb","title":"04_save_inputs_and_outputs.ipynb","text":"<p>Purpose: Save model inputs and outputs alongside activations</p> <p>What you'll learn: - Capture model inputs (tokenized text) - Save model outputs (logits, predictions) - Correlate inputs, activations, and outputs - Analyze input-output relationships</p> <p>Key concepts: - Input/output detection - Data correlation - Analysis workflows</p> <p>Related guides: - Saving Activations - Hooks: Detectors</p>"},{"location":"guide/examples/#05_special_token_maskipynb","title":"05_special_token_mask.ipynb","text":"<p>Purpose: Handle special tokens when saving activations</p> <p>What you'll learn: - Identify special tokens - Mask special tokens from activations - Filter activations by token type - Analyze token-specific patterns</p> <p>Key concepts: - Token masking - Special token handling - Activation filtering</p> <p>Related guides: - Saving Activations</p>"},{"location":"guide/examples/#06_save_activations_with_attention_masksipynb","title":"06_save_activations_with_attention_masks.ipynb","text":"<p>Purpose: Save activations with proper attention mask handling</p> <p>What you'll learn: - Use attention masks during activation saving - Handle padding tokens correctly - Save masked activations - Process variable-length sequences</p> <p>Key concepts: - Attention masks - Padding handling - Sequence processing</p> <p>Related guides: - Saving Activations</p>"},{"location":"guide/examples/#07_save_activations_and_attention_masksipynb","title":"07_save_activations_and_attention_masks.ipynb","text":"<p>Purpose: Advanced activation saving with attention masks</p> <p>What you'll learn: - Combined input/output/activation saving - Attention mask integration - Complex data collection workflows</p> <p>Key concepts: - Multi-modal data collection - Mask integration - Advanced workflows</p> <p>Related guides: - Saving Activations</p>"},{"location":"guide/examples/#08_inference_with_hooksipynb","title":"08_inference_with_hooks.ipynb","text":"<p>Purpose: Direct hook usage for activation control</p> <p>What you'll learn: - Create custom detector hooks - Create custom controller hooks - Register hooks on layers - Modify activations directly - Multi-layer interventions</p> <p>Key concepts: - Hook creation - Hook registration - Activation modification - Hook management</p> <p>Related guides: - Activation Control - Hooks: Fundamentals - Hooks: Controllers</p>"},{"location":"guide/examples/#09_activations_two_modesipynb","title":"09_activations_two_modes.ipynb","text":"<p>Purpose: Compare activations in different modes</p> <p>What you'll learn: - Save activations in training vs evaluation mode - Compare activation patterns - Understand mode effects on activations</p> <p>Key concepts: - Model modes - Activation comparison - Mode effects</p> <p>Related guides: - Saving Activations</p>"},{"location":"guide/examples/#datasetsload_wildguardmixipynb","title":"datasets/load_wildguardmix.ipynb","text":"<p>Purpose: Load custom datasets for experiments</p> <p>What you'll learn: - Load datasets from various sources - Prepare data for mi-crow - Handle dataset formats</p> <p>Key concepts: - Dataset loading - Data preparation - Format conversion</p> <p>Related guides: - Core Concepts - Datasets section</p>"},{"location":"guide/examples/#quick-reference-table","title":"Quick Reference Table","text":"Example Purpose Key Concepts Output Files 01_train_sae_model Train SAE Activation saving, SAE training <code>sae_model.pt</code>, <code>training_metadata.json</code> 02_attach_sae_and_save_texts Concept discovery Text tracking, concept discovery <code>top_texts.json</code> 03_load_concepts Concept manipulation Concept control, activation controllers Modified outputs 04_save_inputs_and_outputs Data collection Input/output detection Input/output files 05_special_token_mask Token handling Token masking, filtering Masked activations 06_save_activations_with_attention_masks Mask handling Attention masks, padding Masked activations 07_save_activations_and_attention_masks Advanced collection Multi-modal collection Combined data 08_inference_with_hooks Hook usage Custom hooks, activation control Hook examples 09_activations_two_modes Mode comparison Training vs eval modes Comparison data"},{"location":"guide/examples/#using-examples","title":"Using Examples","text":"<p>The example notebooks are located in the <code>examples/</code> directory. Each notebook demonstrates specific mi-crow functionality and can be opened in Jupyter or JupyterLab.</p> <p>The examples are designed to be run sequentially, with each building on concepts from previous ones.</p>"},{"location":"guide/examples/#output-directory-structure","title":"Output Directory Structure","text":"<p>After running examples, you'll have:</p> <pre><code>examples/\n\u251c\u2500\u2500 outputs/\n\u2502   \u251c\u2500\u2500 sae_model.pt\n\u2502   \u251c\u2500\u2500 training_metadata.json\n\u2502   \u251c\u2500\u2500 attachment_metadata.json\n\u2502   \u251c\u2500\u2500 top_texts.json\n\u2502   \u2514\u2500\u2500 curated_concepts.csv\n\u251c\u2500\u2500 store/\n\u2502   \u251c\u2500\u2500 activations/\n\u2502   \u2502   \u2514\u2500\u2500 &lt;run_id&gt;/\n\u2502   \u2514\u2500\u2500 runs/\n\u2502       \u2514\u2500\u2500 &lt;training_run_id&gt;/\n\u2514\u2500\u2500 cache/\n    \u2514\u2500\u2500 huggingface/\n</code></pre>"},{"location":"guide/examples/#tips-for-learning","title":"Tips for Learning","text":"<ol> <li>Run in order: Examples build on each other</li> <li>Read the code: Understand what each cell does</li> <li>Modify parameters: Experiment with different values</li> <li>Check outputs: Verify results match expectations</li> <li>Read related guides: Deepen understanding with documentation</li> </ol>"},{"location":"guide/examples/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check Troubleshooting</li> <li>Review Best Practices</li> <li>Consult API Reference</li> <li>Check example notebook comments</li> </ol>"},{"location":"guide/examples/#next-steps","title":"Next Steps","text":"<p>After working through examples:</p> <ul> <li>Workflows - Detailed workflow guides</li> <li>Hooks System - Deep dive into hooks</li> <li>Experiments - Real-world experiments</li> </ul>"},{"location":"guide/installation/","title":"Installation &amp; Setup","text":"<p>This guide will help you install mi-crow and configure your environment for mechanistic interpretability research.</p>"},{"location":"guide/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python: 3.10, 3.11, or 3.12</li> <li>PyTorch: 2.8.0 or later</li> <li>CUDA (optional): For GPU acceleration</li> <li>MPS (optional): For Apple Silicon GPU support</li> </ul>"},{"location":"guide/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"guide/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install mi-crow\n</code></pre>"},{"location":"guide/installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<pre><code>uv pip install mi-crow\n</code></pre>"},{"location":"guide/installation/#from-source","title":"From Source","text":"<p>Clone the repository and install in development mode:</p> <pre><code>git clone https://github.com/AdamKaniasty/Inzynierka.git\ncd Inzynierka\npip install -e .\n</code></pre>"},{"location":"guide/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"guide/installation/#server-dependencies","title":"Server Dependencies","text":"<p>For running the FastAPI server:</p> <pre><code>uv sync --group server\n</code></pre> <p>Or with pip:</p> <pre><code>pip install mi-crow[server]\n</code></pre>"},{"location":"guide/installation/#documentation-dependencies","title":"Documentation Dependencies","text":"<p>For building documentation locally:</p> <pre><code>uv sync --group docs\n</code></pre> <p>Or with pip:</p> <pre><code>pip install mi-crow[docs]\n</code></pre>"},{"location":"guide/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"guide/installation/#virtual-environment","title":"Virtual Environment","text":"<p>We recommend using a virtual environment:</p> <pre><code># Using venv\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Using uv\nuv venv\nsource .venv/bin/activate\n</code></pre>"},{"location":"guide/installation/#device-configuration","title":"Device Configuration","text":"<p>mi-crow automatically detects available devices. You can also explicitly set the device:</p> <pre><code>import torch\n\n# Check available device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Or for Apple Silicon:\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n</code></pre>"},{"location":"guide/installation/#huggingface-setup","title":"HuggingFace Setup","text":"<p>For downloading models from HuggingFace, you may need to authenticate:</p> <pre><code>huggingface-cli login\n</code></pre> <p>Set your cache directory (optional):</p> <pre><code>export HF_HOME=/path/to/huggingface/cache\n</code></pre>"},{"location":"guide/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>from mi_crow import ping\n\nprint(ping())  # Should print \"pong\"\n</code></pre> <p>Test with a simple model load:</p> <pre><code>from mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\nimport torch\n\nstore = LocalStore(base_path=\"./store\")\n\n# Use GPU when available, otherwise CPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlm = LanguageModel.from_huggingface(\n    \"sshleifer/tiny-gpt2\",\n    store=store,\n    device=device,\n)\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/installation/#pytorch-installation-issues","title":"PyTorch Installation Issues","text":"<p>If you encounter PyTorch installation problems:</p> <ol> <li>Visit pytorch.org for platform-specific instructions</li> <li>For CUDA support, ensure your CUDA version matches PyTorch's requirements</li> <li>For Apple Silicon, PyTorch should automatically support MPS</li> </ol>"},{"location":"guide/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors:</p> <ol> <li>Ensure you're in the correct virtual environment</li> <li>Verify installation: <code>pip list | grep mi-crow</code></li> <li>Try reinstalling: <code>pip install --force-reinstall mi-crow</code></li> </ol>"},{"location":"guide/installation/#gpu-not-detected","title":"GPU Not Detected","text":"<p>If CUDA is not detected:</p> <pre><code>import torch\nprint(torch.cuda.is_available())  # Should be True\nprint(torch.cuda.device_count())   # Number of GPUs\n</code></pre> <p>If False, check: - CUDA drivers are installed - PyTorch was installed with CUDA support - GPU is compatible with your CUDA version</p>"},{"location":"guide/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete, proceed to:</p> <ul> <li>Quick Start - Run your first example</li> <li>Core Concepts - Understand the fundamentals</li> </ul>"},{"location":"guide/quickstart/","title":"Quick Start","text":"<p>Get up and running with mi-crow in minutes! This tutorial will walk you through a minimal example that demonstrates the core workflow.</p>"},{"location":"guide/quickstart/#minimal-example","title":"Minimal Example","text":"<p>Let's start with the simplest possible example: loading a model and running inference.</p> <pre><code>from mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\nimport torch\n\n# Create a store for saving data\nstore = LocalStore(base_path=\"./store\")\n\n# Choose device: use GPU when available, otherwise CPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load a small model for testing on the chosen device\nlm = LanguageModel.from_huggingface(\n    \"sshleifer/tiny-gpt2\",\n    store=store,\n    device=device,\n)\n\n# Run inference\ntexts = [\"Hello, world!\", \"How are you?\"]\noutputs, encodings = lm.inference.execute_inference(texts)\n\nprint(outputs.logits.shape)  # (batch_size, seq_len, vocab_size)\n</code></pre>"},{"location":"guide/quickstart/#basic-sae-workflow","title":"Basic SAE Workflow","text":"<p>The core mi-crow workflow consists of three main steps:</p>"},{"location":"guide/quickstart/#step-1-save-activations","title":"Step 1: Save Activations","text":"<p>First, we need to collect activations from a model layer:</p> <pre><code>from mi_crow.datasets import TextDataset\n\n# Create a simple dataset\ndataset = TextDataset(texts=[\"The cat sat on the mat.\"] * 100)\n\n# Save activations from a layer\nrun_id = lm.activations.save(\n    layer_signature=\"transformer.h.0.attn.c_attn\",  # Layer name\n    dataset=dataset,\n    sample_limit=100,\n    batch_size=4\n)\n\nprint(f\"Saved activations with run_id: {run_id}\")\n</code></pre>"},{"location":"guide/quickstart/#step-2-train-an-sae","title":"Step 2: Train an SAE","text":"<p>Train a sparse autoencoder on the saved activations:</p> <pre><code>from mi_crow.mechanistic.sae import TopKSae, SaeTrainer\nfrom mi_crow.mechanistic.sae.train import SaeTrainingConfig\n\n# Create SAE model\nsae = TopKSae(\n    n_latents=512,  # Number of SAE neurons\n    n_inputs=768,   # Must match layer activation size\n    k=8,            # Top-K sparsity\n    device=\"cpu\"\n)\n\n# Configure training\nconfig = SaeTrainingConfig(\n    epochs=10,\n    batch_size=256,\n    lr=1e-3\n)\n\n# Train the SAE\ntrainer = SaeTrainer(sae)\nhistory = trainer.train(\n    store=store,\n    run_id=run_id,\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    config=config\n)\n\nprint(\"Training complete!\")\n</code></pre>"},{"location":"guide/quickstart/#step-3-use-concepts","title":"Step 3: Use Concepts","text":"<p>Attach the SAE to the model and use it for concept discovery:</p> <pre><code># Attach SAE to model\nlm.attach_sae(sae, layer_signature=\"transformer.h.0.attn.c_attn\")\n\n# Enable text tracking to see what activates each neuron\nsae.concepts.enable_text_tracking(top_k=5)\n\n# Run inference with SAE attached\noutputs, encodings = lm.inference.execute_inference([\"The cat sat on the mat.\"])\n\n# Get top texts for each neuron\ntop_texts = sae.concepts.get_top_texts()\nprint(f\"Found {len(top_texts)} neurons with tracked texts\")\n</code></pre>"},{"location":"guide/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete, runnable example:</p> <pre><code>from mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\nfrom mi_crow.datasets import TextDataset\nfrom mi_crow.mechanistic.sae import TopKSae\nfrom mi_crow.mechanistic.sae.train import SaeTrainer, SaeTrainingConfig\n\n# Setup\nstore = LocalStore(base_path=\"./store\")\n\n# Choose device: GPU if available, otherwise CPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlm = LanguageModel.from_huggingface(\"sshleifer/tiny-gpt2\", store=store, device=device)\n\n# Step 1: Save activations\ndataset = TextDataset(texts=[\"The cat sat on the mat.\"] * 50)\nrun_id = lm.activations.save(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    dataset=dataset,\n    sample_limit=50,\n    batch_size=4\n)\n\n# Step 2: Train SAE\nsae = TopKSae(n_latents=256, n_inputs=768, k=4, device=\"cpu\")\ntrainer = SaeTrainer(sae)\nconfig = SaeTrainingConfig(epochs=5, batch_size=64, lr=1e-3)\nhistory = trainer.train(store, run_id, \"transformer.h.0.attn.c_attn\", config)\n\n# Step 3: Use SAE\nlm.attach_sae(sae, \"transformer.h.0.attn.c_attn\")\nsae.concepts.enable_text_tracking(top_k=3)\noutputs, encodings = lm.inference.execute_inference([\"The cat sat on the mat.\"])\ntop_texts = sae.concepts.get_top_texts()\n\nprint(\"Quick start complete!\")\n</code></pre>"},{"location":"guide/quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you've run the basic workflow, explore more:</p> <ul> <li>Core Concepts - Understand SAEs, concepts, and hooks</li> <li>Hooks System - Learn about the powerful hooks framework</li> <li>Saving Activations - Detailed guide for activation collection</li> <li>Training SAE Models - Advanced SAE training techniques</li> <li>Examples - More detailed example notebooks</li> </ul>"},{"location":"guide/quickstart/#tips","title":"Tips","text":"<ul> <li>Start with small models like <code>sshleifer/tiny-gpt2</code> for quick experimentation</li> <li>Use <code>sample_limit</code> to control dataset size during development</li> <li>Check layer names with <code>lm.layers.list_layers()</code> before saving activations</li> <li>Monitor training with <code>history</code> metrics or enable wandb logging</li> </ul>"},{"location":"guide/troubleshooting/","title":"Troubleshooting","text":"<p>This guide covers common issues and their solutions when using mi-crow.</p>"},{"location":"guide/troubleshooting/#common-errors-and-solutions","title":"Common Errors and Solutions","text":""},{"location":"guide/troubleshooting/#layer-signature-not-found","title":"Layer Signature Not Found","text":"<p>Error: <code>ValueError: Layer signature not found</code></p> <p>Causes: - Incorrect layer name - Layer doesn't exist in model - Typo in layer name</p> <p>Solutions:</p> <pre><code># 1. List available layers\nlayers = lm.layers.list_layers()\nprint(\"Available layers:\", layers)\n\n# 2. Use exact name from list\nlayer_name = layers[0]  # Don't guess!\n\n# 3. Verify layer exists before using\nif layer_name in layers:\n    hook_id = lm.layers.register_hook(layer_name, detector)\nelse:\n    print(f\"Layer {layer_name} not found!\")\n</code></pre>"},{"location":"guide/troubleshooting/#out-of-memory","title":"Out of Memory","text":"<p>Error: <code>RuntimeError: CUDA out of memory</code></p> <p>Causes: - Batch size too large - Model too large for GPU - Accumulating tensors in memory</p> <p>Solutions:</p> <pre><code># 1. Reduce batch size\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=1,  # Minimal batch size\n    sample_limit=100\n)\n\n# 2. Use CPU instead of GPU\nsae = TopKSae(n_latents=4096, n_inputs=768, k=32, device=\"cpu\")\n\n# 3. Clear detector data periodically\ndetector.clear_captured()\n\n# 4. Move tensors to CPU\nactivations = detector.get_captured()\nactivations_cpu = activations.detach().cpu()\n</code></pre>"},{"location":"guide/troubleshooting/#hook-not-executing","title":"Hook Not Executing","text":"<p>Symptoms: Hook doesn't seem to run, no data collected</p> <p>Causes: - Hook not registered - Hook disabled - Wrong layer signature - Hook unregistered too early</p> <p>Solutions:</p> <pre><code># 1. Verify hook is registered\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\nassert hook_id in lm.layers.context._hook_id_map\n\n# 2. Check hook is enabled\nassert detector.is_enabled(), \"Hook is disabled!\"\n\n# 3. Verify layer name is correct\nlayers = lm.layers.list_layers()\nassert \"layer_0\" in layers, \"Layer doesn't exist!\"\n\n# 4. Ensure hook stays registered during inference\noutputs, encodings = lm.inference.execute_inference([\"test\"])\nactivations = detector.get_captured()  # Check before unregistering\n</code></pre>"},{"location":"guide/troubleshooting/#sae-training-instability","title":"SAE Training Instability","text":"<p>Symptoms: Loss doesn't decrease, weights not learning</p> <p>Causes: - Learning rate too high - Too much regularization - Not enough training data - Dead features</p> <p>Solutions:</p> <pre><code># 1. Reduce learning rate\nconfig = SaeTrainingConfig(\n    epochs=100,\n    batch_size=256,\n    lr=1e-4,  # Lower learning rate\n    l1_lambda=1e-5  # Lower regularization\n)\n\n# 2. Check for dead features\ndead_count = history['dead_features'][-1]\nif dead_count &gt; sae.n_latents * 0.1:  # More than 10% dead\n    # Reduce sparsity\n    sae = TopKSae(n_latents=4096, n_inputs=768, k=64, device=\"cuda\")  # Increase k\n\n# 3. Verify weights are learning\nweight_var = sae.encoder.weight.var().item()\nif weight_var &lt; 0.01:\n    print(\"Warning: Weights may not be learning!\")\n    # Try different learning rate or more epochs\n</code></pre>"},{"location":"guide/troubleshooting/#poor-sae-reconstruction","title":"Poor SAE Reconstruction","text":"<p>Symptoms: Low R\u00b2 score, high reconstruction error</p> <p>Causes: - Model capacity too small - Not enough training - Wrong hyperparameters</p> <p>Solutions:</p> <pre><code># 1. Increase model capacity\nsae = TopKSae(\n    n_latents=8192,  # More neurons\n    n_inputs=768,\n    k=64,            # More active neurons\n    device=\"cuda\"\n)\n\n# 2. Train longer\nconfig = SaeTrainingConfig(\n    epochs=200,  # More epochs\n    batch_size=256,\n    lr=1e-3\n)\n\n# 3. Adjust hyperparameters\nconfig = SaeTrainingConfig(\n    epochs=100,\n    batch_size=256,\n    lr=1e-3,\n    l1_lambda=1e-5  # Less regularization\n)\n</code></pre>"},{"location":"guide/troubleshooting/#layer-signature-issues","title":"Layer Signature Issues","text":""},{"location":"guide/troubleshooting/#finding-correct-layer-names","title":"Finding Correct Layer Names","text":"<pre><code># List all layers\nall_layers = lm.layers.list_layers()\n\n# Filter by pattern\nattention_layers = [l for l in all_layers if \"attn\" in l]\nmlp_layers = [l for l in all_layers if \"mlp\" in l]\n\n# Print for inspection\nfor i, layer in enumerate(all_layers):\n    print(f\"{i}: {layer}\")\n</code></pre>"},{"location":"guide/troubleshooting/#layer-name-variations","title":"Layer Name Variations","text":"<p>Different models use different naming conventions:</p> <pre><code># GPT-2 style\n\"transformer.h.0.attn.c_attn\"\n\n# BERT style\n\"bert.encoder.layer.0.attention.self.query\"\n\n# Custom models\n\"model.layers.0.self_attn.q_proj\"\n\n# Always check your specific model\nlayers = lm.layers.list_layers()\n</code></pre>"},{"location":"guide/troubleshooting/#memory-problems","title":"Memory Problems","text":""},{"location":"guide/troubleshooting/#gpu-memory","title":"GPU Memory","text":"<pre><code># Check GPU memory\nimport torch\nif torch.cuda.is_available():\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n    print(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n\n# Clear cache if needed\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"guide/troubleshooting/#memory-leaks","title":"Memory Leaks","text":"<pre><code># Check for unregistered hooks\nhook_count = len(lm.layers.context._hook_id_map)\nprint(f\"Registered hooks: {hook_count}\")\n\n# Unregister all if needed\nfor hook_id in list(lm.layers.context._hook_id_map.keys()):\n    lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/troubleshooting/#accumulating-data","title":"Accumulating Data","text":"<pre><code># Clear detector data\ndetector.clear_captured()\ndetector.tensor_metadata.clear()\n\n# Move to CPU\nactivations = detector.get_captured()\nif activations is not None:\n    activations_cpu = activations.detach().cpu()\n    # Use CPU version, original will be garbage collected\n</code></pre>"},{"location":"guide/troubleshooting/#training-instability","title":"Training Instability","text":""},{"location":"guide/troubleshooting/#loss-not-decreasing","title":"Loss Not Decreasing","text":"<pre><code># Check training history\nprint(f\"Initial loss: {history['loss'][0]}\")\nprint(f\"Final loss: {history['loss'][-1]}\")\n\nif history['loss'][-1] &gt;= history['loss'][0]:\n    print(\"Loss didn't decrease!\")\n    # Try:\n    # - Lower learning rate\n    # - More epochs\n    # - Different initialization\n</code></pre>"},{"location":"guide/troubleshooting/#exploding-gradients","title":"Exploding Gradients","text":"<pre><code># Reduce learning rate\nconfig = SaeTrainingConfig(\n    epochs=100,\n    batch_size=256,\n    lr=1e-4,  # Much lower learning rate\n    l1_lambda=1e-5\n)\n\n# Or use gradient clipping (if supported)\n</code></pre>"},{"location":"guide/troubleshooting/#dead-features","title":"Dead Features","text":"<pre><code># Check dead features\ndead_ratio = history['dead_features'][-1] / sae.n_latents\n\nif dead_ratio &gt; 0.1:\n    print(f\"Too many dead features: {dead_ratio:.2%}\")\n    # Solutions:\n    # - Increase k (sparsity)\n    # - Reduce l1_lambda\n    # - Increase learning rate slightly\n</code></pre>"},{"location":"guide/troubleshooting/#device-compatibility","title":"Device Compatibility","text":""},{"location":"guide/troubleshooting/#cuda-issues","title":"CUDA Issues","text":"<pre><code>import torch\nfrom mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\n\n# Check CUDA availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\n\n# Always choose device based on availability\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nstore = LocalStore(base_path=\"./store\")\n\n# This will raise a clear ValueError if you force device=\\\"cuda\\\" but CUDA is not available\nlm = LanguageModel.from_huggingface(\n    \"sshleifer/tiny-gpt2\",\n    store=store,\n    device=device,\n)\n</code></pre>"},{"location":"guide/troubleshooting/#mps-apple-silicon","title":"MPS (Apple Silicon)","text":"<pre><code># Check MPS availability\nif torch.backends.mps.is_available():\n    device = \"mps\"\n    print(\"Using Apple Silicon GPU\")\nelse:\n    device = \"cpu\"\n    print(\"Using CPU\")\n</code></pre>"},{"location":"guide/troubleshooting/#device-mismatch","title":"Device Mismatch","text":"<pre><code># Ensure model and data on same device\nmodel = model.to(device)\ndata = data.to(device)\n\n# Check device\nprint(f\"Model device: {next(model.parameters()).device}\")\nprint(f\"Data device: {data.device}\")\n</code></pre>"},{"location":"guide/troubleshooting/#import-errors","title":"Import Errors","text":""},{"location":"guide/troubleshooting/#module-not-found","title":"Module Not Found","text":"<pre><code># Verify installation\nimport mi_crow\nprint(mi_crow.__version__)\n\n# Check imports\nfrom mi_crow.language_model import LanguageModel\nfrom mi_crow.hooks import Detector\n</code></pre>"},{"location":"guide/troubleshooting/#version-mismatches","title":"Version Mismatches","text":"<pre><code># Check versions\nimport torch\nimport transformers\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\n\n# Update if needed\n# pip install --upgrade torch transformers\n</code></pre>"},{"location":"guide/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"guide/troubleshooting/#check-documentation","title":"Check Documentation","text":"<ul> <li>User guide sections</li> <li>API reference</li> <li>Example notebooks</li> <li>Experiment walkthroughs</li> </ul>"},{"location":"guide/troubleshooting/#debug-information","title":"Debug Information","text":"<pre><code># Enable verbose logging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Check system info\nimport sys\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\n</code></pre>"},{"location":"guide/troubleshooting/#reproduce-issues","title":"Reproduce Issues","text":"<pre><code># Create minimal reproduction\n# 1. Start with simplest possible case\n# 2. Add complexity until issue appears\n# 3. Document exact steps\n\nimport torch\nfrom mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\n\nstore = LocalStore(base_path=\"./store\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nlm = LanguageModel.from_huggingface(\"sshleifer/tiny-gpt2\", store=store, device=device)\nlayers = lm.layers.list_layers()\nprint(layers)\n</code></pre>"},{"location":"guide/troubleshooting/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices - Prevent issues before they occur</li> <li>Examples - See working code</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"guide/hooks/","title":"Hooks System Overview","text":"<p>The hooks system is the foundation of mi-crow's interpretability capabilities. It provides a powerful, flexible framework for intercepting and processing model activations during inference.</p>"},{"location":"guide/hooks/#what-are-hooks","title":"What are Hooks?","text":"<p>Hooks are callbacks that execute at specific points during a neural network's forward pass. In mi-crow, hooks allow you to:</p> <ul> <li>Observe activations without modifying them (Detectors)</li> <li>Modify activations to change model behavior (Controllers)</li> <li>Compose multiple hooks for complex experiments</li> <li>Manage hook lifecycle (register, enable, disable, unregister)</li> </ul>"},{"location":"guide/hooks/#why-hooks-matter","title":"Why Hooks Matter","text":"<p>Hooks are central to everything mi-crow does:</p> <ul> <li>Activation Saving: Detectors capture activations for analysis</li> <li>SAE Integration: SAEs work as hooks to decode activations</li> <li>Concept Discovery: Text tracking uses detectors to collect examples</li> <li>Model Steering: Controllers modify activations to change behavior</li> <li>Experimentation: Combine multiple hooks for intervention studies</li> </ul> <p>Without hooks, you'd need to modify model code directly. Hooks provide a clean, non-invasive way to inspect and control models.</p>"},{"location":"guide/hooks/#hook-architecture","title":"Hook Architecture","text":"<pre><code>graph TD\n    A[Language Model] --&gt; B[Layer Forward Pass]\n    B --&gt; C{Hook Type?}\n    C --&gt;|FORWARD| D[Post-Forward Hook]\n    C --&gt;|PRE_FORWARD| E[Pre-Forward Hook]\n    D --&gt; F[Detector or Controller]\n    E --&gt; F\n    F --&gt; G[Process Activation]\n    G --&gt;|Detector| H[Observe Only]\n    G --&gt;|Controller| I[Modify &amp; Return]\n    H --&gt; J[Continue Forward]\n    I --&gt; J\n</code></pre>"},{"location":"guide/hooks/#detectors-vs-controllers","title":"Detectors vs Controllers","text":""},{"location":"guide/hooks/#detectors","title":"Detectors","text":"<p>Purpose: Observe and collect data without modification</p> <p>Use Cases: - Saving activations for analysis - Tracking statistics (mean, variance, etc.) - Collecting top activating texts - Debugging model behavior</p> <p>Key Property: Detectors never modify activations - they're purely observational.</p>"},{"location":"guide/hooks/#controllers","title":"Controllers","text":"<p>Purpose: Modify activations to change model behavior</p> <p>Use Cases: - Amplifying or suppressing specific features - Concept manipulation through SAE neurons - Intervention experiments - Model steering</p> <p>Key Property: Controllers return modified activations that replace the original.</p>"},{"location":"guide/hooks/#hook-types","title":"Hook Types","text":"<p>Hooks can execute at two points:</p> <ul> <li>PRE_FORWARD: Before a layer processes its input</li> <li>Receives: Layer inputs</li> <li>Can modify: Inputs before processing</li> <li> <p>Use case: Modify what the layer receives</p> </li> <li> <p>FORWARD: After a layer produces its output</p> </li> <li>Receives: Layer outputs</li> <li>Can modify: Outputs before passing to next layer</li> <li>Use case: Modify what the layer produces</li> </ul> <p>Most hooks use FORWARD hooks, as they operate on layer outputs (activations).</p>"},{"location":"guide/hooks/#quick-reference","title":"Quick Reference","text":"Feature Detector Controller Modifies activations \u274c No \u2705 Yes Can save to Store \u2705 Yes Optional Accumulates metadata \u2705 Yes Optional Returns modified value \u274c No \u2705 Yes Use case Observation Intervention"},{"location":"guide/hooks/#documentation-structure","title":"Documentation Structure","text":"<p>This hooks guide is organized into:</p> <ol> <li>Fundamentals - Core concepts, lifecycle, and basics</li> <li>Detectors - Using detector hooks for observation</li> <li>Controllers - Using controller hooks for modification</li> <li>Registration - Managing hooks on layers</li> <li>Advanced - Advanced patterns and best practices</li> </ol>"},{"location":"guide/hooks/#getting-started","title":"Getting Started","text":"<p>If you're new to hooks, start here:</p> <ol> <li>Read Fundamentals to understand the basics</li> <li>Try Detectors for observation tasks</li> <li>Explore Controllers for modification tasks</li> <li>Learn Registration for hook management</li> <li>Check Advanced for complex patterns</li> </ol>"},{"location":"guide/hooks/#example-simple-hook-usage","title":"Example: Simple Hook Usage","text":"<pre><code>from mi_crow.hooks import LayerActivationDetector\nfrom mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\n\n# Setup\nstore = LocalStore(base_path=\"./store\")\nlm = LanguageModel.from_huggingface(\"gpt2\", store=store)\n\n# Create a detector hook\ndetector = LayerActivationDetector(\n    layer_signature=\"transformer.h.0.attn.c_attn\"\n)\n\n# Register the hook\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\n# Run inference - hook automatically executes\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Access collected data\nactivations = detector.tensor_metadata.get(\"activations\")\n\n# Clean up\nlm.layers.unregister_hook(hook_id)\n</code></pre> <p>This simple example demonstrates the core hook workflow: create, register, use, and cleanup.</p>"},{"location":"guide/hooks/#integration-with-other-features","title":"Integration with Other Features","text":"<p>Hooks integrate seamlessly with other mi-crow features:</p> <ul> <li>SAEs: Work as both detectors and controllers</li> <li>Activation Saving: Uses detectors internally</li> <li>Concept Discovery: Uses detectors for text tracking</li> <li>Model Steering: Uses controllers for interventions</li> </ul> <p>Understanding hooks is essential for understanding how mi-crow works under the hood.</p>"},{"location":"guide/hooks/#next-steps","title":"Next Steps","text":"<ul> <li>Hooks Fundamentals - Start here for detailed explanation</li> <li>Using Detectors - Learn about observation hooks</li> <li>Using Controllers - Learn about modification hooks</li> <li>Workflows - See hooks in action</li> </ul>"},{"location":"guide/hooks/advanced/","title":"Advanced Hooks Patterns","text":"<p>This guide covers advanced hook patterns, performance considerations, and complex use cases.</p>"},{"location":"guide/hooks/advanced/#sae-as-both-detector-and-controller","title":"SAE as Both Detector and Controller","text":"<p>SAEs in mi-crow implement both <code>Detector</code> and <code>Controller</code> interfaces, allowing them to:</p> <ul> <li>Detect: Decode activations to sparse latents</li> <li>Control: Modify activations based on concept manipulation</li> </ul>"},{"location":"guide/hooks/advanced/#how-saes-work-as-hooks","title":"How SAEs Work as Hooks","text":"<pre><code>from mi_crow.mechanistic.sae import TopKSae\n\n# Create SAE\nsae = TopKSae(n_latents=512, n_inputs=768, k=8)\n\n# Attach to model (registers as hook internally)\nlm.attach_sae(sae, layer_signature=\"layer_0\")\n\n# SAE now works as both:\n# 1. Detector: Decodes activations to latents\n# 2. Controller: Can modify activations via concept manipulation\n</code></pre>"},{"location":"guide/hooks/advanced/#concept-manipulation-through-sae","title":"Concept Manipulation Through SAE","text":"<pre><code># Enable text tracking (detector functionality)\nsae.concepts.enable_text_tracking(top_k=10)\n\n# Run inference - SAE decodes activations\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Manipulate concepts (controller functionality)\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)\n\n# Run again - activations are modified\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n</code></pre> <p>The SAE automatically handles the dual role - you don't need to manage it as separate hooks.</p>"},{"location":"guide/hooks/advanced/#multi-layer-hook-coordination","title":"Multi-Layer Hook Coordination","text":"<p>Coordinating hooks across multiple layers enables complex interventions:</p>"},{"location":"guide/hooks/advanced/#sequential-layer-processing","title":"Sequential Layer Processing","text":"<pre><code># Register hooks on multiple layers\ndetectors = {}\nfor i in [0, 5, 10]:\n    layer_name = f\"transformer.h.{i}.attn.c_attn\"\n    det = LayerActivationDetector(layer_name)\n    detectors[i] = det\n    lm.layers.register_hook(layer_name, det)\n\n# Process activations sequentially\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Analyze across layers\nfor i, det in detectors.items():\n    acts = det.get_captured()\n    print(f\"Layer {i}: mean={acts.mean().item()}\")\n</code></pre>"},{"location":"guide/hooks/advanced/#cascading-interventions","title":"Cascading Interventions","text":"<pre><code># Modify early layer, then late layer\nearly_controller = FunctionController(\"layer_0\", lambda x: x * 1.2)\nlate_controller = FunctionController(\"layer_10\", lambda x: x * 0.8)\n\nhook1 = lm.layers.register_hook(\"layer_0\", early_controller)\nhook2 = lm.layers.register_hook(\"layer_10\", late_controller)\n\n# Both modifications apply in sequence\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\nlm.layers.unregister_hook(hook1)\nlm.layers.unregister_hook(hook2)\n</code></pre>"},{"location":"guide/hooks/advanced/#cross-layer-communication","title":"Cross-Layer Communication","text":"<pre><code>class CoordinatedController(Controller):\n    \"\"\"Controller that uses information from other layers.\"\"\"\n\n    def __init__(self, layer_signature, reference_detector):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.reference_detector = reference_detector\n\n    def modify_activations(self, module, inputs, output):\n        # Get activations from reference layer\n        ref_activations = self.reference_detector.get_captured()\n\n        if ref_activations is not None:\n            # Modify based on reference layer\n            scale = ref_activations.mean().item()\n            return output * (1.0 + 0.1 * scale)\n\n        return output\n\n# Setup\nref_detector = LayerActivationDetector(\"layer_0\")\nlm.layers.register_hook(\"layer_0\", ref_detector)\n\ncoordinated = CoordinatedController(\"layer_5\", ref_detector)\nlm.layers.register_hook(\"layer_5\", coordinated)\n</code></pre>"},{"location":"guide/hooks/advanced/#conditional-hook-execution","title":"Conditional Hook Execution","text":"<p>Hooks can conditionally execute based on various criteria:</p>"},{"location":"guide/hooks/advanced/#activation-based-conditions","title":"Activation-Based Conditions","text":"<pre><code>class ConditionalController(Controller):\n    \"\"\"Only modifies when activation magnitude exceeds threshold.\"\"\"\n\n    def __init__(self, layer_signature, threshold=1.0):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.threshold = threshold\n\n    def modify_activations(self, module, inputs, output):\n        if output is None:\n            return output\n\n        # Check condition\n        if output.abs().mean().item() &gt; self.threshold:\n            # Only modify if condition met\n            return output * 1.5\n\n        return output\n</code></pre>"},{"location":"guide/hooks/advanced/#token-based-conditions","title":"Token-Based Conditions","text":"<pre><code>class TokenConditionalController(Controller):\n    \"\"\"Modifies only for specific token positions.\"\"\"\n\n    def __init__(self, layer_signature, token_indices):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.token_indices = set(token_indices)\n\n    def modify_activations(self, module, inputs, output):\n        if output is None:\n            return output\n\n        modified = output.clone()\n        # Modify only specified token positions\n        for idx in self.token_indices:\n            if idx &lt; modified.shape[1]:  # seq_len dimension\n                modified[:, idx, :] *= 2.0\n\n        return modified\n</code></pre>"},{"location":"guide/hooks/advanced/#batch-based-conditions","title":"Batch-Based Conditions","text":"<pre><code>class BatchConditionalController(Controller):\n    \"\"\"Modifies only for certain batches.\"\"\"\n\n    def __init__(self, layer_signature):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.batch_count = 0\n\n    def modify_activations(self, module, inputs, output):\n        self.batch_count += 1\n\n        # Modify only every 10th batch\n        if self.batch_count % 10 == 0:\n            return output * 1.5\n\n        return output\n</code></pre>"},{"location":"guide/hooks/advanced/#hook-composition-patterns","title":"Hook Composition Patterns","text":""},{"location":"guide/hooks/advanced/#pipeline-of-transformations","title":"Pipeline of Transformations","text":"<pre><code>class PipelineController(Controller):\n    \"\"\"Applies multiple transformations in sequence.\"\"\"\n\n    def __init__(self, layer_signature, transformations):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.transformations = transformations\n\n    def modify_activations(self, module, inputs, output):\n        result = output\n        for transform in self.transformations:\n            result = transform(result)\n        return result\n\n# Usage\npipeline = PipelineController(\n    \"layer_0\",\n    transformations=[\n        lambda x: x * 1.2,           # Scale\n        lambda x: torch.clamp(x, -2, 2),  # Clamp\n        lambda x: (x - x.mean()) / (x.std() + 1e-8)  # Normalize\n    ]\n)\n</code></pre>"},{"location":"guide/hooks/advanced/#conditional-composition","title":"Conditional Composition","text":"<pre><code>class ConditionalPipeline(Controller):\n    \"\"\"Applies different pipelines based on condition.\"\"\"\n\n    def __init__(self, layer_signature, condition_fn, pipeline_a, pipeline_b):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.condition_fn = condition_fn\n        self.pipeline_a = pipeline_a\n        self.pipeline_b = pipeline_b\n\n    def modify_activations(self, module, inputs, output):\n        if self.condition_fn(output):\n            return self.pipeline_a(output)\n        else:\n            return self.pipeline_b(output)\n</code></pre>"},{"location":"guide/hooks/advanced/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guide/hooks/advanced/#hook-overhead","title":"Hook Overhead","text":"<p>Hooks add overhead to forward passes. Minimize it:</p> <pre><code># \u274c Slow - creates new tensors every time\nclass SlowController(Controller):\n    def modify_activations(self, module, inputs, output):\n        return output.clone() * 2.0  # Unnecessary clone\n\n# \u2705 Fast - in-place when safe, or reuse operations\nclass FastController(Controller):\n    def modify_activations(self, module, inputs, output):\n        return output * 2.0  # No clone needed\n</code></pre>"},{"location":"guide/hooks/advanced/#batch-processing-optimization","title":"Batch Processing Optimization","text":"<pre><code># Process multiple examples efficiently\nhook_ids = []\ntry:\n    # Register once\n    for layer in layers:\n        det = LayerActivationDetector(layer)\n        hook_ids.append(lm.layers.register_hook(layer, det))\n\n    # Process all batches\n    for batch in dataset:\n        outputs, encodings = lm.inference.execute_inference(batch)\n        # Access data after batch\nfinally:\n    # Cleanup once\n    for hook_id in hook_ids:\n        lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/advanced/#memory-management","title":"Memory Management","text":"<pre><code>class MemoryEfficientDetector(Detector):\n    \"\"\"Detector that clears old data automatically.\"\"\"\n\n    def __init__(self, layer_signature, max_batches=100):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.max_batches = max_batches\n        self.batch_count = 0\n\n    def process_activations(self, module, input, output):\n        self.batch_count += 1\n\n        # Clear old data periodically\n        if self.batch_count &gt; self.max_batches:\n            self.tensor_metadata.clear()\n            self.batch_count = 0\n\n        # Store current batch (moved to CPU)\n        if output is not None:\n            self.tensor_metadata['activations'] = output.detach().cpu()\n</code></pre>"},{"location":"guide/hooks/advanced/#memory-management-with-hooks","title":"Memory Management with Hooks","text":""},{"location":"guide/hooks/advanced/#moving-to-cpu","title":"Moving to CPU","text":"<p>Always move large tensors to CPU in detectors:</p> <pre><code>def process_activations(self, module, input, output):\n    if output is not None:\n        # Move to CPU to save GPU memory\n        self.tensor_metadata['activations'] = output.detach().cpu()\n</code></pre>"},{"location":"guide/hooks/advanced/#clearing-old-data","title":"Clearing Old Data","text":"<pre><code># Clear detector data periodically\nif batch_count % 100 == 0:\n    detector.clear_captured()\n    detector.tensor_metadata.clear()\n</code></pre>"},{"location":"guide/hooks/advanced/#limiting-accumulation","title":"Limiting Accumulation","text":"<pre><code>class LimitedAccumulator(Detector):\n    \"\"\"Only keeps last N batches.\"\"\"\n\n    def __init__(self, layer_signature, max_batches=10):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.max_batches = max_batches\n        self.batches = []\n\n    def process_activations(self, module, input, output):\n        self.batches.append(output.detach().cpu())\n        if len(self.batches) &gt; self.max_batches:\n            self.batches.pop(0)  # Remove oldest\n</code></pre>"},{"location":"guide/hooks/advanced/#debugging-hook-execution","title":"Debugging Hook Execution","text":""},{"location":"guide/hooks/advanced/#logging-hook-calls","title":"Logging Hook Calls","text":"<pre><code>class LoggingController(Controller):\n    \"\"\"Logs all modifications for debugging.\"\"\"\n\n    def modify_activations(self, module, inputs, output):\n        import logging\n        logger = logging.getLogger(__name__)\n\n        if output is not None:\n            logger.debug(\n                f\"Hook {self.id}: shape={output.shape}, \"\n                f\"mean={output.mean().item()}, std={output.std().item()}\"\n            )\n\n        return output * 2.0\n</code></pre>"},{"location":"guide/hooks/advanced/#tracking-hook-statistics","title":"Tracking Hook Statistics","text":"<pre><code>class StatisticsController(Controller):\n    \"\"\"Tracks statistics about modifications.\"\"\"\n\n    def __init__(self, layer_signature):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.modification_count = 0\n        self.total_scale = 0.0\n\n    def modify_activations(self, module, inputs, output):\n        self.modification_count += 1\n        scale = 2.0\n        self.total_scale += scale\n\n        return output * scale\n\n    def get_stats(self):\n        return {\n            'modifications': self.modification_count,\n            'avg_scale': self.total_scale / self.modification_count if self.modification_count &gt; 0 else 0\n        }\n</code></pre>"},{"location":"guide/hooks/advanced/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"guide/hooks/advanced/#pitfall-1-in-place-modification","title":"Pitfall 1: In-place Modification","text":"<pre><code># \u274c Wrong - modifies in place\ndef modify_fn(x):\n    x *= 2.0\n    return x\n\n# \u2705 Correct - returns new tensor\ndef modify_fn(x):\n    return x * 2.0\n</code></pre>"},{"location":"guide/hooks/advanced/#pitfall-2-not-handling-none","title":"Pitfall 2: Not Handling None","text":"<pre><code># \u274c Wrong - crashes if output is None\ndef modify_fn(x):\n    return x * 2.0\n\n# \u2705 Correct - handles None\ndef modify_fn(x):\n    if x is None:\n        return None\n    return x * 2.0\n</code></pre>"},{"location":"guide/hooks/advanced/#pitfall-3-memory-leaks","title":"Pitfall 3: Memory Leaks","text":"<pre><code># \u274c Wrong - accumulates without clearing\ndef process_activations(self, module, input, output):\n    self.all_activations.append(output)  # Never cleared!\n\n# \u2705 Correct - clear periodically\ndef process_activations(self, module, input, output):\n    if len(self.all_activations) &gt; 100:\n        self.all_activations.clear()\n    self.all_activations.append(output.detach().cpu())\n</code></pre>"},{"location":"guide/hooks/advanced/#pitfall-4-not-unregistering","title":"Pitfall 4: Not Unregistering","text":"<pre><code># \u274c Wrong - hook never unregistered\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\n# ... use hook ...\n# Forgot to unregister!\n\n# \u2705 Correct - always cleanup\ntry:\n    hook_id = lm.layers.register_hook(\"layer_0\", detector)\n    # ... use hook ...\nfinally:\n    lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Hook Registration - Managing complex hook setups</li> <li>Workflows - Real-world hook usage</li> <li>Best Practices - General best practices</li> </ul>"},{"location":"guide/hooks/controllers/","title":"Using Controller Hooks","text":"<p>Controller hooks modify activations during inference to change model behavior. This guide covers built-in controllers, creating custom controllers, and intervention patterns.</p>"},{"location":"guide/hooks/controllers/#what-are-controllers","title":"What are Controllers?","text":"<p>Controllers are hooks that: - Modify activations during the forward pass - Return modified values that replace originals - Change model behavior in real-time - Enable intervention experiments</p> <p>Controllers are used for: - Concept manipulation through SAE neurons - Activation scaling and masking - Intervention studies - Model steering</p>"},{"location":"guide/hooks/controllers/#built-in-controller-implementations","title":"Built-in Controller Implementations","text":""},{"location":"guide/hooks/controllers/#functioncontroller","title":"FunctionController","text":"<p>Applies a user-provided function to activations.</p> <pre><code>from mi_crow.hooks import FunctionController\nimport torch\n\n# Scale activations by a factor\ncontroller = FunctionController(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    function=lambda x: x * 2.0  # Double all activations\n)\n\n# Register and use\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", controller)\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n</code></pre> <p>Function Requirements: - Takes a <code>torch.Tensor</code> as input - Returns a <code>torch.Tensor</code> of the same shape - Must be deterministic (no random operations)</p> <p>Common Functions:</p> <pre><code># Scale by constant\nscale_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: x * 1.5\n)\n\n# Clamp values\nclamp_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: torch.clamp(x, min=-1.0, max=1.0)\n)\n\n# Add noise (for experimentation)\nnoise_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: x + torch.randn_like(x) * 0.1\n)\n</code></pre>"},{"location":"guide/hooks/controllers/#creating-custom-controllers","title":"Creating Custom Controllers","text":"<p>To create a custom controller, inherit from <code>Controller</code> and implement <code>modify_activations</code>:</p>"},{"location":"guide/hooks/controllers/#simple-scaling-controller","title":"Simple Scaling Controller","text":"<pre><code>from mi_crow.hooks import Controller\nfrom mi_crow.hooks.hook import HookType\nimport torch\n\nclass ScalingController(Controller):\n    \"\"\"Controller that scales activations by a factor.\"\"\"\n\n    def __init__(self, layer_signature: str | int, scale_factor: float):\n        super().__init__(\n            hook_type=HookType.FORWARD,\n            layer_signature=layer_signature\n        )\n        self.scale_factor = scale_factor\n\n    def modify_activations(self, module, inputs, output):\n        \"\"\"Scale the output activations.\"\"\"\n        if output is None:\n            return output\n\n        if isinstance(output, torch.Tensor):\n            return output * self.scale_factor\n        elif isinstance(output, (tuple, list)):\n            # Handle tuple outputs (e.g., (hidden_states, attention_weights))\n            return tuple(x * self.scale_factor if isinstance(x, torch.Tensor) else x \n                        for x in output)\n\n        return output\n</code></pre>"},{"location":"guide/hooks/controllers/#selective-activation-controller","title":"Selective Activation Controller","text":"<pre><code>class SelectiveController(Controller):\n    \"\"\"Controller that modifies only specific neurons.\"\"\"\n\n    def __init__(self, layer_signature: str | int, neuron_indices: list[int], scale: float):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.neuron_indices = set(neuron_indices)\n        self.scale = scale\n\n    def modify_activations(self, module, inputs, output):\n        \"\"\"Scale only specified neurons.\"\"\"\n        if output is None or not isinstance(output, torch.Tensor):\n            return output\n\n        # Create modified output\n        modified = output.clone()\n\n        # Scale only specified neurons (assuming last dimension is features)\n        for idx in self.neuron_indices:\n            if idx &lt; modified.shape[-1]:\n                modified[..., idx] *= self.scale\n\n        return modified\n</code></pre>"},{"location":"guide/hooks/controllers/#conditional-controller","title":"Conditional Controller","text":"<pre><code>class ConditionalController(Controller):\n    \"\"\"Controller that applies modifications conditionally.\"\"\"\n\n    def __init__(self, layer_signature: str | int, condition_fn, modification_fn):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.condition_fn = condition_fn\n        self.modification_fn = modification_fn\n\n    def modify_activations(self, module, inputs, output):\n        \"\"\"Apply modification if condition is met.\"\"\"\n        if output is None:\n            return output\n\n        # Check condition (e.g., based on activation statistics)\n        if self.condition_fn(output):\n            return self.modification_fn(output)\n\n        return output\n</code></pre>"},{"location":"guide/hooks/controllers/#modifying-inputs-vs-outputs","title":"Modifying Inputs vs Outputs","text":""},{"location":"guide/hooks/controllers/#forward-hooks-modify-outputs","title":"FORWARD Hooks (Modify Outputs)","text":"<p>Most common - modify layer outputs:</p> <pre><code>class OutputController(Controller):\n    def modify_activations(self, module, inputs, output):\n        # output is the layer's output\n        # Modify and return\n        return modified_output\n</code></pre> <p>When to use: - Modifying activations after processing - SAE-based interventions - Concept manipulation - Most intervention experiments</p>"},{"location":"guide/hooks/controllers/#pre_forward-hooks-modify-inputs","title":"PRE_FORWARD Hooks (Modify Inputs)","text":"<p>Modify inputs before layer processes them:</p> <pre><code>class InputController(Controller):\n    def __init__(self, layer_signature: str | int):\n        super().__init__(\n            hook_type=HookType.PRE_FORWARD,\n            layer_signature=layer_signature\n        )\n\n    def modify_activations(self, module, inputs, output):\n        # inputs is a tuple of input tensors\n        # Modify and return new input tuple\n        modified_inputs = tuple(x * 2.0 if isinstance(x, torch.Tensor) else x \n                               for x in inputs)\n        return modified_inputs\n</code></pre> <p>When to use: - Early intervention in forward pass - Input preprocessing - Modifying residual connections</p>"},{"location":"guide/hooks/controllers/#activation-scaling-masking-and-transformation","title":"Activation Scaling, Masking, and Transformation","text":""},{"location":"guide/hooks/controllers/#scaling-patterns","title":"Scaling Patterns","text":"<pre><code># Uniform scaling\nuniform_scale = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: x * 1.5\n)\n\n# Per-neuron scaling (requires custom controller)\n# See SelectiveController example above\n\n# Adaptive scaling based on magnitude\nadaptive_scale = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: x * (1.0 + 0.1 * torch.sigmoid(x.abs().mean()))\n)\n</code></pre>"},{"location":"guide/hooks/controllers/#masking-patterns","title":"Masking Patterns","text":"<pre><code># Zero out small activations\nmasking_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: x * (x.abs() &gt; 0.1).float()\n)\n\n# Top-K masking\ntopk_masking = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: torch.where(\n        x.abs() &gt;= torch.topk(x.abs(), k=10, dim=-1)[0][..., -1:],\n        x, torch.zeros_like(x)\n    )\n)\n</code></pre>"},{"location":"guide/hooks/controllers/#transformation-patterns","title":"Transformation Patterns","text":"<pre><code># Normalization\nnormalize_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: (x - x.mean()) / (x.std() + 1e-8)\n)\n\n# Clipping\nclip_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: torch.clamp(x, min=-2.0, max=2.0)\n)\n\n# Non-linear transformation\ntanh_controller = FunctionController(\n    layer_signature=\"layer_0\",\n    function=lambda x: torch.tanh(x * 2.0)\n)\n</code></pre>"},{"location":"guide/hooks/controllers/#use-cases","title":"Use Cases","text":""},{"location":"guide/hooks/controllers/#concept-manipulation","title":"Concept Manipulation","text":"<pre><code># Amplify a specific SAE concept (neuron)\ndef amplify_concept(neuron_idx: int, scale: float):\n    def modify_fn(x):\n        modified = x.clone()\n        modified[..., neuron_idx] *= scale\n        return modified\n\n    return FunctionController(\n        layer_signature=\"layer_0\",\n        function=modify_fn\n    )\n\ncontroller = amplify_concept(neuron_idx=42, scale=2.0)\nhook_id = lm.layers.register_hook(\"layer_0\", controller)\n</code></pre>"},{"location":"guide/hooks/controllers/#intervention-experiments","title":"Intervention Experiments","text":"<pre><code># A/B testing: with and without intervention\nbaseline_outputs, _ = lm.inference.execute_inference([\"prompt\"], with_controllers=False)\n\n# Apply intervention\ncontroller = FunctionController(\"layer_0\", lambda x: x * 1.5)\nhook_id = lm.layers.register_hook(\"layer_0\", controller)\nintervention_outputs, _ = lm.inference.execute_inference([\"prompt\"], with_controllers=True)\n\n# Compare\ndifference = intervention_outputs.logits - baseline_outputs.logits\n</code></pre>"},{"location":"guide/hooks/controllers/#model-steering","title":"Model Steering","text":"<pre><code># Steer model toward certain behaviors\ndef steer_toward_concept(concept_vector: torch.Tensor, strength: float):\n    def modify_fn(x):\n        # Add concept vector scaled by strength\n        return x + strength * concept_vector\n\n    return FunctionController(\"layer_0\", modify_fn)\n\n# Use learned concept vector\nconcept_vec = torch.randn(768)  # Example concept vector\ncontroller = steer_toward_concept(concept_vec, strength=0.1)\nhook_id = lm.layers.register_hook(\"layer_0\", controller)\n</code></pre>"},{"location":"guide/hooks/controllers/#best-practices","title":"Best Practices","text":"<ol> <li>Preserve shapes: Return tensors with the same shape as input</li> <li>Handle None: Check for None before modifying</li> <li>Clone when needed: Use <code>.clone()</code> to avoid in-place modifications</li> <li>Test functions: Verify your function works on sample tensors</li> <li>Document effects: Clearly document what your controller does</li> <li>Clean up: Always unregister controllers when done</li> </ol>"},{"location":"guide/hooks/controllers/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"guide/hooks/controllers/#in-place-modification","title":"In-place Modification","text":"<pre><code># \u274c Wrong - modifies in place\ndef modify_fn(x):\n    x *= 2.0  # In-place modification\n    return x\n\n# \u2705 Correct - return new tensor\ndef modify_fn(x):\n    return x * 2.0  # Creates new tensor\n</code></pre>"},{"location":"guide/hooks/controllers/#shape-mismatches","title":"Shape Mismatches","text":"<pre><code># \u274c Wrong - may cause shape errors\ndef modify_fn(x):\n    return x.reshape(-1)  # Changes shape\n\n# \u2705 Correct - preserve shape\ndef modify_fn(x):\n    return x * 2.0  # Same shape\n</code></pre>"},{"location":"guide/hooks/controllers/#non-deterministic-functions","title":"Non-deterministic Functions","text":"<pre><code># \u274c Wrong - random operations\ndef modify_fn(x):\n    return x + torch.randn_like(x)  # Non-deterministic\n\n# \u2705 Correct - deterministic (or document randomness)\ndef modify_fn(x):\n    return x * 2.0  # Deterministic\n</code></pre>"},{"location":"guide/hooks/controllers/#integration-with-saes","title":"Integration with SAEs","text":"<p>SAEs work as controllers when attached to models:</p> <pre><code># SAE automatically works as a controller\nlm.attach_sae(sae, layer_signature=\"layer_0\")\n\n# SAE decodes activations and can modify them\n# Concept manipulation uses SAE as controller\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)\n</code></pre>"},{"location":"guide/hooks/controllers/#next-steps","title":"Next Steps","text":"<ul> <li>Hook Registration - Managing controllers on layers</li> <li>Advanced Patterns - Complex controller patterns</li> <li>Concept Manipulation - Using controllers for concepts</li> <li>Activation Control - Workflow guide</li> </ul>"},{"location":"guide/hooks/detectors/","title":"Using Detector Hooks","text":"<p>Detector hooks are used to observe and collect data from model activations without modifying them. This guide covers built-in detectors, creating custom detectors, and common use cases.</p>"},{"location":"guide/hooks/detectors/#what-are-detectors","title":"What are Detectors?","text":"<p>Detectors are hooks that: - Observe activations during inference - Collect data (tensors, metadata, statistics) - Never modify activations - they're purely observational - Can save data to the Store for persistence</p> <p>Detectors are perfect for: - Saving activations for analysis - Tracking statistics across batches - Collecting examples for concept discovery - Debugging model behavior</p>"},{"location":"guide/hooks/detectors/#built-in-detector-implementations","title":"Built-in Detector Implementations","text":""},{"location":"guide/hooks/detectors/#layeractivationdetector","title":"LayerActivationDetector","text":"<p>Captures activations from a specific layer.</p> <pre><code>from mi_crow.hooks import LayerActivationDetector\n\n# Create detector\ndetector = LayerActivationDetector(\n    layer_signature=\"transformer.h.0.attn.c_attn\"\n)\n\n# Register on model\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\n# Run inference\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Access captured activations\nactivations = detector.get_captured()\nprint(f\"Activations shape: {activations.shape}\")\n\n# Clear for next batch\ndetector.clear_captured()\n</code></pre> <p>Key Methods: - <code>get_captured()</code>: Get the current batch's activations - <code>clear_captured()</code>: Clear stored activations - <code>tensor_metadata['activations']</code>: Direct access to tensor - <code>metadata['activations_shape']</code>: Shape information</p> <p>Use Cases: - Saving activations for SAE training - Analyzing activation patterns - Debugging layer outputs</p>"},{"location":"guide/hooks/detectors/#modelinputdetector","title":"ModelInputDetector","text":"<p>Captures model inputs (tokenized text).</p> <pre><code>from mi_crow.hooks import ModelInputDetector\n\n# Create detector\ndetector = ModelInputDetector()\n\n# Register on model (attaches to input layer)\nhook_id = lm.layers.register_hook(\"input\", detector)\n\n# Run inference\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Access inputs\ninputs = detector.tensor_metadata.get(\"inputs\")\n</code></pre> <p>Use Cases: - Tracking input tokens - Saving input-output pairs - Attention mask handling</p>"},{"location":"guide/hooks/detectors/#modeloutputdetector","title":"ModelOutputDetector","text":"<p>Captures final model outputs.</p> <pre><code>from mi_crow.hooks import ModelOutputDetector\n\n# Create detector\ndetector = ModelOutputDetector()\n\n# Register on output layer\nhook_id = lm.layers.register_hook(\"output\", detector)\n\n# Run inference\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Access outputs\nmodel_outputs = detector.tensor_metadata.get(\"outputs\")\n</code></pre> <p>Use Cases: - Saving model predictions - Analyzing output distributions - Collecting generation results</p>"},{"location":"guide/hooks/detectors/#creating-custom-detectors","title":"Creating Custom Detectors","text":"<p>To create a custom detector, inherit from <code>Detector</code> and implement <code>process_activations</code>:</p> <pre><code>from mi_crow.hooks import Detector\nfrom mi_crow.hooks.hook import HookType\nimport torch\n\nclass StatisticsDetector(Detector):\n    \"\"\"Detector that tracks activation statistics.\"\"\"\n\n    def __init__(self, layer_signature: str | int):\n        super().__init__(\n            hook_type=HookType.FORWARD,\n            layer_signature=layer_signature\n        )\n        # Initialize statistics\n        self.metadata['mean'] = 0.0\n        self.metadata['std'] = 0.0\n        self.metadata['count'] = 0\n\n    def process_activations(self, module, input, output):\n        \"\"\"Process and accumulate statistics.\"\"\"\n        # Extract tensor from output\n        tensor = output\n        if isinstance(output, (tuple, list)):\n            tensor = output[0]\n\n        if tensor is not None and isinstance(tensor, torch.Tensor):\n            # Update running statistics\n            batch_mean = tensor.mean().item()\n            batch_std = tensor.std().item()\n            count = self.metadata['count']\n\n            # Running average\n            total = count + 1\n            self.metadata['mean'] = (\n                (self.metadata['mean'] * count + batch_mean) / total\n            )\n            self.metadata['std'] = (\n                (self.metadata['std'] * count + batch_std) / total\n            )\n            self.metadata['count'] = total\n</code></pre> <p>Key Points: - Inherit from <code>Detector</code> - Implement <code>process_activations(module, input, output)</code> - Use <code>self.metadata</code> for scalar data - Use <code>self.tensor_metadata</code> for tensors - Don't return anything (detectors don't modify)</p>"},{"location":"guide/hooks/detectors/#accumulating-metadata-across-batches","title":"Accumulating Metadata Across Batches","text":"<p>Detectors can accumulate data across multiple forward passes:</p> <pre><code>class BatchAccumulator(Detector):\n    \"\"\"Accumulates activations across batches.\"\"\"\n\n    def __init__(self, layer_signature: str | int):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.all_activations = []\n\n    def process_activations(self, module, input, output):\n        tensor = output\n        if isinstance(output, (tuple, list)):\n            tensor = output[0]\n\n        if tensor is not None:\n            # Store each batch (move to CPU to save memory)\n            self.all_activations.append(tensor.detach().cpu())\n\n    def get_all_activations(self):\n        \"\"\"Get all accumulated activations.\"\"\"\n        if self.all_activations:\n            return torch.cat(self.all_activations, dim=0)\n        return None\n</code></pre> <p>Memory Considerations: - Move tensors to CPU: <code>tensor.detach().cpu()</code> - Consider batching large accumulations - Clear old data periodically</p>"},{"location":"guide/hooks/detectors/#saving-detector-data-to-store","title":"Saving Detector Data to Store","text":"<p>Detectors can save data to the Store:</p> <pre><code>from mi_crow.store import LocalStore\nfrom mi_crow.hooks import LayerActivationDetector\n\nstore = LocalStore(base_path=\"./store\")\n\n# Create detector with store\ndetector = LayerActivationDetector(\n    layer_signature=\"transformer.h.0.attn.c_attn\"\n)\ndetector.store = store  # Attach store\n\n# Register and use\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\n# After inference, save data\nactivations = detector.get_captured()\nif activations is not None:\n    # Save to store (example - actual API may vary)\n    store.save_tensor(\"activations\", activations)\n</code></pre>"},{"location":"guide/hooks/detectors/#use-cases","title":"Use Cases","text":""},{"location":"guide/hooks/detectors/#activation-analysis","title":"Activation Analysis","text":"<pre><code>detector = LayerActivationDetector(\"transformer.h.0.attn.c_attn\")\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\n# Analyze multiple examples\nfor text in dataset:\n    lm.inference.execute_inference([text])\n    activations = detector.get_captured()\n    print(f\"Mean activation: {activations.mean().item()}\")\n    detector.clear_captured()\n</code></pre>"},{"location":"guide/hooks/detectors/#debugging-model-behavior","title":"Debugging Model Behavior","text":"<pre><code># Track activations at multiple layers\ndetectors = {}\nfor layer_name in [\"layer_0\", \"layer_5\", \"layer_10\"]:\n    det = LayerActivationDetector(layer_name)\n    detectors[layer_name] = det\n    lm.layers.register_hook(layer_name, det)\n\n# Run inference and inspect\noutputs, encodings = lm.inference.execute_inference([\"Debug this\"])\n\nfor name, det in detectors.items():\n    acts = det.get_captured()\n    print(f\"{name}: {acts.shape}, mean={acts.mean().item()}\")\n</code></pre>"},{"location":"guide/hooks/detectors/#data-collection-for-training","title":"Data Collection for Training","text":"<pre><code># Collect activations for SAE training\ndetector = LayerActivationDetector(\"transformer.h.0.attn.c_attn\")\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\nall_activations = []\nfor batch in dataset:\n    lm.inference.execute_inference(batch)\n    acts = detector.get_captured()\n    all_activations.append(acts.detach().cpu())\n    detector.clear_captured()\n\n# Concatenate for training\ntraining_data = torch.cat(all_activations, dim=0)\n</code></pre>"},{"location":"guide/hooks/detectors/#best-practices","title":"Best Practices","text":"<ol> <li>Clear between batches: Use <code>clear_captured()</code> to avoid memory leaks</li> <li>Move to CPU: Detach and move to CPU for large accumulations</li> <li>Use metadata: Store scalar statistics in <code>metadata</code>, not <code>tensor_metadata</code></li> <li>Handle None: Check for None before accessing tensors</li> <li>Error handling: Wrap operations in try/except blocks</li> </ol>"},{"location":"guide/hooks/detectors/#integration-with-other-features","title":"Integration with Other Features","text":"<p>Detectors integrate with:</p> <ul> <li>Activation Saving: <code>lm.activations.save()</code> uses detectors internally</li> <li>SAE Training: Collect activations for training data</li> <li>Concept Discovery: Track top activating texts</li> <li>Store: Save detector data persistently</li> </ul>"},{"location":"guide/hooks/detectors/#next-steps","title":"Next Steps","text":"<ul> <li>Using Controllers - Learn about modification hooks</li> <li>Hook Registration - Managing detectors on layers</li> <li>Advanced Patterns - Complex detector patterns</li> <li>Saving Activations - Workflow guide</li> </ul>"},{"location":"guide/hooks/fundamentals/","title":"Hooks Fundamentals","text":"<p>This guide covers the fundamental concepts of the hooks system: the base Hook class, hook types, lifecycle, and basic usage patterns.</p>"},{"location":"guide/hooks/fundamentals/#understanding-the-hook-base-class","title":"Understanding the Hook Base Class","text":"<p>All hooks in mi-crow inherit from the <code>Hook</code> base class, which provides:</p> <ul> <li>Unique identification: Each hook has a unique ID</li> <li>Layer association: Hooks know which layer they're attached to</li> <li>Type specification: FORWARD or PRE_FORWARD</li> <li>Enable/disable: Toggle hook execution without unregistering</li> <li>Context access: Access to the language model context</li> </ul>"},{"location":"guide/hooks/fundamentals/#hook-initialization","title":"Hook Initialization","text":"<pre><code>from mi_crow.hooks.hook import Hook, HookType\n\n# Hooks are typically created by subclasses\n# But you can see the initialization parameters:\n\nhook = SomeHook(\n    layer_signature=\"transformer.h.0.attn.c_attn\",  # Optional: layer name\n    hook_type=HookType.FORWARD,                     # FORWARD or PRE_FORWARD\n    hook_id=\"my-custom-id\"                          # Optional: custom ID\n)\n</code></pre>"},{"location":"guide/hooks/fundamentals/#hook-id","title":"Hook ID","text":"<p>Every hook gets a unique ID, either: - Auto-generated UUID if not provided - Custom ID if specified during creation</p> <p>The ID is used for: - Unregistering hooks - Looking up hooks in the registry - Error reporting</p>"},{"location":"guide/hooks/fundamentals/#hook-types-forward-vs-pre_forward","title":"Hook Types: FORWARD vs PRE_FORWARD","text":"<p>Hooks execute at different points in the forward pass:</p>"},{"location":"guide/hooks/fundamentals/#forward-hooks","title":"FORWARD Hooks","text":"<p>Execute after a layer produces its output.</p> <pre><code># Hook receives the layer's output\ndef hook_fn(module, input, output):\n    # output is the layer's activation\n    # Can modify and return new output\n    return modified_output\n</code></pre> <p>When to use: - Most common hook type - Operating on layer activations (outputs) - SAE decoding - Activation analysis - Concept manipulation</p>"},{"location":"guide/hooks/fundamentals/#pre_forward-hooks","title":"PRE_FORWARD Hooks","text":"<p>Execute before a layer processes its input.</p> <pre><code># Hook receives the layer's input\ndef hook_fn(module, input):\n    # input is the layer's input tuple\n    # Can modify and return new input\n    return modified_input\n</code></pre> <p>When to use: - Modifying inputs before processing - Early intervention in the forward pass - Input preprocessing</p>"},{"location":"guide/hooks/fundamentals/#choosing-hook-type","title":"Choosing Hook Type","text":"<p>Most use cases use FORWARD hooks because: - Activations (outputs) are what we typically analyze - SAEs decode outputs, not inputs - Concept manipulation operates on outputs</p> <p>Use PRE_FORWARD only when you need to modify inputs.</p>"},{"location":"guide/hooks/fundamentals/#hook-lifecycle","title":"Hook Lifecycle","text":"<p>Understanding the hook lifecycle is crucial for proper usage:</p>"},{"location":"guide/hooks/fundamentals/#1-creation","title":"1. Creation","text":"<pre><code>from mi_crow.hooks import LayerActivationDetector\n\ndetector = LayerActivationDetector(\n    layer_signature=\"transformer.h.0.attn.c_attn\"\n)\n</code></pre> <p>At this point, the hook exists but isn't active.</p>"},{"location":"guide/hooks/fundamentals/#2-registration","title":"2. Registration","text":"<pre><code>hook_id = lm.layers.register_hook(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    hook=detector\n)\n</code></pre> <p>Registration: - Attaches the hook to the specified layer - Creates a PyTorch hook handle - Adds hook to the registry - Returns the hook ID</p>"},{"location":"guide/hooks/fundamentals/#3-execution","title":"3. Execution","text":"<p>During inference, the hook automatically executes:</p> <pre><code># Hook executes automatically during forward pass\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n</code></pre> <p>The hook's <code>_hook_fn</code> method is called for each forward pass.</p>"},{"location":"guide/hooks/fundamentals/#4-enabledisable","title":"4. Enable/Disable","text":"<p>You can temporarily disable hooks without unregistering:</p> <pre><code># Disable\ndetector.disable()\n\n# Hook won't execute\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Re-enable\ndetector.enable()\n\n# Hook executes again\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n</code></pre>"},{"location":"guide/hooks/fundamentals/#5-cleanup","title":"5. Cleanup","text":"<p>Always unregister hooks when done:</p> <pre><code>lm.layers.unregister_hook(hook_id)\n</code></pre> <p>This: - Removes the PyTorch hook handle - Removes hook from registry - Prevents memory leaks</p>"},{"location":"guide/hooks/fundamentals/#hook-context-and-layer-signatures","title":"Hook Context and Layer Signatures","text":""},{"location":"guide/hooks/fundamentals/#layer-signatures","title":"Layer Signatures","text":"<p>Layer signatures identify which layer to attach a hook to:</p> <pre><code># By name (string)\nlayer_signature = \"transformer.h.0.attn.c_attn\"\n\n# By index (integer)\nlayer_signature = 0  # First layer\n\n# Register hook\nhook_id = lm.layers.register_hook(layer_signature, hook)\n</code></pre> <p>You can find available layers:</p> <pre><code># List all layer names\nlayer_names = lm.layers.list_layers()\nprint(layer_names)\n</code></pre>"},{"location":"guide/hooks/fundamentals/#hook-context","title":"Hook Context","text":"<p>When registered, hooks receive access to the language model context:</p> <pre><code># Context is automatically set during registration\n# Access it in your hook implementation:\n\nclass MyHook(Hook):\n    def _hook_fn(self, module, input, output):\n        # Access context\n        context = self._context\n        model = context.language_model\n        # Use context for advanced operations\n        ...\n</code></pre> <p>The context provides access to: - The language model instance - The layers manager - The store - Other registered hooks</p>"},{"location":"guide/hooks/fundamentals/#enabling-and-disabling-hooks","title":"Enabling and Disabling Hooks","text":"<p>Hooks can be enabled/disabled without unregistering:</p> <pre><code># Disable a hook\nhook.disable()\n\n# Check if enabled\nif hook.is_enabled():\n    print(\"Hook is active\")\n\n# Re-enable\nhook.enable()\n</code></pre> <p>Use cases: - Temporarily skip hook execution - A/B testing (with vs without hook) - Performance optimization - Conditional execution</p>"},{"location":"guide/hooks/fundamentals/#hook-error-handling","title":"Hook Error Handling","text":"<p>Hooks have built-in error handling:</p> <pre><code>from mi_crow.hooks.hook import HookError\n\ntry:\n    outputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\nexcept HookError as e:\n    print(f\"Hook {e.hook_id} failed: {e.original_error}\")\n</code></pre> <p>Hook errors: - Don't crash the entire forward pass - Are wrapped in <code>HookError</code> with context - Include hook ID and original error - Allow graceful degradation</p>"},{"location":"guide/hooks/fundamentals/#best-practices","title":"Best Practices","text":"<ol> <li>Handle errors in hooks: Don't let exceptions propagate</li> <li>Validate inputs: Check tensor shapes and types</li> <li>Use try/except: Catch and handle errors gracefully</li> <li>Log errors: Use logging for debugging</li> </ol>"},{"location":"guide/hooks/fundamentals/#basic-usage-pattern","title":"Basic Usage Pattern","text":"<p>Here's the standard pattern for using hooks:</p> <pre><code>from mi_crow.hooks import LayerActivationDetector\nfrom mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\n\n# 1. Setup\nstore = LocalStore(base_path=\"./store\")\nlm = LanguageModel.from_huggingface(\"gpt2\", store=store)\n\n# 2. Create hook\ndetector = LayerActivationDetector(\n    layer_signature=\"transformer.h.0.attn.c_attn\"\n)\n\n# 3. Register hook\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\ntry:\n    # 4. Use hook (runs automatically)\n    outputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n    # 5. Access hook data\n    activations = detector.tensor_metadata.get(\"activations\")\n\nfinally:\n    # 6. Always cleanup\n    lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/fundamentals/#common-patterns","title":"Common Patterns","text":""},{"location":"guide/hooks/fundamentals/#multiple-hooks-on-different-layers","title":"Multiple Hooks on Different Layers","text":"<pre><code># Register hooks on multiple layers\nhook1_id = lm.layers.register_hook(\"layer_0\", detector1)\nhook2_id = lm.layers.register_hook(\"layer_10\", detector2)\n\n# All hooks execute during forward pass\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Cleanup all\nlm.layers.unregister_hook(hook1_id)\nlm.layers.unregister_hook(hook2_id)\n</code></pre>"},{"location":"guide/hooks/fundamentals/#conditional-hook-execution","title":"Conditional Hook Execution","text":"<pre><code>class ConditionalHook(Detector):\n    def _hook_fn(self, module, input, output):\n        if some_condition:\n            # Only process when condition is met\n            self.process_activation(output)\n</code></pre>"},{"location":"guide/hooks/fundamentals/#hook-composition","title":"Hook Composition","text":"<pre><code># Register multiple hooks on same layer (if compatible)\n# Note: Only one hook class type (Detector or Controller) per layer\ndetector_id = lm.layers.register_hook(\"layer_0\", detector)\n# Can't register another detector on same layer\n# But can register a controller if needed\n</code></pre>"},{"location":"guide/hooks/fundamentals/#next-steps","title":"Next Steps","text":"<p>Now that you understand the fundamentals:</p> <ul> <li>Using Detectors - Learn about detector hooks</li> <li>Using Controllers - Learn about controller hooks</li> <li>Hook Registration - Detailed registration guide</li> <li>Advanced Patterns - Complex hook patterns</li> </ul>"},{"location":"guide/hooks/registration/","title":"Hook Registration and Management","text":"<p>This guide covers registering hooks on layers, managing hook lifecycles, and best practices for hook management.</p>"},{"location":"guide/hooks/registration/#registering-hooks-on-layers","title":"Registering Hooks on Layers","text":""},{"location":"guide/hooks/registration/#basic-registration","title":"Basic Registration","text":"<p>The simplest way to register a hook:</p> <pre><code>from mi_crow.hooks import LayerActivationDetector\n\n# Create hook\ndetector = LayerActivationDetector(layer_signature=\"transformer.h.0.attn.c_attn\")\n\n# Register on model\nhook_id = lm.layers.register_hook(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    hook=detector\n)\n\n# Hook is now active and will execute during forward passes\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n</code></pre> <p>The <code>register_hook</code> method: - Returns the hook's unique ID - Automatically sets the hook's layer signature - Attaches the hook to the PyTorch layer - Adds the hook to the registry</p>"},{"location":"guide/hooks/registration/#finding-layers","title":"Finding Layers","text":"<p>Before registering, you need to know the layer name:</p> <pre><code># List all available layers\nlayer_names = lm.layers.list_layers()\nprint(layer_names)\n\n# Example output:\n# ['transformer.wte', 'transformer.h.0.attn.c_attn', ...]\n</code></pre> <p>You can also use layer indices:</p> <pre><code># Register on first layer (index 0)\nhook_id = lm.layers.register_hook(0, detector)\n</code></pre>"},{"location":"guide/hooks/registration/#layer-signatures","title":"Layer Signatures","text":"<p>Layer signatures can be: - String: Exact layer name (e.g., <code>\"transformer.h.0.attn.c_attn\"</code>) - Integer: Layer index (e.g., <code>0</code> for first layer)</p> <p>The system automatically resolves layer names to actual PyTorch modules.</p>"},{"location":"guide/hooks/registration/#hook-id-management","title":"Hook ID Management","text":""},{"location":"guide/hooks/registration/#automatic-ids","title":"Automatic IDs","text":"<p>If you don't specify an ID, one is auto-generated:</p> <pre><code>detector = LayerActivationDetector(\"layer_0\")\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\nprint(hook_id)  # e.g., \"550e8400-e29b-41d4-a716-446655440000\"\n</code></pre>"},{"location":"guide/hooks/registration/#custom-ids","title":"Custom IDs","text":"<p>You can specify custom IDs for easier management:</p> <pre><code>detector = LayerActivationDetector(\n    layer_signature=\"layer_0\",\n    hook_id=\"my-detector-1\"\n)\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\nassert hook_id == \"my-detector-1\"\n</code></pre> <p>Use cases for custom IDs: - Organizing hooks by experiment - Easier debugging and logging - Referencing hooks in configuration files</p>"},{"location":"guide/hooks/registration/#id-uniqueness","title":"ID Uniqueness","text":"<p>Hook IDs must be unique across all registered hooks:</p> <pre><code># \u274c This will raise ValueError\ndetector1 = LayerActivationDetector(\"layer_0\", hook_id=\"same-id\")\ndetector2 = LayerActivationDetector(\"layer_1\", hook_id=\"same-id\")  # Error!\n\n# \u2705 Use unique IDs\ndetector1 = LayerActivationDetector(\"layer_0\", hook_id=\"detector-layer-0\")\ndetector2 = LayerActivationDetector(\"layer_1\", hook_id=\"detector-layer-1\")\n</code></pre>"},{"location":"guide/hooks/registration/#multiple-hooks-on-same-layer","title":"Multiple Hooks on Same Layer","text":""},{"location":"guide/hooks/registration/#restrictions","title":"Restrictions","text":"<p>Important: Only one hook class type (Detector or Controller) can be registered per layer.</p> <pre><code># \u2705 This works - both are Detectors\ndetector1 = LayerActivationDetector(\"layer_0\")\ndetector2 = ModelInputDetector()\n# But wait - you can't register two Detectors on the same layer\n\n# \u274c This raises ValueError - mixing Detector and Controller\ndetector = LayerActivationDetector(\"layer_0\")\ncontroller = FunctionController(\"layer_0\", lambda x: x * 2.0)\nlm.layers.register_hook(\"layer_0\", detector)\nlm.layers.register_hook(\"layer_0\", controller)  # Error!\n</code></pre>"},{"location":"guide/hooks/registration/#multiple-hooks-on-different-layers","title":"Multiple Hooks on Different Layers","text":"<p>You can register hooks on multiple layers:</p> <pre><code># Register detectors on different layers\ndetector1 = LayerActivationDetector(\"layer_0\")\ndetector2 = LayerActivationDetector(\"layer_5\")\ndetector3 = LayerActivationDetector(\"layer_10\")\n\nhook1_id = lm.layers.register_hook(\"layer_0\", detector1)\nhook2_id = lm.layers.register_hook(\"layer_5\", detector2)\nhook3_id = lm.layers.register_hook(\"layer_10\", detector3)\n\n# All execute during forward pass\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n</code></pre>"},{"location":"guide/hooks/registration/#unregistering-hooks","title":"Unregistering Hooks","text":"<p>Always unregister hooks when done to prevent memory leaks:</p>"},{"location":"guide/hooks/registration/#by-hook-id","title":"By Hook ID","text":"<pre><code># Register\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\n\n# Use hook\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Unregister\nlm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/registration/#by-hook-instance","title":"By Hook Instance","text":"<pre><code># Register\nhook_id = lm.layers.register_hook(\"layer_0\", detector)\n\n# Unregister using hook instance\nlm.layers.unregister_hook(detector)  # Works too!\n</code></pre>"},{"location":"guide/hooks/registration/#unregistering-multiple-hooks","title":"Unregistering Multiple Hooks","text":"<pre><code>hook_ids = []\n\n# Register multiple hooks\nfor layer_name in [\"layer_0\", \"layer_5\", \"layer_10\"]:\n    det = LayerActivationDetector(layer_name)\n    hook_id = lm.layers.register_hook(layer_name, det)\n    hook_ids.append(hook_id)\n\n# Unregister all\nfor hook_id in hook_ids:\n    lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/registration/#safe-unregistering","title":"Safe Unregistering","text":"<p>Unregistering returns <code>True</code> if successful, <code>False</code> if hook not found:</p> <pre><code>success = lm.layers.unregister_hook(hook_id)\nif not success:\n    print(f\"Hook {hook_id} not found\")\n</code></pre>"},{"location":"guide/hooks/registration/#listing-and-querying-registered-hooks","title":"Listing and Querying Registered Hooks","text":""},{"location":"guide/hooks/registration/#get-all-hooks","title":"Get All Hooks","text":"<pre><code># Get all registered hooks\nall_hooks = lm.layers.get_hooks()\nprint(f\"Total hooks: {len(all_hooks)}\")\n</code></pre>"},{"location":"guide/hooks/registration/#get-hooks-by-layer","title":"Get Hooks by Layer","text":"<pre><code># Get hooks on a specific layer\nlayer_hooks = lm.layers.get_hooks(layer_signature=\"layer_0\")\nprint(f\"Hooks on layer_0: {len(layer_hooks)}\")\n</code></pre>"},{"location":"guide/hooks/registration/#get-hooks-by-type","title":"Get Hooks by Type","text":"<pre><code>from mi_crow.hooks import Detector, Controller\n\n# Get only detectors\ndetectors = lm.layers.get_detectors()\nprint(f\"Detectors: {len(detectors)}\")\n\n# Get only controllers\ncontrollers = lm.layers.get_controllers()\nprint(f\"Controllers: {len(controllers)}\")\n</code></pre>"},{"location":"guide/hooks/registration/#check-if-hook-is-registered","title":"Check if Hook is Registered","text":"<pre><code># Check by ID\nif hook_id in lm.layers.context._hook_id_map:\n    print(\"Hook is registered\")\n\n# Or try to get it\nhook_info = lm.layers.context._hook_id_map.get(hook_id)\nif hook_info:\n    layer, hook_type, hook = hook_info\n    print(f\"Hook on {layer}, type: {hook_type}\")\n</code></pre>"},{"location":"guide/hooks/registration/#hook-registry-inspection","title":"Hook Registry Inspection","text":"<p>The hook registry is accessible through the context:</p> <pre><code># Access registry directly\nregistry = lm.layers.context._hook_registry\n\n# Structure: {layer_signature: {hook_type: [(hook, handle), ...]}}\nfor layer_name, hook_types in registry.items():\n    print(f\"Layer: {layer_name}\")\n    for hook_type, hooks in hook_types.items():\n        print(f\"  {hook_type}: {len(hooks)} hooks\")\n</code></pre>"},{"location":"guide/hooks/registration/#id-map","title":"ID Map","text":"<p>The ID map provides quick lookup:</p> <pre><code>id_map = lm.layers.context._hook_id_map\n\n# Structure: {hook_id: (layer_signature, hook_type, hook)}\nfor hook_id, (layer, hook_type, hook) in id_map.items():\n    print(f\"{hook_id}: {hook.__class__.__name__} on {layer}\")\n</code></pre>"},{"location":"guide/hooks/registration/#best-practices-for-hook-lifecycle-management","title":"Best Practices for Hook Lifecycle Management","text":""},{"location":"guide/hooks/registration/#1-use-context-managers-recommended-pattern","title":"1. Use Context Managers (Recommended Pattern)","text":"<pre><code>class HookContext:\n    \"\"\"Context manager for hook lifecycle.\"\"\"\n\n    def __init__(self, layers, hook, layer_signature):\n        self.layers = layers\n        self.hook = hook\n        self.layer_signature = layer_signature\n        self.hook_id = None\n\n    def __enter__(self):\n        self.hook_id = self.layers.register_hook(\n            self.layer_signature, \n            self.hook\n        )\n        return self.hook\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.hook_id:\n            self.layers.unregister_hook(self.hook_id)\n\n# Usage\nwith HookContext(lm.layers, detector, \"layer_0\") as hook:\n    outputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n    activations = hook.get_captured()\n# Hook automatically unregistered\n</code></pre>"},{"location":"guide/hooks/registration/#2-register-in-tryfinally","title":"2. Register in Try/Finally","text":"<pre><code>hook_id = None\ntry:\n    detector = LayerActivationDetector(\"layer_0\")\n    hook_id = lm.layers.register_hook(\"layer_0\", detector)\n    outputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\nfinally:\n    if hook_id:\n        lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/registration/#3-track-hook-ids","title":"3. Track Hook IDs","text":"<pre><code>class Experiment:\n    def __init__(self):\n        self.hook_ids = []\n\n    def add_hook(self, layers, hook, layer_signature):\n        hook_id = layers.register_hook(layer_signature, hook)\n        self.hook_ids.append(hook_id)\n        return hook_id\n\n    def cleanup(self, layers):\n        for hook_id in self.hook_ids:\n            layers.unregister_hook(hook_id)\n        self.hook_ids.clear()\n\n# Usage\nexp = Experiment()\nexp.add_hook(lm.layers, detector, \"layer_0\")\n# ... use hooks ...\nexp.cleanup(lm.layers)\n</code></pre>"},{"location":"guide/hooks/registration/#4-disable-instead-of-unregister","title":"4. Disable Instead of Unregister","text":"<p>For temporary disabling:</p> <pre><code># Disable temporarily\ndetector.disable()\noutputs, encodings = lm.inference.execute_inference([\"Hello\"])  # Hook doesn't execute\n\n# Re-enable\ndetector.enable()\noutputs, encodings = lm.inference.execute_inference([\"Hello\"])  # Hook executes again\n\n# Unregister when truly done\nlm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/registration/#common-patterns","title":"Common Patterns","text":""},{"location":"guide/hooks/registration/#conditional-registration","title":"Conditional Registration","text":"<pre><code>def register_if_needed(layers, hook, layer_signature, condition):\n    if condition:\n        return layers.register_hook(layer_signature, hook)\n    return None\n</code></pre>"},{"location":"guide/hooks/registration/#batch-processing-with-hooks","title":"Batch Processing with Hooks","text":"<pre><code>hook_ids = []\n\ntry:\n    # Register hooks\n    for layer in [\"layer_0\", \"layer_5\"]:\n        det = LayerActivationDetector(layer)\n        hook_id = lm.layers.register_hook(layer, det)\n        hook_ids.append(hook_id)\n\n    # Process batches\n    for batch in dataset:\n        outputs, encodings = lm.inference.execute_inference(batch)\n        # Access detector data\n        for hook_id in hook_ids:\n            # Get hook and access data\n            pass\nfinally:\n    # Cleanup\n    for hook_id in hook_ids:\n        lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/hooks/registration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/hooks/registration/#hook-not-executing","title":"Hook Not Executing","text":"<ul> <li>Check hook is registered: <code>hook_id in lm.layers.context._hook_id_map</code></li> <li>Verify hook is enabled: <code>hook.is_enabled()</code></li> <li>Check layer signature is correct: <code>lm.layers.list_layers()</code></li> </ul>"},{"location":"guide/hooks/registration/#memory-leaks","title":"Memory Leaks","text":"<ul> <li>Always unregister hooks when done</li> <li>Use context managers or try/finally blocks</li> <li>Check registry size: <code>len(lm.layers.context._hook_id_map)</code></li> </ul>"},{"location":"guide/hooks/registration/#registration-errors","title":"Registration Errors","text":"<ul> <li>Ensure hook IDs are unique</li> <li>Don't mix Detector and Controller on same layer</li> <li>Verify layer signature exists</li> </ul>"},{"location":"guide/hooks/registration/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Patterns - Complex hook management patterns</li> <li>Using Detectors - Detector-specific registration</li> <li>Using Controllers - Controller-specific registration</li> </ul>"},{"location":"guide/workflows/","title":"Workflows","text":"<p>This section provides step-by-step guides for common tasks in mi-crow. Each workflow is designed to be self-contained and complete.</p>"},{"location":"guide/workflows/#available-workflows","title":"Available Workflows","text":""},{"location":"guide/workflows/#saving-activations","title":"Saving Activations","text":"<p>Learn how to collect and save activations from model layers for analysis and SAE training.</p> <p>When to use: - Preparing data for SAE training - Analyzing activation patterns - Debugging model behavior - Collecting datasets for research</p>"},{"location":"guide/workflows/#training-sae-models","title":"Training SAE Models","text":"<p>Complete guide to training sparse autoencoders on saved activations.</p> <p>When to use: - Training your first SAE - Understanding hyperparameter selection - Monitoring training progress - Saving and loading trained models</p>"},{"location":"guide/workflows/#concept-discovery","title":"Concept Discovery","text":"<p>Discover interpretable concepts by analyzing SAE neuron activations.</p> <p>When to use: - Finding what each SAE neuron represents - Collecting examples for manual curation - Validating concept quality - Understanding model features</p>"},{"location":"guide/workflows/#concept-manipulation","title":"Concept Manipulation","text":"<p>Control model behavior by manipulating discovered concepts.</p> <p>When to use: - Steering model outputs - Running intervention experiments - A/B testing concept effects - Real-time model control</p>"},{"location":"guide/workflows/#activation-control","title":"Activation Control","text":"<p>Directly manipulate activations using hooks for fine-grained control.</p> <p>When to use: - Custom intervention experiments - Fine-grained activation modification - Multi-layer interventions - Advanced control patterns</p>"},{"location":"guide/workflows/#workflow-dependencies","title":"Workflow Dependencies","text":"<p>Most workflows build on each other:</p> <pre><code>Saving Activations\n    \u2193\nTraining SAE Models\n    \u2193\nConcept Discovery\n    \u2193\nConcept Manipulation\n</code></pre> <p>Activation Control can be used independently or in combination with SAE-based workflows.</p>"},{"location":"guide/workflows/#quick-reference","title":"Quick Reference","text":"Workflow Input Output Time Saving Activations Model + Dataset Saved activations Minutes Training SAE Saved activations Trained SAE Hours Concept Discovery Trained SAE + Dataset Top texts per neuron Minutes Concept Manipulation Trained SAE + Concepts Modified outputs Seconds Activation Control Model + Hooks Modified outputs Seconds"},{"location":"guide/workflows/#getting-started","title":"Getting Started","text":"<p>If you're new to mi-crow, we recommend following workflows in order:</p> <ol> <li>Start with Saving Activations to understand data collection</li> <li>Move to Training SAE Models to learn feature discovery</li> <li>Try Concept Discovery to find interpretable features</li> <li>Use Concept Manipulation to control model behavior</li> </ol> <p>For advanced users, Activation Control provides direct hook-based control.</p>"},{"location":"guide/workflows/#integration-with-examples","title":"Integration with Examples","text":"<p>Each workflow references relevant example notebooks:</p> <ul> <li>Examples are in the <code>examples/</code> directory</li> <li>Workflows explain the concepts</li> <li>Examples provide runnable code</li> <li>See Examples Guide for the full list</li> </ul>"},{"location":"guide/workflows/#next-steps","title":"Next Steps","text":"<p>Choose a workflow that matches your goal:</p> <ul> <li>New to mi-crow? \u2192 Start with Saving Activations</li> <li>Want to train SAEs? \u2192 See Training SAE Models</li> <li>Ready to discover concepts? \u2192 Try Concept Discovery</li> <li>Need model control? \u2192 Use Concept Manipulation or Activation Control</li> </ul>"},{"location":"guide/workflows/activation-control/","title":"Activation Control with Hooks","text":"<p>This guide covers directly manipulating activations using hooks for fine-grained model control.</p>"},{"location":"guide/workflows/activation-control/#overview","title":"Overview","text":"<p>Activation control provides: - Direct manipulation of layer activations - Fine-grained control without SAEs - Custom intervention patterns - Multi-layer coordination</p>"},{"location":"guide/workflows/activation-control/#when-to-use-activation-control","title":"When to Use Activation Control","text":"<p>Use activation control when: - You need direct control over activations - SAE-based manipulation is insufficient - You want custom intervention patterns - You're experimenting with new control methods</p>"},{"location":"guide/workflows/activation-control/#basic-activation-control","title":"Basic Activation Control","text":""},{"location":"guide/workflows/activation-control/#using-detector-hooks-for-inspection","title":"Using Detector Hooks for Inspection","text":"<p>First, inspect activations to understand what you're working with:</p> <pre><code>from mi_crow.hooks import LayerActivationDetector\n\n# Create detector\ndetector = LayerActivationDetector(\"transformer.h.0.attn.c_attn\")\n\n# Register hook\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\n# Run inference\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Inspect activations\nactivations = detector.get_captured()\nprint(f\"Activations shape: {activations.shape}\")\nprint(f\"Mean: {activations.mean().item()}\")\nprint(f\"Std: {activations.std().item()}\")\n\n# Cleanup\nlm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/workflows/activation-control/#using-controller-hooks-for-modification","title":"Using Controller Hooks for Modification","text":"<p>Modify activations directly:</p> <pre><code>from mi_crow.hooks import FunctionController\n\n# Create controller that scales activations\ncontroller = FunctionController(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    function=lambda x: x * 1.5  # Scale by 1.5\n)\n\n# Register hook\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", controller)\n\n# Run inference with modification\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Cleanup\nlm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/workflows/activation-control/#custom-controller-implementation","title":"Custom Controller Implementation","text":"<p>Create custom controllers for specific needs:</p>"},{"location":"guide/workflows/activation-control/#scaling-controller","title":"Scaling Controller","text":"<pre><code>from mi_crow.hooks import Controller\nfrom mi_crow.hooks.hook import HookType\nimport torch\n\nclass ScalingController(Controller):\n    \"\"\"Scales activations by a factor.\"\"\"\n\n    def __init__(self, layer_signature: str | int, scale_factor: float):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.scale_factor = scale_factor\n\n    def modify_activations(self, module, inputs, output):\n        if output is None:\n            return output\n        return output * self.scale_factor\n\n# Use\ncontroller = ScalingController(\"transformer.h.0.attn.c_attn\", scale_factor=1.5)\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", controller)\n</code></pre>"},{"location":"guide/workflows/activation-control/#selective-neuron-controller","title":"Selective Neuron Controller","text":"<pre><code>class SelectiveController(Controller):\n    \"\"\"Modifies only specific neurons.\"\"\"\n\n    def __init__(self, layer_signature: str | int, neuron_indices: list[int], scale: float):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.neuron_indices = set(neuron_indices)\n        self.scale = scale\n\n    def modify_activations(self, module, inputs, output):\n        if output is None or not isinstance(output, torch.Tensor):\n            return output\n\n        modified = output.clone()\n        for idx in self.neuron_indices:\n            if idx &lt; modified.shape[-1]:\n                modified[..., idx] *= self.scale\n        return modified\n\n# Use\ncontroller = SelectiveController(\n    \"transformer.h.0.attn.c_attn\",\n    neuron_indices=[42, 100, 200],\n    scale=2.0\n)\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", controller)\n</code></pre>"},{"location":"guide/workflows/activation-control/#multi-layer-interventions","title":"Multi-Layer Interventions","text":"<p>Coordinate interventions across multiple layers:</p>"},{"location":"guide/workflows/activation-control/#sequential-modifications","title":"Sequential Modifications","text":"<pre><code># Modify early layer\nearly_controller = FunctionController(\"transformer.h.0.attn.c_attn\", lambda x: x * 1.2)\nhook1 = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", early_controller)\n\n# Modify late layer\nlate_controller = FunctionController(\"transformer.h.10.attn.c_attn\", lambda x: x * 0.8)\nhook2 = lm.layers.register_hook(\"transformer.h.10.attn.c_attn\", late_controller)\n\n# Both apply during forward pass\noutputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n\n# Cleanup\nlm.layers.unregister_hook(hook1)\nlm.layers.unregister_hook(hook2)\n</code></pre>"},{"location":"guide/workflows/activation-control/#cross-layer-communication","title":"Cross-Layer Communication","text":"<pre><code>class CoordinatedController(Controller):\n    \"\"\"Uses information from another layer.\"\"\"\n\n    def __init__(self, layer_signature, reference_detector):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.reference_detector = reference_detector\n\n    def modify_activations(self, module, inputs, output):\n        # Get activations from reference layer\n        ref_activations = self.reference_detector.get_captured()\n\n        if ref_activations is not None and output is not None:\n            # Scale based on reference layer\n            scale = 1.0 + 0.1 * ref_activations.mean().item()\n            return output * scale\n\n        return output\n\n# Setup\nref_detector = LayerActivationDetector(\"transformer.h.0.attn.c_attn\")\nlm.layers.register_hook(\"transformer.h.0.attn.c_attn\", ref_detector)\n\ncoordinated = CoordinatedController(\"transformer.h.5.attn.c_attn\", ref_detector)\nhook_id = lm.layers.register_hook(\"transformer.h.5.attn.c_attn\", coordinated)\n</code></pre>"},{"location":"guide/workflows/activation-control/#ab-testing-with-hooks","title":"A/B Testing with Hooks","text":"<p>Compare behavior with and without interventions:</p>"},{"location":"guide/workflows/activation-control/#baseline","title":"Baseline","text":"<pre><code># Get baseline\nbaseline_outputs, _ = lm.inference.execute_inference(\n    [\"Your prompt\"],\n    with_controllers=False  # Disable all controllers\n)\n</code></pre>"},{"location":"guide/workflows/activation-control/#with-intervention","title":"With Intervention","text":"<pre><code># Apply intervention\ncontroller = FunctionController(\"layer_0\", lambda x: x * 1.5)\nhook_id = lm.layers.register_hook(\"layer_0\", controller)\n\n# Get modified output\nintervention_outputs, _ = lm.inference.execute_inference(\n    [\"Your prompt\"],\n    with_controllers=True  # Enable controllers\n)\n\n# Compare\ndifference = intervention_outputs.logits - baseline_outputs.logits\n\n# Cleanup\nlm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/workflows/activation-control/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guide/workflows/activation-control/#conditional-control","title":"Conditional Control","text":"<pre><code>class ConditionalController(Controller):\n    \"\"\"Applies modification conditionally.\"\"\"\n\n    def __init__(self, layer_signature, condition_fn, modification_fn):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.condition_fn = condition_fn\n        self.modification_fn = modification_fn\n\n    def modify_activations(self, module, inputs, output):\n        if output is None:\n            return output\n\n        if self.condition_fn(output):\n            return self.modification_fn(output)\n\n        return output\n\n# Use: only modify if activation magnitude is high\ncontroller = ConditionalController(\n    \"layer_0\",\n    condition_fn=lambda x: x.abs().mean() &gt; 1.0,\n    modification_fn=lambda x: x * 1.5\n)\n</code></pre>"},{"location":"guide/workflows/activation-control/#pipeline-of-transformations","title":"Pipeline of Transformations","text":"<pre><code>class PipelineController(Controller):\n    \"\"\"Applies multiple transformations.\"\"\"\n\n    def __init__(self, layer_signature, transformations):\n        super().__init__(hook_type=HookType.FORWARD, layer_signature=layer_signature)\n        self.transformations = transformations\n\n    def modify_activations(self, module, inputs, output):\n        result = output\n        for transform in self.transformations:\n            result = transform(result)\n        return result\n\n# Use\npipeline = PipelineController(\n    \"layer_0\",\n    transformations=[\n        lambda x: x * 1.2,                    # Scale\n        lambda x: torch.clamp(x, -2, 2),      # Clamp\n        lambda x: (x - x.mean()) / (x.std() + 1e-8)  # Normalize\n    ]\n)\n</code></pre>"},{"location":"guide/workflows/activation-control/#best-practices","title":"Best Practices","text":"<ol> <li>Always cleanup: Unregister hooks when done</li> <li>Use context managers: For automatic cleanup</li> <li>Test incrementally: Start with simple modifications</li> <li>Monitor effects: Compare before/after</li> <li>Document interventions: Record what each does</li> </ol>"},{"location":"guide/workflows/activation-control/#common-patterns","title":"Common Patterns","text":""},{"location":"guide/workflows/activation-control/#context-manager-pattern","title":"Context Manager Pattern","text":"<pre><code>class HookContext:\n    \"\"\"Context manager for hook lifecycle.\"\"\"\n\n    def __init__(self, layers, hook, layer_signature):\n        self.layers = layers\n        self.hook = hook\n        self.layer_signature = layer_signature\n        self.hook_id = None\n\n    def __enter__(self):\n        self.hook_id = self.layers.register_hook(self.layer_signature, self.hook)\n        return self.hook\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.hook_id:\n            self.layers.unregister_hook(self.hook_id)\n\n# Usage\nwith HookContext(lm.layers, controller, \"layer_0\") as hook:\n    outputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\n# Hook automatically unregistered\n</code></pre>"},{"location":"guide/workflows/activation-control/#tryfinally-pattern","title":"Try/Finally Pattern","text":"<pre><code>hook_id = None\ntry:\n    controller = FunctionController(\"layer_0\", lambda x: x * 1.5)\n    hook_id = lm.layers.register_hook(\"layer_0\", controller)\n    outputs, encodings = lm.inference.execute_inference([\"Hello, world!\"])\nfinally:\n    if hook_id:\n        lm.layers.unregister_hook(hook_id)\n</code></pre>"},{"location":"guide/workflows/activation-control/#next-steps","title":"Next Steps","text":"<p>After learning activation control:</p> <ul> <li>Hooks: Controllers - Detailed controller guide</li> <li>Hooks: Advanced - Advanced hook patterns</li> <li>Concept Manipulation - SAE-based control</li> <li>Examples - See example notebooks</li> </ul>"},{"location":"guide/workflows/activation-control/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/08_inference_with_hooks.ipynb</code> - Complete hooks example</li> <li><code>examples/03_load_concepts.ipynb</code> - Concept manipulation with hooks</li> </ul>"},{"location":"guide/workflows/concept-discovery/","title":"Concept Discovery","text":"<p>This guide covers discovering interpretable concepts by analyzing SAE neuron activations and collecting top activating texts.</p>"},{"location":"guide/workflows/concept-discovery/#overview","title":"Overview","text":"<p>Concept discovery involves: 1. Attaching a trained SAE to the model 2. Enabling text tracking 3. Running inference on a dataset 4. Analyzing top activating texts for each neuron 5. Manually curating concept names</p>"},{"location":"guide/workflows/concept-discovery/#prerequisites","title":"Prerequisites","text":"<p>Before discovering concepts, you need: - A trained SAE (see Training SAE Models) - A dataset for inference - The SAE attached to the model</p>"},{"location":"guide/workflows/concept-discovery/#step-1-load-and-attach-sae","title":"Step 1: Load and Attach SAE","text":"<pre><code>from mi_crow.mechanistic.sae import TopKSae\nimport torch\n\n# Load trained SAE\nsae = TopKSae(n_latents=4096, n_inputs=768, k=32, device=\"cuda\")\nsae.load_state_dict(torch.load(\"sae_model.pt\"))\nsae.eval()\n\n# Attach to model\nlm.attach_sae(sae, layer_signature=\"transformer.h.0.attn.c_attn\")\n</code></pre> <p>The <code>attach_sae</code> method: - Registers the SAE as a hook on the specified layer - Enables SAE decoding during inference - Makes the SAE available for concept operations</p>"},{"location":"guide/workflows/concept-discovery/#step-2-enable-text-tracking","title":"Step 2: Enable Text Tracking","text":"<pre><code># Enable automatic text tracking\nsae.concepts.enable_text_tracking(top_k=10)\n</code></pre> <p>Parameters: - <code>top_k</code>: Number of top activating texts to track per neuron - Higher k = more examples but more memory</p> <p>Text tracking: - Automatically collects text snippets during inference - Tracks which texts activate each neuron most strongly - Accumulates data across batches</p>"},{"location":"guide/workflows/concept-discovery/#step-3-run-inference-on-dataset","title":"Step 3: Run Inference on Dataset","text":"<pre><code>from mi_crow.datasets import TextDataset\n\n# Prepare dataset\ntexts = [\n    \"The cat sat on the mat.\",\n    \"Dogs are loyal companions.\",\n    \"Science requires careful observation.\",\n    # ... more texts\n] * 100  # Repeat for more examples\n\ndataset = TextDataset(texts=texts)\n\n# Run inference - text tracking happens automatically\noutputs, encodings = lm.inference.execute_inference(dataset.texts)\n</code></pre> <p>During inference: - SAE decodes activations to sparse latents - Text tracker records which texts activate each neuron - Top-K texts are maintained for each neuron</p>"},{"location":"guide/workflows/concept-discovery/#step-4-get-top-texts","title":"Step 4: Get Top Texts","text":"<pre><code># Get top activating texts for all neurons\ntop_texts = sae.concepts.get_top_texts()\n\n# Access texts for a specific neuron\nneuron_42_texts = top_texts.get(42, [])\nprint(f\"Neuron 42 top texts:\")\nfor text, activation in neuron_42_texts:\n    print(f\"  Activation: {activation:.4f} - {text}\")\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#understanding-top-texts","title":"Understanding Top Texts","text":"<p>Each entry contains: - Text: The input text that activated the neuron - Activation: The activation strength (higher = stronger)</p> <p>Patterns in top texts reveal what the neuron detects: - Semantic concepts (e.g., \"family\", \"science\") - Syntactic patterns (e.g., \"question words\", \"past tense\") - Domain-specific (e.g., \"medical terms\", \"programming\")</p>"},{"location":"guide/workflows/concept-discovery/#step-5-export-for-manual-curation","title":"Step 5: Export for Manual Curation","text":"<pre><code># Export top texts to JSON\nimport json\n\nwith open(\"top_texts.json\", \"w\") as f:\n    json.dump(top_texts, f, indent=2)\n\n# Or export in a more readable format\nexport_data = []\nfor neuron_idx, texts in top_texts.items():\n    for text, activation in texts:\n        export_data.append({\n            \"neuron\": neuron_idx,\n            \"text\": text,\n            \"activation\": activation\n        })\n\nwith open(\"top_texts_flat.json\", \"w\") as f:\n    json.dump(export_data, f, indent=2)\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#step-6-manual-concept-curation","title":"Step 6: Manual Concept Curation","text":"<p>Create a CSV file mapping neurons to concept names:</p> <pre><code>neuron_idx,concept_name,confidence\n0,family relationships,0.9\n0,parent-child interactions,0.8\n1,nature and outdoors,0.9\n1,animals and wildlife,0.8\n2,scientific terminology,0.95\n2,academic language,0.85\n</code></pre> <p>Curation Tips: - Look for common themes in top texts - Assign multiple concepts if neuron is polysemous - Use confidence scores to indicate certainty - Review multiple top texts, not just the first</p>"},{"location":"guide/workflows/concept-discovery/#step-7-load-curated-concepts","title":"Step 7: Load Curated Concepts","text":"<pre><code>import pandas as pd\n\n# Load curated concepts\nconcepts_df = pd.read_csv(\"curated_concepts.csv\")\n\n# Create concept dictionary\nconcepts = {}\nfor _, row in concepts_df.iterrows():\n    neuron_idx = int(row['neuron_idx'])\n    concept_name = row['concept_name']\n    confidence = float(row['confidence'])\n\n    if neuron_idx not in concepts:\n        concepts[neuron_idx] = []\n\n    concepts[neuron_idx].append({\n        'name': concept_name,\n        'confidence': confidence\n    })\n\n# Save to SAE\nsae.concepts.load_concepts(concepts)\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#analyzing-concepts","title":"Analyzing Concepts","text":""},{"location":"guide/workflows/concept-discovery/#most-active-neurons","title":"Most Active Neurons","text":"<pre><code># Find neurons with highest average activation\nneuron_activity = {}\nfor neuron_idx, texts in top_texts.items():\n    if texts:\n        avg_activation = sum(act for _, act in texts) / len(texts)\n        neuron_activity[neuron_idx] = avg_activation\n\n# Sort by activity\nsorted_neurons = sorted(neuron_activity.items(), key=lambda x: x[1], reverse=True)\nprint(\"Most active neurons:\")\nfor neuron_idx, activity in sorted_neurons[:10]:\n    print(f\"  Neuron {neuron_idx}: {activity:.4f}\")\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#concept-distribution","title":"Concept Distribution","text":"<pre><code># Analyze concept coverage\nconcept_counts = {}\nfor neuron_idx, concepts_list in concepts.items():\n    for concept in concepts_list:\n        name = concept['name']\n        concept_counts[name] = concept_counts.get(name, 0) + 1\n\nprint(\"Concept distribution:\")\nfor concept, count in sorted(concept_counts.items(), key=lambda x: x[1], reverse=True):\n    print(f\"  {concept}: {count} neurons\")\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#validation","title":"Validation","text":""},{"location":"guide/workflows/concept-discovery/#check-concept-quality","title":"Check Concept Quality","text":"<pre><code># Verify concepts make sense\nfor neuron_idx in [0, 1, 2]:  # Check first few\n    texts = top_texts.get(neuron_idx, [])\n    concepts_list = concepts.get(neuron_idx, [])\n\n    print(f\"\\nNeuron {neuron_idx}:\")\n    print(f\"  Concepts: {[c['name'] for c in concepts_list]}\")\n    print(f\"  Top texts:\")\n    for text, act in texts[:3]:\n        print(f\"    {act:.4f}: {text[:50]}...\")\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#test-concept-consistency","title":"Test Concept Consistency","text":"<pre><code># Run inference on new texts and check if concepts activate\ntest_texts = [\n    \"My family loves to travel together.\",\n    \"The scientist conducted experiments.\",\n    \"Dogs are friendly animals.\"\n]\n\noutputs, encodings = lm.inference.execute_inference(test_texts)\n\n# Check which concepts activated\n# (Implementation depends on SAE concept API)\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"guide/workflows/concept-discovery/#filtering-by-activation-threshold","title":"Filtering by Activation Threshold","text":"<pre><code># Only consider texts above threshold\nthreshold = 0.5\nfiltered_texts = {}\n\nfor neuron_idx, texts in top_texts.items():\n    filtered = [(text, act) for text, act in texts if act &gt; threshold]\n    if filtered:\n        filtered_texts[neuron_idx] = filtered\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#clustering-similar-neurons","title":"Clustering Similar Neurons","text":"<pre><code># Group neurons with similar activation patterns\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Create activation matrix (neurons x samples)\n# Then cluster neurons\n# (Implementation depends on your data structure)\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#best-practices","title":"Best Practices","text":"<ol> <li>Use diverse datasets: Include various topics and styles</li> <li>Review multiple top texts: Don't judge by first example alone</li> <li>Check for polysemy: Neurons may detect multiple concepts</li> <li>Validate on held-out data: Test concepts on new texts</li> <li>Iterate: Refine concepts based on analysis</li> </ol>"},{"location":"guide/workflows/concept-discovery/#common-issues","title":"Common Issues","text":""},{"location":"guide/workflows/concept-discovery/#no-texts-collected","title":"No Texts Collected","text":"<pre><code># Solution: Ensure text tracking is enabled\nsae.concepts.enable_text_tracking(top_k=10)\n\n# And run inference\noutputs, encodings = lm.inference.execute_inference(dataset.texts)\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#too-manyfew-examples","title":"Too Many/Few Examples","text":"<pre><code># Adjust top_k\nsae.concepts.enable_text_tracking(top_k=20)  # More examples\n# or\nsae.concepts.enable_text_tracking(top_k=5)   # Fewer examples\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#unclear-concepts","title":"Unclear Concepts","text":"<pre><code># Solution: Use more diverse dataset\n# Or check if SAE training was successful\n# Or increase dataset size\n</code></pre>"},{"location":"guide/workflows/concept-discovery/#next-steps","title":"Next Steps","text":"<p>After discovering concepts:</p> <ul> <li>Concept Manipulation - Use concepts to control model</li> <li>Training SAE Models - Improve SAE if concepts unclear</li> <li>Examples - See example notebooks</li> </ul>"},{"location":"guide/workflows/concept-discovery/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/02_attach_sae_and_save_texts.ipynb</code> - Complete concept discovery example</li> <li><code>experiments/verify_sae_training/04_name_sae_concepts.ipynb</code> - Concept naming</li> </ul>"},{"location":"guide/workflows/concept-manipulation/","title":"Concept Manipulation","text":"<p>This guide covers controlling model behavior by manipulating discovered SAE concepts.</p>"},{"location":"guide/workflows/concept-manipulation/#overview","title":"Overview","text":"<p>Concept manipulation allows you to: - Amplify or suppress specific concepts - Compare model behavior with/without interventions - Steer model outputs toward desired behaviors - Run controlled intervention experiments</p>"},{"location":"guide/workflows/concept-manipulation/#prerequisites","title":"Prerequisites","text":"<p>Before manipulating concepts, you need: - A trained SAE attached to the model - Discovered concepts (see Concept Discovery) - The SAE registered on the target layer</p>"},{"location":"guide/workflows/concept-manipulation/#basic-concept-manipulation","title":"Basic Concept Manipulation","text":""},{"location":"guide/workflows/concept-manipulation/#amplify-a-concept","title":"Amplify a Concept","text":"<pre><code># Amplify neuron 42 (which represents a concept)\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)\n\n# Run inference with amplified concept\noutputs, encodings = lm.inference.execute_inference([\"Your prompt here\"])\n</code></pre> <p>Scale values: - <code>scale &gt; 1.0</code>: Amplify (increase concept strength) - <code>scale &lt; 1.0</code>: Suppress (decrease concept strength) - <code>scale = 0.0</code>: Completely remove concept</p>"},{"location":"guide/workflows/concept-manipulation/#suppress-a-concept","title":"Suppress a Concept","text":"<pre><code># Suppress neuron 42\nsae.concepts.manipulate_concept(neuron_idx=42, scale=0.5)\n\n# Run inference\noutputs, encodings = lm.inference.execute_inference([\"Your prompt here\"])\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#reset-manipulation","title":"Reset Manipulation","text":"<pre><code># Reset to original (scale = 1.0)\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.0)\n\n# Or reset all manipulations\nsae.concepts.reset_manipulations()\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#ab-testing-interventions","title":"A/B Testing Interventions","text":"<p>Compare model behavior with and without concept manipulation:</p>"},{"location":"guide/workflows/concept-manipulation/#baseline-no-manipulation","title":"Baseline (No Manipulation)","text":"<pre><code># Get baseline output\nbaseline_outputs, _ = lm.inference.execute_inference(\n    [\"Your prompt here\"],\n    with_controllers=False  # Disable SAE manipulation\n)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#with-manipulation","title":"With Manipulation","text":"<pre><code># Apply concept manipulation\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)\n\n# Get manipulated output\nmanipulated_outputs, _ = lm.inference.execute_inference(\n    [\"Your prompt here\"],\n    with_controllers=True  # Enable SAE manipulation\n)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#compare-results","title":"Compare Results","text":"<pre><code># Compare logits\ndifference = manipulated_outputs.logits - baseline_outputs.logits\n\n# Compare predictions\nbaseline_pred = baseline_outputs.logits.argmax(dim=-1)\nmanipulated_pred = manipulated_outputs.logits.argmax(dim=-1)\n\nprint(f\"Baseline prediction: {baseline_pred}\")\nprint(f\"Manipulated prediction: {manipulated_pred}\")\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#multiple-concept-manipulation","title":"Multiple Concept Manipulation","text":""},{"location":"guide/workflows/concept-manipulation/#manipulate-multiple-neurons","title":"Manipulate Multiple Neurons","text":"<pre><code># Amplify multiple concepts\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)  # Concept A\nsae.concepts.manipulate_concept(neuron_idx=100, scale=2.0)  # Concept B\nsae.concepts.manipulate_concept(neuron_idx=200, scale=0.5)  # Suppress Concept C\n\n# All manipulations apply simultaneously\noutputs, encodings = lm.inference.execute_inference([\"Your prompt here\"])\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#batch-manipulation","title":"Batch Manipulation","text":"<pre><code># Manipulate multiple concepts at once\nmanipulations = {\n    42: 1.5,   # Amplify concept A\n    100: 2.0,  # Amplify concept B\n    200: 0.5   # Suppress concept C\n}\n\nfor neuron_idx, scale in manipulations.items():\n    sae.concepts.manipulate_concept(neuron_idx=neuron_idx, scale=scale)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#real-time-control","title":"Real-Time Control","text":""},{"location":"guide/workflows/concept-manipulation/#dynamic-manipulation","title":"Dynamic Manipulation","text":"<pre><code># Change manipulation during generation\ndef generate_with_control(prompt, concept_idx, scale):\n    # Set manipulation\n    sae.concepts.manipulate_concept(neuron_idx=concept_idx, scale=scale)\n\n    # Generate\n    outputs, encodings = lm.inference.execute_inference([prompt])\n\n    return outputs\n\n# Use different scales\noutput1 = generate_with_control(\"Tell me about\", neuron_idx=42, scale=1.0)\noutput2 = generate_with_control(\"Tell me about\", neuron_idx=42, scale=1.5)\noutput3 = generate_with_control(\"Tell me about\", neuron_idx=42, scale=2.0)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#conditional-manipulation","title":"Conditional Manipulation","text":"<pre><code># Manipulate based on input\ndef conditional_manipulate(prompt):\n    if \"science\" in prompt.lower():\n        # Amplify scientific concepts\n        sae.concepts.manipulate_concept(neuron_idx=200, scale=1.5)\n    elif \"family\" in prompt.lower():\n        # Amplify family concepts\n        sae.concepts.manipulate_concept(neuron_idx=42, scale=1.5)\n\n    return lm.inference.execute_inference([prompt])\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#concept-configurations","title":"Concept Configurations","text":"<p>Save and load concept manipulation configurations:</p>"},{"location":"guide/workflows/concept-manipulation/#save-configuration","title":"Save Configuration","text":"<pre><code># Get current manipulations\nconfig = sae.concepts.get_manipulation_config()\n\n# Save to file\nimport json\nwith open(\"concept_config.json\", \"w\") as f:\n    json.dump(config, f, indent=2)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#load-configuration","title":"Load Configuration","text":"<pre><code># Load from file\nwith open(\"concept_config.json\", \"r\") as f:\n    config = json.load(f)\n\n# Apply configuration\nfor neuron_idx, scale in config.items():\n    sae.concepts.manipulate_concept(neuron_idx=int(neuron_idx), scale=scale)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#use-cases","title":"Use Cases","text":""},{"location":"guide/workflows/concept-manipulation/#steering-generation","title":"Steering Generation","text":"<pre><code># Steer model toward specific topics\nsae.concepts.manipulate_concept(neuron_idx=42, scale=2.0)  # Science concept\noutputs, encodings = lm.inference.execute_inference([\"Write about\"])\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#reducing-bias","title":"Reducing Bias","text":"<pre><code># Suppress potentially biased concepts\nbiased_concept_neurons = [100, 150, 200]  # Identified through analysis\n\nfor neuron_idx in biased_concept_neurons:\n    sae.concepts.manipulate_concept(neuron_idx=neuron_idx, scale=0.3)\n\noutputs, encodings = lm.inference.execute_inference([\"Your prompt\"])\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#enhancing-specific-features","title":"Enhancing Specific Features","text":"<pre><code># Enhance desired features\ndesired_features = {\n    42: 1.5,   # Clarity\n    100: 1.3,  # Accuracy\n    200: 1.2   # Helpfulness\n}\n\nfor neuron_idx, scale in desired_features.items():\n    sae.concepts.manipulate_concept(neuron_idx=neuron_idx, scale=scale)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guide/workflows/concept-manipulation/#gradual-manipulation","title":"Gradual Manipulation","text":"<pre><code># Gradually increase concept strength\nfor scale in [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]:\n    sae.concepts.manipulate_concept(neuron_idx=42, scale=scale)\n    outputs, encodings = lm.inference.execute_inference([\"Your prompt\"])\n    print(f\"Scale {scale}: {outputs.logits[0, 0, :5]}\")  # First 5 logits\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#concept-interaction-studies","title":"Concept Interaction Studies","text":"<pre><code># Study interactions between concepts\nconcept_a = 42\nconcept_b = 100\n\n# Individual effects\nsae.concepts.manipulate_concept(neuron_idx=concept_a, scale=1.5)\noutput_a, _ = lm.inference.execute_inference([\"Prompt\"])\n\nsae.concepts.reset_manipulations()\nsae.concepts.manipulate_concept(neuron_idx=concept_b, scale=1.5)\noutput_b, _ = lm.inference.execute_inference([\"Prompt\"])\n\n# Combined effect\nsae.concepts.reset_manipulations()\nsae.concepts.manipulate_concept(neuron_idx=concept_a, scale=1.5)\nsae.concepts.manipulate_concept(neuron_idx=concept_b, scale=1.5)\noutput_combined, _ = lm.inference.execute_inference([\"Prompt\"])\n\n# Compare\nprint(\"Individual A:\", output_a.logits)\nprint(\"Individual B:\", output_b.logits)\nprint(\"Combined:\", output_combined.logits)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#best-practices","title":"Best Practices","text":"<ol> <li>Start small: Use moderate scales (1.2-1.5) initially</li> <li>Test systematically: Compare baseline vs manipulated</li> <li>Document effects: Record what each manipulation does</li> <li>Reset between experiments: Use <code>reset_manipulations()</code></li> <li>Validate concepts: Ensure concepts are well-understood</li> </ol>"},{"location":"guide/workflows/concept-manipulation/#common-issues","title":"Common Issues","text":""},{"location":"guide/workflows/concept-manipulation/#no-effect","title":"No Effect","text":"<pre><code># Check SAE is attached\nassert sae in lm.layers.get_hooks()\n\n# Check manipulation is applied\nconfig = sae.concepts.get_manipulation_config()\nprint(f\"Current manipulations: {config}\")\n\n# Ensure with_controllers=True\noutputs, encodings = lm.inference.execute_inference([\"Prompt\"], with_controllers=True)\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#too-strong-effect","title":"Too Strong Effect","text":"<pre><code># Reduce scale\nsae.concepts.manipulate_concept(neuron_idx=42, scale=1.2)  # Instead of 2.0\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#unexpected-behavior","title":"Unexpected Behavior","text":"<pre><code># Verify concept is correct\ntop_texts = sae.concepts.get_top_texts()\nprint(f\"Neuron 42 top texts: {top_texts.get(42, [])[:5]}\")\n</code></pre>"},{"location":"guide/workflows/concept-manipulation/#next-steps","title":"Next Steps","text":"<p>After learning concept manipulation:</p> <ul> <li>Activation Control - Direct activation manipulation</li> <li>Hooks: Controllers - Custom controller hooks</li> <li>Examples - See example notebooks</li> </ul>"},{"location":"guide/workflows/concept-manipulation/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/03_load_concepts.ipynb</code> - Complete concept manipulation example</li> <li><code>experiments/verify_sae_training/05_show_concepts.ipynb</code> - Concept visualization</li> </ul>"},{"location":"guide/workflows/saving-activations/","title":"Saving Activations","text":"<p>This workflow guide covers collecting and saving activations from model layers for analysis and SAE training.</p>"},{"location":"guide/workflows/saving-activations/#when-and-why-to-save-activations","title":"When and Why to Save Activations","text":"<p>Activations are the internal representations that models use to process information. Saving them enables:</p> <ul> <li>SAE Training: Train sparse autoencoders to discover interpretable features</li> <li>Analysis: Understand what models learn at different layers</li> <li>Debugging: Inspect model internals during inference</li> <li>Research: Build datasets for interpretability studies</li> </ul>"},{"location":"guide/workflows/saving-activations/#basic-workflow","title":"Basic Workflow","text":""},{"location":"guide/workflows/saving-activations/#step-1-load-model-and-create-store","title":"Step 1: Load Model and Create Store","text":"<pre><code>from mi_crow.language_model import LanguageModel\nfrom mi_crow.store import LocalStore\n\nstore = LocalStore(base_path=\"./store\")\nlm = LanguageModel.from_huggingface(\"gpt2\", store=store)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#step-2-prepare-dataset","title":"Step 2: Prepare Dataset","text":"<pre><code>from mi_crow.datasets import TextDataset\n\n# Simple text dataset\ntexts = [\"The cat sat on the mat.\"] * 100\ndataset = TextDataset(texts=texts)\n\n# Or use HuggingFace dataset\nfrom mi_crow.datasets import HuggingFaceDataset\ndataset = HuggingFaceDataset(\n    name=\"wikitext\",\n    split=\"train\",\n    text_field=\"text\"\n)\n\n# For large datasets, you can sample a subset\ndataset = TextDataset.from_huggingface(\n    \"roneneldan/TinyStories\",\n    split=\"train\",\n    store=store,\n    text_field=\"text\"\n)\n\n# Randomly sample 1000 items (useful for testing or smaller experiments)\nsampled_dataset = dataset.random_sample(1000, seed=42)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#step-3-find-layer-name","title":"Step 3: Find Layer Name","text":"<pre><code># List available layers\nlayer_names = lm.layers.list_layers()\nprint(layer_names)\n\n# Example output:\n# ['transformer.wte', 'transformer.h.0.attn.c_attn', ...]\n</code></pre>"},{"location":"guide/workflows/saving-activations/#step-4-save-activations","title":"Step 4: Save Activations","text":"<pre><code># Save activations from a specific layer\nrun_id = lm.activations.save(\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    dataset=dataset,\n    sample_limit=1000,  # Number of samples to process\n    batch_size=4,        # Batch size for processing\n    shard_size=64        # Activations per shard file\n)\n\nprint(f\"Saved activations with run_id: {run_id}\")\n</code></pre> <p>The <code>save</code> method: - Processes the dataset in batches - Captures activations using detector hooks - Saves to the store in organized shards - Returns a run_id for later reference</p>"},{"location":"guide/workflows/saving-activations/#layer-selection-strategies","title":"Layer Selection Strategies","text":""},{"location":"guide/workflows/saving-activations/#choosing-the-right-layer","title":"Choosing the Right Layer","text":"<p>Different layers capture different information:</p> <ul> <li>Early layers: Low-level features (token patterns, syntax)</li> <li>Middle layers: Semantic combinations</li> <li>Late layers: High-level concepts (task-specific)</li> </ul>"},{"location":"guide/workflows/saving-activations/#common-layer-types","title":"Common Layer Types","text":"<pre><code># Attention layers (common choice)\nlayer = \"transformer.h.0.attn.c_attn\"\n\n# MLP layers\nlayer = \"transformer.h.0.mlp.c_fc\"\n\n# Residual stream (post-attention)\nlayer = \"transformer.h.0\"  # If available\n\n# Embedding layer\nlayer = \"transformer.wte\"\n</code></pre>"},{"location":"guide/workflows/saving-activations/#finding-layer-names","title":"Finding Layer Names","text":"<pre><code># List all layers\nall_layers = lm.layers.list_layers()\n\n# Filter by pattern\nattention_layers = [l for l in all_layers if \"attn\" in l]\nprint(f\"Found {len(attention_layers)} attention layers\")\n</code></pre>"},{"location":"guide/workflows/saving-activations/#batch-processing","title":"Batch Processing","text":""},{"location":"guide/workflows/saving-activations/#configuring-batch-size","title":"Configuring Batch Size","text":"<pre><code># Small batch size (lower memory, slower)\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=2,  # Small batches\n    sample_limit=100\n)\n\n# Large batch size (higher memory, faster)\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=32,  # Larger batches\n    sample_limit=1000\n)\n</code></pre> <p>Considerations: - GPU memory limits batch size - Larger batches = faster processing - Start small and increase if memory allows</p>"},{"location":"guide/workflows/saving-activations/#processing-large-datasets","title":"Processing Large Datasets","text":""},{"location":"guide/workflows/saving-activations/#option-1-random-sampling","title":"Option 1: Random Sampling","text":"<p>For large datasets, use <code>random_sample()</code> to create a manageable subset:</p> <pre><code># Load full dataset\ndataset = TextDataset.from_huggingface(\n    \"large-dataset\",\n    split=\"train\",\n    store=store\n)\n\n# Sample a subset for activation saving\nsampled_dataset = dataset.random_sample(10000, seed=42)\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=sampled_dataset,\n    sample_limit=10000,\n    batch_size=16\n)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#option-2-process-in-chunks","title":"Option 2: Process in Chunks","text":"<p>Alternatively, process the dataset in chunks:</p> <pre><code># Process in chunks\nchunk_size = 1000\ntotal_samples = 10000\n\nfor i in range(0, total_samples, chunk_size):\n    chunk_dataset = TextDataset(texts=texts[i:i+chunk_size])\n    run_id = lm.activations.save(\n        layer_signature=\"layer_0\",\n        dataset=chunk_dataset,\n        sample_limit=chunk_size,\n        batch_size=16\n    )\n    print(f\"Processed chunk {i//chunk_size + 1}\")\n</code></pre>"},{"location":"guide/workflows/saving-activations/#attention-mask-handling","title":"Attention Mask Handling","text":"<p>When saving activations, attention masks ensure only valid tokens are processed:</p> <pre><code># Activations are automatically masked\n# Only tokens that should be attended to are saved\n\n# The save method handles:\n# - Padding tokens (excluded)\n# - Special tokens (configurable)\n# - Sequence boundaries\n</code></pre>"},{"location":"guide/workflows/saving-activations/#special-token-handling","title":"Special Token Handling","text":"<pre><code># By default, special tokens are included\n# You can configure this if needed\n\n# Check tokenizer special tokens\nprint(lm.tokenizer.special_tokens_map)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#storage-organization","title":"Storage Organization","text":"<p>Saved activations are organized in the store:</p> <pre><code>store/\n\u2514\u2500\u2500 activations/\n    \u2514\u2500\u2500 &lt;run_id&gt;/\n        \u251c\u2500\u2500 batch_0/\n        \u2502   \u2514\u2500\u2500 &lt;layer_name&gt;/\n        \u2502       \u2514\u2500\u2500 activations.safetensors\n        \u251c\u2500\u2500 batch_1/\n        \u2502   \u2514\u2500\u2500 &lt;layer_name&gt;/\n        \u2502       \u2514\u2500\u2500 activations.safetensors\n        \u2514\u2500\u2500 meta.json\n</code></pre>"},{"location":"guide/workflows/saving-activations/#metadata","title":"Metadata","text":"<p>Each run includes metadata in <code>meta.json</code>:</p> <pre><code># Access metadata\nimport json\nwith open(f\"store/activations/{run_id}/meta.json\") as f:\n    metadata = json.load(f)\n\nprint(metadata)\n# Contains: layer_name, sample_count, batch_info, etc.\n</code></pre>"},{"location":"guide/workflows/saving-activations/#shard-size","title":"Shard Size","text":"<p>Control how activations are split into files:</p> <pre><code># Small shards (more files, easier to load)\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    shard_size=32  # 32 samples per file\n)\n\n# Large shards (fewer files, faster loading)\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    shard_size=256  # 256 samples per file\n)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guide/workflows/saving-activations/#saving-from-multiple-layers","title":"Saving from Multiple Layers","text":"<pre><code># Save from multiple layers sequentially\nlayers = [\"transformer.h.0.attn.c_attn\", \"transformer.h.5.attn.c_attn\"]\n\nrun_ids = {}\nfor layer in layers:\n    run_id = lm.activations.save(\n        layer_signature=layer,\n        dataset=dataset,\n        sample_limit=1000\n    )\n    run_ids[layer] = run_id\n</code></pre>"},{"location":"guide/workflows/saving-activations/#custom-activation-saving","title":"Custom Activation Saving","text":"<pre><code>from mi_crow.hooks import LayerActivationDetector\n\n# Manual saving with custom detector\ndetector = LayerActivationDetector(\"transformer.h.0.attn.c_attn\")\nhook_id = lm.layers.register_hook(\"transformer.h.0.attn.c_attn\", detector)\n\n# Process dataset\nfor batch in dataset:\n    outputs, encodings = lm.inference.execute_inference(batch)\n    activations = detector.get_captured()\n    # Save manually\n    detector.clear_captured()\n</code></pre>"},{"location":"guide/workflows/saving-activations/#verification","title":"Verification","text":"<p>After saving, verify the activations:</p> <pre><code># Check run exists\nfrom pathlib import Path\nrun_path = Path(f\"store/activations/{run_id}\")\nassert run_path.exists()\n\n# Check metadata\nimport json\nwith open(run_path / \"meta.json\") as f:\n    meta = json.load(f)\n    print(f\"Samples: {meta['sample_count']}\")\n    print(f\"Batches: {meta['batch_count']}\")\n</code></pre>"},{"location":"guide/workflows/saving-activations/#common-issues","title":"Common Issues","text":""},{"location":"guide/workflows/saving-activations/#out-of-memory","title":"Out of Memory","text":"<pre><code># Solution: Reduce batch size\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=1,  # Minimal batch size\n    sample_limit=100\n)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#layer-not-found","title":"Layer Not Found","text":"<pre><code># Solution: List available layers first\nlayers = lm.layers.list_layers()\nprint(\"Available layers:\", layers)\n\n# Use exact layer name from list\n</code></pre>"},{"location":"guide/workflows/saving-activations/#slow-processing","title":"Slow Processing","text":"<pre><code># Solution: Increase batch size (if memory allows)\nrun_id = lm.activations.save(\n    layer_signature=\"layer_0\",\n    dataset=dataset,\n    batch_size=32,  # Larger batches\n    sample_limit=1000\n)\n</code></pre>"},{"location":"guide/workflows/saving-activations/#next-steps","title":"Next Steps","text":"<p>After saving activations:</p> <ul> <li>Training SAE Models - Train SAEs on saved activations</li> <li>Hooks: Detectors - Learn about detector hooks</li> <li>Examples - See example notebooks</li> </ul>"},{"location":"guide/workflows/saving-activations/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/04_save_inputs_and_outputs.ipynb</code> - Saving inputs and outputs</li> <li><code>examples/06_save_activations_with_attention_masks.ipynb</code> - Attention mask handling</li> <li><code>examples/07_save_activations_and_attention_masks.ipynb</code> - Advanced saving</li> </ul>"},{"location":"guide/workflows/training-sae/","title":"Training SAE Models","text":"<p>This guide covers training sparse autoencoders (SAEs) on saved activations to discover interpretable features.</p>"},{"location":"guide/workflows/training-sae/#overview","title":"Overview","text":"<p>Training an SAE involves: 1. Loading saved activations 2. Creating an SAE model 3. Configuring training parameters 4. Training the SAE 5. Saving the trained model</p>"},{"location":"guide/workflows/training-sae/#prerequisites","title":"Prerequisites","text":"<p>Before training, you need: - Saved activations (see Saving Activations) - A run_id from the activation saving step - Knowledge of the activation dimensions</p>"},{"location":"guide/workflows/training-sae/#step-1-load-saved-activations","title":"Step 1: Load Saved Activations","text":"<pre><code>from mi_crow.store import LocalStore\n\nstore = LocalStore(base_path=\"./store\")\n\n# Use the run_id from saving activations\nrun_id = \"your-run-id-here\"\n\n# Verify activations exist\nfrom pathlib import Path\nrun_path = Path(f\"store/activations/{run_id}\")\nassert run_path.exists(), f\"Run {run_id} not found\"\n</code></pre>"},{"location":"guide/workflows/training-sae/#step-2-determine-activation-dimensions","title":"Step 2: Determine Activation Dimensions","text":"<p>You need to know the activation size for your layer:</p> <pre><code># Check metadata\nimport json\nwith open(f\"store/activations/{run_id}/meta.json\") as f:\n    metadata = json.load(f)\n\n# Activation size is typically in metadata\n# Or inspect a saved activation file\n</code></pre> <p>Common activation sizes: - GPT-2 small: 768 - GPT-2 medium: 1024 - GPT-2 large: 1280 - BERT base: 768</p>"},{"location":"guide/workflows/training-sae/#step-3-create-sae-model","title":"Step 3: Create SAE Model","text":"<pre><code>from mi_crow.mechanistic.sae import TopKSae\n\n# Create SAE\nsae = TopKSae(\n    n_latents=4096,  # Number of SAE neurons (overcomplete)\n    n_inputs=768,    # Must match activation size\n    k=32,            # Top-K sparsity (only 32 neurons active)\n    device=\"cuda\"    # or \"cpu\" or \"mps\"\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#sae-architecture-choices","title":"SAE Architecture Choices","text":"<p>Overcompleteness Ratio: <code>n_latents / n_inputs</code> - 2x: Fewer features, faster training - 4x: Balanced (common choice) - 8x+: More features, slower training</p> <p>Top-K Sparsity: <code>k</code> - Smaller k: More sparse, fewer active neurons - Larger k: Less sparse, more active neurons - Typical: 8-32</p>"},{"location":"guide/workflows/training-sae/#step-4-configure-training","title":"Step 4: Configure Training","text":"<pre><code>from mi_crow.mechanistic.sae.train import SaeTrainingConfig\n\nconfig = SaeTrainingConfig(\n    epochs=100,           # Number of training epochs\n    batch_size=256,       # Training batch size\n    lr=1e-3,             # Learning rate\n    l1_lambda=1e-4,       # L1 regularization strength\n    use_wandb=False,      # Enable Weights &amp; Biases logging\n    wandb_project=\"sae\"   # W&amp;B project name\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#hyperparameter-selection","title":"Hyperparameter Selection","text":"<p>Learning Rate: - Start with <code>1e-3</code> - Reduce if training is unstable - Increase if convergence is slow</p> <p>L1 Lambda: - Controls sparsity - Higher = more sparse - Typical: <code>1e-4</code> to <code>1e-3</code></p> <p>Batch Size: - Larger = faster training - Limited by GPU memory - Typical: 128-512</p> <p>Epochs: - Depends on dataset size - Monitor loss to determine convergence - Typical: 50-200</p>"},{"location":"guide/workflows/training-sae/#step-5-train-the-sae","title":"Step 5: Train the SAE","text":"<pre><code>from mi_crow.mechanistic.sae.train import SaeTrainer\n\n# Create trainer\ntrainer = SaeTrainer(sae)\n\n# Train\nhistory = trainer.train(\n    store=store,\n    run_id=run_id,\n    layer_signature=\"transformer.h.0.attn.c_attn\",\n    config=config\n)\n\nprint(\"Training complete!\")\n</code></pre>"},{"location":"guide/workflows/training-sae/#training-output","title":"Training Output","text":"<p>The <code>history</code> object contains: - <code>loss</code>: Reconstruction loss over time - <code>r2</code>: R\u00b2 score (reconstruction quality) - <code>l0</code>: L0 norm (sparsity) - <code>dead_features</code>: Number of dead neurons</p>"},{"location":"guide/workflows/training-sae/#step-6-monitor-training","title":"Step 6: Monitor Training","text":""},{"location":"guide/workflows/training-sae/#check-training-progress","title":"Check Training Progress","text":"<pre><code># Access training history\nprint(f\"Final loss: {history['loss'][-1]}\")\nprint(f\"Final R\u00b2: {history['r2'][-1]}\")\nprint(f\"Final L0: {history['l0'][-1]}\")\n</code></pre>"},{"location":"guide/workflows/training-sae/#visualize-training","title":"Visualize Training","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Plot loss\nplt.plot(history['loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n# Plot R\u00b2\nplt.plot(history['r2'])\nplt.xlabel('Epoch')\nplt.ylabel('R\u00b2')\nplt.title('Reconstruction Quality')\nplt.show()\n</code></pre>"},{"location":"guide/workflows/training-sae/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<pre><code>config = SaeTrainingConfig(\n    epochs=100,\n    batch_size=256,\n    lr=1e-3,\n    use_wandb=True,        # Enable W&amp;B\n    wandb_project=\"sae\",   # Project name\n    wandb_run_name=\"gpt2-layer0\"  # Run name\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#step-7-save-trained-model","title":"Step 7: Save Trained Model","text":"<p>The trainer automatically saves the model, but you can also save manually:</p> <pre><code># Model is saved automatically during training\n# Check store/runs/&lt;training_run_id&gt;/\n\n# Or save manually\nimport torch\ntorch.save(sae.state_dict(), \"sae_model.pt\")\n</code></pre>"},{"location":"guide/workflows/training-sae/#loading-a-trained-sae","title":"Loading a Trained SAE","text":"<pre><code>from mi_crow.mechanistic.sae import TopKSae\n\n# Create SAE with same architecture\nsae = TopKSae(n_latents=4096, n_inputs=768, k=32, device=\"cuda\")\n\n# Load weights\nsae.load_state_dict(torch.load(\"sae_model.pt\"))\nsae.eval()  # Set to evaluation mode\n</code></pre>"},{"location":"guide/workflows/training-sae/#training-tips","title":"Training Tips","text":""},{"location":"guide/workflows/training-sae/#start-small","title":"Start Small","text":"<pre><code># Start with small SAE for testing\nsae = TopKSae(\n    n_latents=512,   # Small overcompleteness\n    n_inputs=768,\n    k=8,             # Small sparsity\n    device=\"cpu\"      # CPU for testing\n)\n\nconfig = SaeTrainingConfig(\n    epochs=10,        # Few epochs for testing\n    batch_size=64\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#monitor-dead-features","title":"Monitor Dead Features","text":"<p>Dead features (neurons that never activate) indicate: - Too much sparsity (increase k or reduce l1_lambda) - Learning rate too high - Not enough training data</p> <pre><code># Check dead features\ndead_count = history['dead_features'][-1]\ntotal_features = sae.n_latents\ndead_ratio = dead_count / total_features\n\nif dead_ratio &gt; 0.1:  # More than 10% dead\n    print(\"Warning: Many dead features!\")\n</code></pre>"},{"location":"guide/workflows/training-sae/#verify-learning","title":"Verify Learning","text":"<pre><code># Check that weights are learning (not uniform)\nweight_variance = sae.encoder.weight.var().item()\nprint(f\"Weight variance: {weight_variance}\")\n\n# Should be &gt; 0.01 for learned features\nif weight_variance &lt; 0.01:\n    print(\"Warning: Weights may not be learning!\")\n</code></pre>"},{"location":"guide/workflows/training-sae/#common-issues","title":"Common Issues","text":""},{"location":"guide/workflows/training-sae/#out-of-memory","title":"Out of Memory","text":"<pre><code># Solution: Reduce batch size\nconfig = SaeTrainingConfig(\n    epochs=100,\n    batch_size=64,  # Smaller batch\n    lr=1e-3\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#training-instability","title":"Training Instability","text":"<pre><code># Solution: Reduce learning rate\nconfig = SaeTrainingConfig(\n    epochs=100,\n    batch_size=256,\n    lr=1e-4,  # Lower learning rate\n    l1_lambda=1e-5  # Lower regularization\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#poor-reconstruction","title":"Poor Reconstruction","text":"<pre><code># Solution: Increase model capacity\nsae = TopKSae(\n    n_latents=8192,  # More neurons\n    n_inputs=768,\n    k=64,            # More active neurons\n    device=\"cuda\"\n)\n</code></pre>"},{"location":"guide/workflows/training-sae/#next-steps","title":"Next Steps","text":"<p>After training an SAE:</p> <ul> <li>Concept Discovery - Find what each neuron represents</li> <li>Concept Manipulation - Use SAE to control model</li> <li>Hooks: Advanced - SAE as detector and controller</li> </ul>"},{"location":"guide/workflows/training-sae/#related-examples","title":"Related Examples","text":"<ul> <li><code>examples/01_train_sae_model.ipynb</code> - Complete SAE training example</li> <li><code>experiments/verify_sae_training/</code> - Detailed training experiment</li> </ul>"}]}