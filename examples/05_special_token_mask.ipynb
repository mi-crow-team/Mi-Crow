{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 5: Special Token Mask Detection\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load a language model\n",
        "2. Create `ModelInputDetector` with special token mask detection enabled\n",
        "3. Use model's automatic special token detection OR provide custom special token IDs\n",
        "4. Run inference and capture special token masks\n",
        "5. Visualize and verify the mask correctness\n",
        "6. Save the mask to store\n",
        "\n",
        "The special token mask is a binary mask (1 for special tokens, 0 for regular tokens) that has the same shape as `input_ids`. This is useful for:\n",
        "- Filtering out special tokens during analysis\n",
        "- Understanding tokenization behavior\n",
        "- Creating attention masks that exclude special tokens\n",
        "- Analyzing model behavior on special vs regular tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "‚úÖ Imports completed\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "from amber.hooks import ModelInputDetector\n",
        "from amber.language_model.language_model import LanguageModel\n",
        "from amber.store.local_store import LocalStore\n",
        "\n",
        "print(\"‚úÖ Imports completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Special Token Mask Example\n",
            "üì± Using device: cpu\n",
            "üîß Model: speakleash/Bielik-1.5B-v3.0-Instruct\n",
            "üìù Number of test texts: 3\n",
            "\n",
            "‚úÖ Output directories created\n"
          ]
        }
      ],
      "source": [
        "MODEL_ID = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
        "STORE_DIR = Path(\"store\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "TEST_TEXTS = [\n",
        "    \"Hello world! This is a test.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is fascinating.\",\n",
        "]\n",
        "\n",
        "print(\"üöÄ Starting Special Token Mask Example\")\n",
        "print(f\"üì± Using device: {DEVICE}\")\n",
        "print(f\"üîß Model: {MODEL_ID}\")\n",
        "print(f\"üìù Number of test texts: {len(TEST_TEXTS)}\")\n",
        "print()\n",
        "\n",
        "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"‚úÖ Output directories created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading language model...\n",
            "‚úÖ Model loaded: speakleash_Bielik-1.5B-v3.0-Instruct\n",
            "üì± Device: cpu\n",
            "üìÅ Store location: store\n",
            "\n",
            "üîç Tokenizer special tokens:\n",
            "  pad_token_id: 2\n",
            "  eos_token_id: 4\n",
            "  bos_token_id: 1\n",
            "  unk_token_id: 0\n",
            "  all_special_ids: [1, 4, 0, 2, 3, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "print(\"üì• Loading language model...\")\n",
        "\n",
        "store = LocalStore(STORE_DIR)\n",
        "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
        "lm.model.to(DEVICE)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
        "print(f\"üì± Device: {DEVICE}\")\n",
        "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n",
        "print()\n",
        "\n",
        "tokenizer = lm.tokenizer\n",
        "print(\"üîç Tokenizer special tokens:\")\n",
        "special_token_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id', \n",
        "                       'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
        "special_tokens = {}\n",
        "for attr in special_token_attrs:\n",
        "    token_id = getattr(tokenizer, attr, None)\n",
        "    if token_id is not None:\n",
        "        special_tokens[attr] = token_id\n",
        "        print(f\"  {attr}: {token_id}\")\n",
        "\n",
        "if hasattr(tokenizer, 'all_special_ids'):\n",
        "    print(f\"  all_special_ids: {tokenizer.all_special_ids}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Auto-detect Special Tokens from Model\n",
        "\n",
        "The detector will automatically extract special token IDs from the model's tokenizer or config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating ModelInputDetector with auto-detection of special tokens...\n",
            "üìù Added 'model_inputs_with_mask' to layers registry\n",
            "‚úÖ Detector attached to model\n",
            "üÜî Detector ID: model_input_detector_with_mask\n",
            "üíæ Will save: input_ids, special_token_mask\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß Creating ModelInputDetector with auto-detection of special tokens...\")\n",
        "\n",
        "layer_signature = \"model_inputs_with_mask\"\n",
        "if layer_signature not in lm.layers.name_to_layer:\n",
        "    lm.layers.name_to_layer[layer_signature] = lm.model\n",
        "    print(f\"üìù Added '{layer_signature}' to layers registry\")\n",
        "\n",
        "input_detector = ModelInputDetector(\n",
        "    layer_signature=layer_signature,\n",
        "    hook_id=\"model_input_detector_with_mask\",\n",
        "    save_input_ids=True,\n",
        "    save_attention_mask=False,\n",
        "    save_special_token_mask=True,\n",
        "    special_token_ids=None,\n",
        ")\n",
        "\n",
        "hook_id = lm.layers.register_hook(layer_signature, input_detector)\n",
        "\n",
        "print(f\"‚úÖ Detector attached to model\")\n",
        "print(f\"üÜî Detector ID: {input_detector.id}\")\n",
        "print(f\"üíæ Will save: input_ids, special_token_mask\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running inference...\n",
            "üìù Processing 3 texts\n",
            "‚úÖ Inference completed\n",
            "üìä Encodings keys: ['input_ids', 'attention_mask']\n",
            "\n",
            "üí° Data captured in detector - ready to inspect\n"
          ]
        }
      ],
      "source": [
        "print(\"üöÄ Running inference...\")\n",
        "print(f\"üìù Processing {len(TEST_TEXTS)} texts\")\n",
        "\n",
        "input_detector.clear_captured()\n",
        "\n",
        "output, encodings = lm.forwards(\n",
        "    TEST_TEXTS,\n",
        "    tok_kwargs={\"max_length\": 128, \"padding\": True, \"truncation\": True, \"add_special_tokens\": True},\n",
        "    autocast=False,\n",
        ")\n",
        "\n",
        "input_detector.set_inputs_from_encodings(encodings, module=lm.model)\n",
        "\n",
        "print(\"‚úÖ Inference completed\")\n",
        "print(f\"üìä Encodings keys: {list(encodings.keys())}\")\n",
        "print()\n",
        "print(\"üí° Data captured in detector - ready to inspect\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Captured Data:\n",
            "  input_ids shape: torch.Size([3, 18])\n",
            "  special_token_mask shape: torch.Size([3, 18])\n",
            "  special_token_mask dtype: torch.bool\n",
            "\n",
            "‚úÖ Shapes match!\n",
            "\n",
            "üîç Special Token Mask Analysis:\n",
            "\n",
            "Text 1: Hello world! This is a test....\n",
            "  input_ids: [2, 2, 2, 2, 2, 2, 2, 2, 1, 10404, 397, 22299, 31964, 22382, 3707, 322, 6291, 31917]\n",
            "  mask:      [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "  Special tokens: 9/18 (50.0%)\n",
            "\n",
            "Text 2: The quick brown fox jumps over the lazy dog....\n",
            "  input_ids: [1, 2091, 9108, 23156, 31225, 31892, 2228, 31967, 590, 4742, 31896, 17419, 1226, 1347, 395, 303, 31908, 31917]\n",
            "  mask:      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "  Special tokens: 1/18 (5.6%)\n",
            "\n",
            "Text 3: Machine learning is fascinating....\n",
            "  input_ids: [2, 2, 2, 2, 2, 1, 739, 1437, 289, 568, 300, 4957, 3707, 1075, 31896, 5910, 19217, 31917]\n",
            "  mask:      [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "  Special tokens: 6/18 (33.3%)\n"
          ]
        }
      ],
      "source": [
        "input_ids = input_detector.get_captured_input_ids()\n",
        "special_token_mask = input_detector.get_captured_special_token_mask()\n",
        "\n",
        "print(\"üìä Captured Data:\")\n",
        "print(f\"  input_ids shape: {input_ids.shape}\")\n",
        "print(f\"  special_token_mask shape: {special_token_mask.shape}\")\n",
        "print(f\"  special_token_mask dtype: {special_token_mask.dtype}\")\n",
        "print()\n",
        "\n",
        "assert input_ids.shape == special_token_mask.shape, \"Shapes must match!\"\n",
        "print(\"‚úÖ Shapes match!\")\n",
        "print()\n",
        "\n",
        "print(\"üîç Special Token Mask Analysis:\")\n",
        "for i, text in enumerate(TEST_TEXTS):\n",
        "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
        "    print(f\"  input_ids: {input_ids[i].tolist()}\")\n",
        "    print(f\"  mask:      {special_token_mask[i].int().tolist()}\")\n",
        "    \n",
        "    num_special = special_token_mask[i].sum().item()\n",
        "    num_total = len(input_ids[i])\n",
        "    print(f\"  Special tokens: {num_special}/{num_total} ({100*num_special/num_total:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Visualizing Special Token Mask:\n",
            "\n",
            "Text 1: Hello world! This is a test.\n",
            "  Token IDs:     [2, 2, 2, 2, 2, 2, 2, 2, 1, 10404, 397, 22299, 31964, 22382, 3707, 322, 6291, 31917]\n",
            "  Special Mask:  ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë\n",
            "  Values:        1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "  Token breakdown:\n",
            "    [ 0] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 1] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 2] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 3] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 4] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 5] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 6] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 7] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 8] ID=   1 | <s>                  [SPECIAL]\n",
            "    [ 9] ID=10404 | Hel                  \n",
            "    [10] ID= 397 | lo                   \n",
            "    [11] ID=22299 | world                \n",
            "    [12] ID=31964 | !                    \n",
            "    [13] ID=22382 | This                 \n",
            "    [14] ID=3707 | is                   \n",
            "    [15] ID= 322 | a                    \n",
            "    [16] ID=6291 | test                 \n",
            "    [17] ID=31917 | .                    \n",
            "\n",
            "Text 2: The quick brown fox jumps over the lazy dog.\n",
            "  Token IDs:     [1, 2091, 9108, 23156, 31225, 31892, 2228, 31967, 590, 4742, 31896, 17419, 1226, 1347, 395, 303, 31908, 31917]\n",
            "  Special Mask:  ‚ñà ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë\n",
            "  Values:        1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "  Token breakdown:\n",
            "    [ 0] ID=   1 | <s>                  [SPECIAL]\n",
            "    [ 1] ID=2091 | The                  \n",
            "    [ 2] ID=9108 | qu                   \n",
            "    [ 3] ID=23156 | ick                  \n",
            "    [ 4] ID=31225 | brow                 \n",
            "    [ 5] ID=31892 | n                    \n",
            "    [ 6] ID=2228 | fo                   \n",
            "    [ 7] ID=31967 | x                    \n",
            "    [ 8] ID= 590 | ju                   \n",
            "    [ 9] ID=4742 | mp                   \n",
            "    [10] ID=31896 | s                    \n",
            "    [11] ID=17419 | over                 \n",
            "    [12] ID=1226 | the                  \n",
            "    [13] ID=1347 | la                   \n",
            "    [14] ID= 395 | zy                   \n",
            "    [15] ID= 303 | do                   \n",
            "    [16] ID=31908 | g                    \n",
            "    [17] ID=31917 | .                    \n",
            "\n",
            "Text 3: Machine learning is fascinating.\n",
            "  Token IDs:     [2, 2, 2, 2, 2, 1, 739, 1437, 289, 568, 300, 4957, 3707, 1075, 31896, 5910, 19217, 31917]\n",
            "  Special Mask:  ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë\n",
            "  Values:        1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "  Token breakdown:\n",
            "    [ 0] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 1] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 2] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 3] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 4] ID=   2 | </s>                 [SPECIAL]\n",
            "    [ 5] ID=   1 | <s>                  [SPECIAL]\n",
            "    [ 6] ID= 739 | Ma                   \n",
            "    [ 7] ID=1437 | chi                  \n",
            "    [ 8] ID= 289 | ne                   \n",
            "    [ 9] ID= 568 | le                   \n",
            "    [10] ID= 300 | ar                   \n",
            "    [11] ID=4957 | ning                 \n",
            "    [12] ID=3707 | is                   \n",
            "    [13] ID=1075 | fa                   \n",
            "    [14] ID=31896 | s                    \n",
            "    [15] ID=5910 | cina                 \n",
            "    [16] ID=19217 | ting                 \n",
            "    [17] ID=31917 | .                    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Visualize mask\n",
        "print(\"üìà Visualizing Special Token Mask:\")\n",
        "print()\n",
        "\n",
        "for i, text in enumerate(TEST_TEXTS):\n",
        "    print(f\"Text {i+1}: {text}\")\n",
        "    print(f\"  Token IDs:     {input_ids[i].tolist()}\")\n",
        "    print(f\"  Special Mask:  {' '.join(['‚ñà' if m else '‚ñë' for m in special_token_mask[i].tolist()])}\")\n",
        "    print(f\"  Values:        {' '.join(['1' if m else '0' for m in special_token_mask[i].tolist()])}\")\n",
        "    print()\n",
        "    \n",
        "    # Decode tokens to verify\n",
        "    token_ids_list = input_ids[i].tolist()\n",
        "    mask_list = special_token_mask[i].tolist()\n",
        "    \n",
        "    print(\"  Token breakdown:\")\n",
        "    for j, (token_id, is_special) in enumerate(zip(token_ids_list, mask_list)):\n",
        "        token_str = tokenizer.decode([token_id])\n",
        "        special_marker = \"[SPECIAL]\" if is_special else \"\"\n",
        "        print(f\"    [{j:2d}] ID={token_id:4d} | {token_str:20s} {special_marker}\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
