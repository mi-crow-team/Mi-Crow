{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8: Inference with Hooks\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load Bielik model\n",
    "2. Attach hooks (ModelInputDetector for attention masks)\n",
    "3. Verify hooks work correctly\n",
    "4. Run inference on texts using `infer_texts()`\n",
    "5. Run inference on dataset using `infer_dataset()`\n",
    "6. Verify metadata was saved correctly\n",
    "\n",
    "This shows the new inference API that separates basic inference from activation saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from amber.datasets import TextDataset\n",
    "from amber.language_model.language_model import LanguageModel\n",
    "from amber.hooks.implementations.model_input_detector import ModelInputDetector\n",
    "from amber.hooks import HookType\n",
    "from amber.store.local_store import LocalStore\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"‚úÖ Imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration:\n",
      "   Model: speakleash/Bielik-1.5B-v3.0-Instruct\n",
      "   Device: cpu\n",
      "   Batch size: 4\n",
      "   Max length: 128\n",
      "   Dataset: roneneldan/TinyStories\n",
      "   Data limit: 10 samples\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
    "STORE_DIR = Path(\"store\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 128\n",
    "DATA_LIMIT = 10\n",
    "\n",
    "HF_DATASET = \"roneneldan/TinyStories\"\n",
    "TEXT_FIELD = \"text\"\n",
    "DATA_SPLIT = \"train\"\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Model: {MODEL_ID}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Max length: {MAX_LENGTH}\")\n",
    "print(f\"   Dataset: {HF_DATASET}\")\n",
    "print(f\"   Data limit: {DATA_LIMIT} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading Bielik model...\n",
      "‚úÖ Model loaded: speakleash_Bielik-1.5B-v3.0-Instruct\n",
      "üì± Device: cpu\n",
      "üìÅ Store location: store\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Bielik model\n",
    "print(\"üì• Loading Bielik model...\")\n",
    "\n",
    "store = LocalStore(STORE_DIR)\n",
    "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
    "lm.model.to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
    "print(f\"üì± Device: {DEVICE}\")\n",
    "print(f\"üìÅ Store location: {lm.store.base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 5175.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset created: 10 samples\n",
      "üìù Sample text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create small dataset\n",
    "print(\"üìä Creating dataset...\")\n",
    "\n",
    "hf_dataset = load_dataset(HF_DATASET, split=DATA_SPLIT, streaming=False)\n",
    "if DATA_LIMIT > 0:\n",
    "    hf_dataset = hf_dataset.select(range(min(DATA_LIMIT, len(hf_dataset))))\n",
    "\n",
    "dataset = TextDataset(hf_dataset, store=store, text_field=TEXT_FIELD)\n",
    "\n",
    "print(f\"‚úÖ Dataset created: {len(dataset)} samples\")\n",
    "print(f\"üìù Sample text: {dataset[0][:100]}...\" if len(dataset[0]) > 100 else f\"üìù Sample text: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Attaching ModelInputDetector hook...\n",
      "\n",
      "   üìù Added 'attention_masks' to layers registry\n",
      "   ‚úÖ Attached to root model\n",
      "   üÜî Hook ID: attention_mask_detector\n",
      "\n",
      "‚úÖ Hook attached successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Attach ModelInputDetector hook for attention masks\n",
    "print(\"üîß Attaching ModelInputDetector hook...\")\n",
    "print()\n",
    "\n",
    "attention_mask_layer_sig = \"attention_masks\"\n",
    "root_model = lm.model\n",
    "\n",
    "if attention_mask_layer_sig not in lm.layers.name_to_layer:\n",
    "    lm.layers.name_to_layer[attention_mask_layer_sig] = root_model\n",
    "    print(f\"   üìù Added '{attention_mask_layer_sig}' to layers registry\")\n",
    "\n",
    "attention_mask_detector = ModelInputDetector(\n",
    "    layer_signature=attention_mask_layer_sig,\n",
    "    hook_id=\"attention_mask_detector\",\n",
    "    save_input_ids=False,\n",
    "    save_attention_mask=True,\n",
    ")\n",
    "attention_mask_hook_id = lm.layers.register_hook(\n",
    "    attention_mask_layer_sig, attention_mask_detector, HookType.PRE_FORWARD\n",
    ")\n",
    "print(f\"   ‚úÖ Attached to root model\")\n",
    "print(f\"   üÜî Hook ID: {attention_mask_hook_id}\")\n",
    "print()\n",
    "print(\"‚úÖ Hook attached successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying hook works with basic inference...\n",
      "\n",
      "‚úÖ Hook works! Captured attention mask:\n",
      "   Shape: torch.Size([2, 6])\n",
      "   Dtype: torch.bool\n",
      "   Sample values (first 5 tokens of first sample): [False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Verify hook works with basic inference\n",
    "print(\"üîç Verifying hook works with basic inference...\")\n",
    "print()\n",
    "\n",
    "test_texts = [\"Hello, world!\", \"This is a test.\"]\n",
    "\n",
    "attention_mask_detector.clear_captured()\n",
    "\n",
    "output, encodings = lm.forwards(\n",
    "    test_texts,\n",
    "    tok_kwargs={\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"padding\": True,\n",
    "        \"truncation\": True,\n",
    "        \"add_special_tokens\": True\n",
    "    },\n",
    "    autocast=False,\n",
    ")\n",
    "\n",
    "captured_mask = attention_mask_detector.get_captured_attention_mask()\n",
    "\n",
    "if captured_mask is not None:\n",
    "    print(f\"‚úÖ Hook works! Captured attention mask:\")\n",
    "    print(f\"   Shape: {captured_mask.shape}\")\n",
    "    print(f\"   Dtype: {captured_mask.dtype}\")\n",
    "    print(f\"   Sample values (first 5 tokens of first sample): {captured_mask[0, :5].tolist()}\")\n",
    "else:\n",
    "    print(\"‚ùå Hook did not capture attention mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Mode 1: Inference on texts using infer_texts()\n",
      "============================================================\n",
      "\n",
      "üìù Processing 6 texts...\n",
      "\n",
      "üìÅ Run name: inference_texts_20251209_220644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:06:44,866 [INFO] amber.language_model.inference: Saved batch 0 for run=inference_texts_20251209_220644\n",
      "2025-12-09 22:06:45,658 [INFO] amber.language_model.inference: Saved batch 1 for run=inference_texts_20251209_220644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Inference completed!\n",
      "   Number of batches: 2\n",
      "   Output type: <class 'list'>\n",
      "\n",
      "üì¶ Saved 2 batches to store\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Mode 1 - Inference on texts using infer_texts()\n",
    "print(\"üöÄ Mode 1: Inference on texts using infer_texts()\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "texts = [dataset[i] for i in range(min(6, len(dataset)))]\n",
    "print(f\"üìù Processing {len(texts)} texts...\")\n",
    "print()\n",
    "\n",
    "run_name_texts = f\"inference_texts_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"üìÅ Run name: {run_name_texts}\")\n",
    "print()\n",
    "\n",
    "attention_mask_detector.clear_captured()\n",
    "\n",
    "outputs, encodings = lm.inference.infer_texts(\n",
    "    texts,\n",
    "    run_name=run_name_texts,\n",
    "    batch_size=3,\n",
    "    tok_kwargs={\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"padding\": True,\n",
    "        \"truncation\": True,\n",
    "        \"add_special_tokens\": True\n",
    "    },\n",
    "    autocast=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f\"‚úÖ Inference completed!\")\n",
    "print(f\"   Number of batches: {len(outputs) if isinstance(outputs, list) else 1}\")\n",
    "print(f\"   Output type: {type(outputs)}\")\n",
    "print()\n",
    "\n",
    "batches = lm.store.list_run_batches(run_name_texts)\n",
    "print(f\"üì¶ Saved {len(batches)} batches to store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying saved metadata from infer_texts()...\n",
      "\n",
      "üìä Batch 0 metadata:\n",
      "   Layers with data: ['attention_masks']\n",
      "   ‚úÖ Attention mask found:\n",
      "      Shape: torch.Size([3, 128])\n",
      "      Dtype: torch.bool\n",
      "   ‚úÖ Run metadata found:\n",
      "      Model: LlamaForCausalLM\n",
      "      Batch size: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Verify saved metadata from infer_texts()\n",
    "print(\"üîç Verifying saved metadata from infer_texts()...\")\n",
    "print()\n",
    "\n",
    "if len(batches) > 0:\n",
    "    batch_idx = 0\n",
    "    retrieved_metadata, retrieved_tensors = lm.store.get_detector_metadata(run_name_texts, batch_idx)\n",
    "    \n",
    "    print(f\"üìä Batch {batch_idx} metadata:\")\n",
    "    print(f\"   Layers with data: {list(retrieved_tensors.keys())}\")\n",
    "    \n",
    "    if \"attention_masks\" in retrieved_tensors:\n",
    "        attention_mask = retrieved_tensors[\"attention_masks\"].get(\"attention_mask\")\n",
    "        if attention_mask is not None:\n",
    "            print(f\"   ‚úÖ Attention mask found:\")\n",
    "            print(f\"      Shape: {attention_mask.shape}\")\n",
    "            print(f\"      Dtype: {attention_mask.dtype}\")\n",
    "    \n",
    "    run_metadata = lm.store.get_run_metadata(run_name_texts)\n",
    "    if run_metadata:\n",
    "        print(f\"   ‚úÖ Run metadata found:\")\n",
    "        print(f\"      Model: {run_metadata.get('model', 'N/A')}\")\n",
    "        print(f\"      Batch size: {run_metadata.get('options', {}).get('batch_size', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ùå No batches found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:06:45,701 [INFO] amber.language_model.inference: Starting infer_dataset: run=inference_dataset_20251209_220645, batch_size=4, device=cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Mode 2: Inference on dataset using infer_dataset()\n",
      "============================================================\n",
      "\n",
      "üìÅ Run name: inference_dataset_20251209_220645\n",
      "üìä Dataset size: 10 samples\n",
      "üì¶ Batch size: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:06:46,720 [INFO] amber.language_model.inference: Saved batch 0 for run=inference_dataset_20251209_220645\n",
      "2025-12-09 22:06:47,764 [INFO] amber.language_model.inference: Saved batch 1 for run=inference_dataset_20251209_220645\n",
      "2025-12-09 22:06:48,357 [INFO] amber.language_model.inference: Saved batch 2 for run=inference_dataset_20251209_220645\n",
      "2025-12-09 22:06:48,358 [INFO] amber.language_model.inference: Completed infer_dataset: run=inference_dataset_20251209_220645, batches_saved=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Inference completed!\n",
      "üìÅ Run name: inference_dataset_20251209_220645\n",
      "\n",
      "üì¶ Saved 3 batches to store\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Mode 2 - Inference on dataset using infer_dataset()\n",
    "print(\"üöÄ Mode 2: Inference on dataset using infer_dataset()\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "run_name_dataset = f\"inference_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"üìÅ Run name: {run_name_dataset}\")\n",
    "print(f\"üìä Dataset size: {len(dataset)} samples\")\n",
    "print(f\"üì¶ Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "run_name = lm.inference.infer_dataset(\n",
    "    dataset,\n",
    "    run_name=run_name_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tok_kwargs={\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"padding\": True,\n",
    "        \"truncation\": True,\n",
    "        \"add_special_tokens\": True\n",
    "    },\n",
    "    autocast=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f\"‚úÖ Inference completed!\")\n",
    "print(f\"üìÅ Run name: {run_name}\")\n",
    "print()\n",
    "\n",
    "batches = lm.store.list_run_batches(run_name)\n",
    "print(f\"üì¶ Saved {len(batches)} batches to store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying saved metadata from infer_dataset()...\n",
      "\n",
      "üìä Batch 0 metadata:\n",
      "   Layers with data: ['attention_masks']\n",
      "   ‚úÖ Attention mask found:\n",
      "      Shape: torch.Size([4, 128])\n",
      "      Dtype: torch.bool\n",
      "   ‚úÖ Run metadata found:\n",
      "      Model: LlamaForCausalLM\n",
      "      Batch size: 4\n",
      "      Dataset length: 10\n",
      "\n",
      "üìä All batches summary:\n",
      "   Batch 0: attention_mask shape torch.Size([4, 128])\n",
      "   Batch 1: attention_mask shape torch.Size([4, 128])\n",
      "   Batch 2: attention_mask shape torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Verify saved metadata from infer_dataset()\n",
    "print(\"üîç Verifying saved metadata from infer_dataset()...\")\n",
    "print()\n",
    "\n",
    "if len(batches) > 0:\n",
    "    batch_idx = 0\n",
    "    retrieved_metadata, retrieved_tensors = lm.store.get_detector_metadata(run_name, batch_idx)\n",
    "    \n",
    "    print(f\"üìä Batch {batch_idx} metadata:\")\n",
    "    print(f\"   Layers with data: {list(retrieved_tensors.keys())}\")\n",
    "    \n",
    "    if \"attention_masks\" in retrieved_tensors:\n",
    "        attention_mask = retrieved_tensors[\"attention_masks\"].get(\"attention_mask\")\n",
    "        if attention_mask is not None:\n",
    "            print(f\"   ‚úÖ Attention mask found:\")\n",
    "            print(f\"      Shape: {attention_mask.shape}\")\n",
    "            print(f\"      Dtype: {attention_mask.dtype}\")\n",
    "    \n",
    "    run_metadata = lm.store.get_run_metadata(run_name)\n",
    "    if run_metadata:\n",
    "        print(f\"   ‚úÖ Run metadata found:\")\n",
    "        print(f\"      Model: {run_metadata.get('model', 'N/A')}\")\n",
    "        print(f\"      Batch size: {run_metadata.get('options', {}).get('batch_size', 'N/A')}\")\n",
    "        print(f\"      Dataset length: {run_metadata.get('dataset', {}).get('length', 'N/A')}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"üìä All batches summary:\")\n",
    "    for i in range(min(3, len(batches))):\n",
    "        meta, tensors = lm.store.get_detector_metadata(run_name, i)\n",
    "        mask_shape = tensors.get(\"attention_masks\", {}).get(\"attention_mask\", None)\n",
    "        if mask_shape is not None:\n",
    "            print(f\"   Batch {i}: attention_mask shape {mask_shape.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå No batches found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This example demonstrated:\n",
    "\n",
    "1. ‚úÖ **Loading Bielik model** - Successfully loaded from HuggingFace\n",
    "2. ‚úÖ **Attaching hooks** - ModelInputDetector for attention masks\n",
    "3. ‚úÖ **Verifying hooks work** - Confirmed hooks capture data during inference\n",
    "4. ‚úÖ **Mode 1: infer_texts()** - Inference on list of texts with metadata saving\n",
    "5. ‚úÖ **Mode 2: infer_dataset()** - Inference on whole dataset with batch processing\n",
    "6. ‚úÖ **Verification** - Confirmed all metadata saved correctly to disk\n",
    "\n",
    "**Key Benefits:**\n",
    "- `infer_texts()` - Simple inference on text lists with optional batching\n",
    "- `infer_dataset()` - Efficient batch processing of datasets\n",
    "- Both methods automatically save metadata when `run_name` is provided\n",
    "- Hooks work seamlessly with both inference modes\n",
    "- Metadata structure is consistent across both modes\n",
    "\n",
    "**Conclusion:** ‚úÖ The new inference API provides clean separation between basic inference and activation saving!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
