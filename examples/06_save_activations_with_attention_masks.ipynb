{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 6: Save Activations with Attention Masks\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load a language model (Bielik)\n",
        "2. Create a text dataset\n",
        "3. Save activations from a specific layer WITH attention masks\n",
        "4. Load and verify both activations and attention masks from the store\n",
        "5. Demonstrate how to match attention masks with activations per batch\n",
        "\n",
        "This example shows the new feature where attention masks are automatically saved alongside activations when using `save_activations_dataset()`, making it easy to identify which tokens are regular (non-padding) tokens in the internal representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adam/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports completed\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from amber.datasets import TextDataset\n",
        "from amber.language_model.language_model import LanguageModel\n",
        "from amber.store.local_store import LocalStore\n",
        "\n",
        "print(\"‚úÖ Imports completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Activations with Attention Masks Example\n",
            "üì± Using device: cpu\n",
            "üîß Model: speakleash/Bielik-1.5B-v3.0-Instruct\n",
            "üìä Dataset: roneneldan/TinyStories\n",
            "üíæ Run ID: activations_with_masks_20251209_220425\n",
            "\n",
            "‚úÖ Output directories created\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_ID = \"speakleash/Bielik-1.5B-v3.0-Instruct\"  # Bielik model\n",
        "HF_DATASET = \"roneneldan/TinyStories\"\n",
        "DATA_SPLIT = \"train\"\n",
        "TEXT_FIELD = \"text\"\n",
        "DATA_LIMIT = 100  # Number of text samples to use\n",
        "MAX_LENGTH = 128  # Maximum sequence length\n",
        "BATCH_SIZE_SAVE = 16  # Batch size for saving activations\n",
        "\n",
        "# Choose which layer to hook - you can inspect available layers with model.layers.print_layer_names()\n",
        "# For Bielik, we'll use a transformer layer - adjust based on actual layer names\n",
        "LAYER_SIGNATURE = None  # Will be set after model loading\n",
        "\n",
        "# Storage locations\n",
        "STORE_DIR = Path(\"store\")\n",
        "RUN_ID = f\"activations_with_masks_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"üöÄ Starting Activations with Attention Masks Example\")\n",
        "print(f\"üì± Using device: {DEVICE}\")\n",
        "print(f\"üîß Model: {MODEL_ID}\")\n",
        "print(f\"üìä Dataset: {HF_DATASET}\")\n",
        "print(f\"üíæ Run ID: {RUN_ID}\")\n",
        "print()\n",
        "\n",
        "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"‚úÖ Output directories created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading language model...\n",
            "‚úÖ Model loaded: speakleash_Bielik-1.5B-v3.0-Instruct\n",
            "üì± Device: cpu\n",
            "üìÅ Store location: store\n",
            "\n",
            "üîç Available layers (first 20):\n",
            "  0: llamaforcausallm_model\n",
            "  1: llamaforcausallm_model_embed_tokens\n",
            "  2: llamaforcausallm_model_layers\n",
            "  3: llamaforcausallm_model_layers_0\n",
            "  4: llamaforcausallm_model_layers_0_self_attn\n",
            "  5: llamaforcausallm_model_layers_0_self_attn_q_proj\n",
            "  6: llamaforcausallm_model_layers_0_self_attn_k_proj\n",
            "  7: llamaforcausallm_model_layers_0_self_attn_v_proj\n",
            "  8: llamaforcausallm_model_layers_0_self_attn_o_proj\n",
            "  9: llamaforcausallm_model_layers_0_mlp\n",
            "  10: llamaforcausallm_model_layers_0_mlp_gate_proj\n",
            "  11: llamaforcausallm_model_layers_0_mlp_up_proj\n",
            "  12: llamaforcausallm_model_layers_0_mlp_down_proj\n",
            "  13: llamaforcausallm_model_layers_0_mlp_act_fn\n",
            "  14: llamaforcausallm_model_layers_0_input_layernorm\n",
            "  15: llamaforcausallm_model_layers_0_post_attention_layernorm\n",
            "  16: llamaforcausallm_model_layers_1\n",
            "  17: llamaforcausallm_model_layers_1_self_attn\n",
            "  18: llamaforcausallm_model_layers_1_self_attn_q_proj\n",
            "  19: llamaforcausallm_model_layers_1_self_attn_k_proj\n",
            "  ... and 402 more\n",
            "\n",
            "üéØ Using first layer: llamaforcausallm_model\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load language model and store\n",
        "print(\"üì• Loading language model...\")\n",
        "\n",
        "store = LocalStore(base_path=STORE_DIR)\n",
        "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
        "print(f\"üì± Device: {DEVICE}\")\n",
        "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n",
        "print()\n",
        "\n",
        "# Print available layers to choose one\n",
        "print(\"üîç Available layers (first 20):\")\n",
        "layer_names = lm.layers.get_layer_names()\n",
        "for i, name in enumerate(layer_names[:20]):\n",
        "    print(f\"  {i}: {name}\")\n",
        "if len(layer_names) > 20:\n",
        "    print(f\"  ... and {len(layer_names) - 20} more\")\n",
        "print()\n",
        "\n",
        "# Auto-select a transformer layer if available, otherwise use first layer\n",
        "if LAYER_SIGNATURE is None:\n",
        "    # Try to find a transformer layer\n",
        "    transformer_layers = [name for name in layer_names if 'transformer' in name.lower() and ('layer' in name.lower() or 'h_' in name.lower())]\n",
        "    if transformer_layers:\n",
        "        LAYER_SIGNATURE = transformer_layers[0]\n",
        "        print(f\"üéØ Auto-selected layer: {LAYER_SIGNATURE}\")\n",
        "    else:\n",
        "        LAYER_SIGNATURE = layer_names[0] if layer_names else 0\n",
        "        print(f\"üéØ Using first layer: {LAYER_SIGNATURE}\")\n",
        "else:\n",
        "    print(f\"üéØ Using specified layer: {LAYER_SIGNATURE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Creating dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 61572.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset created: 100 samples\n",
            "üìù Sample text: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Create dataset\n",
        "print(\"üìä Creating dataset...\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "hf_dataset = load_dataset(HF_DATASET, split=DATA_SPLIT, streaming=False)\n",
        "if DATA_LIMIT > 0:\n",
        "    hf_dataset = hf_dataset.select(range(min(DATA_LIMIT, len(hf_dataset))))\n",
        "\n",
        "dataset = TextDataset(hf_dataset, store=store, text_field=TEXT_FIELD)\n",
        "\n",
        "print(f\"‚úÖ Dataset created: {len(dataset)} samples\")\n",
        "print(f\"üìù Sample text: {dataset[0][:100]}...\" if len(dataset[0]) > 100 else f\"üìù Sample text: {dataset[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-09 22:04:52,870 [INFO] amber.language_model.activations: Starting save_activations_dataset: run=activations_with_masks_20251209_220425, layer=llamaforcausallm_model, batch_size=16, device=cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Saving activations with attention masks...\n",
            "   Layer: llamaforcausallm_model\n",
            "   Batch size: 16\n",
            "   Max length: 128\n",
            "   Save attention masks: True (default)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Save activations WITH attention masks\n",
        "print(\"üíæ Saving activations with attention masks...\")\n",
        "print(f\"   Layer: {LAYER_SIGNATURE}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE_SAVE}\")\n",
        "print(f\"   Max length: {MAX_LENGTH}\")\n",
        "print(f\"   Save attention masks: True (default)\")\n",
        "print()\n",
        "\n",
        "# Save activations with attention masks enabled (default behavior)\n",
        "run_name = lm.activations.save_activations_dataset(\n",
        "    dataset,\n",
        "    layer_signature=LAYER_SIGNATURE,\n",
        "    run_name=RUN_ID,\n",
        "    batch_size=BATCH_SIZE_SAVE,\n",
        "    max_length=MAX_LENGTH,\n",
        "    autocast=False,  # Disable autocast for consistency\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Saved activations with attention masks\")\n",
        "print(f\"üìÅ Run ID: {run_name}\")\n",
        "print(f\"üìÅ Store location: {lm.context.store.base_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Verify saved data by loading it back\n",
        "print(\"üîç Verifying saved data...\")\n",
        "\n",
        "# Get list of batches\n",
        "batches = lm.context.store.list_run_batches(run_name)\n",
        "print(f\"‚úÖ Found {len(batches)} batches\")\n",
        "print()\n",
        "\n",
        "# Load first batch to inspect structure\n",
        "batch_idx = 0\n",
        "retrieved_metadata, retrieved_tensors = lm.context.store.get_detector_metadata(run_name, batch_idx)\n",
        "\n",
        "print(f\"üì¶ Batch {batch_idx} structure:\")\n",
        "print(f\"   Layers with data: {list(retrieved_tensors.keys())}\")\n",
        "print()\n",
        "\n",
        "# Check activations\n",
        "if str(LAYER_SIGNATURE) in retrieved_tensors:\n",
        "    activations = retrieved_tensors[str(LAYER_SIGNATURE)].get(\"activations\")\n",
        "    if activations is not None:\n",
        "        print(f\"‚úÖ Activations found:\")\n",
        "        print(f\"   Shape: {activations.shape}\")\n",
        "        print(f\"   Dtype: {activations.dtype}\")\n",
        "        print(f\"   Device: {activations.device}\")\n",
        "    else:\n",
        "        print(\"‚ùå Activations not found\")\n",
        "else:\n",
        "    print(f\"‚ùå Layer {LAYER_SIGNATURE} not found in saved data\")\n",
        "print()\n",
        "\n",
        "# Check attention masks\n",
        "if \"attention_masks\" in retrieved_tensors:\n",
        "    attention_mask = retrieved_tensors[\"attention_masks\"].get(\"attention_mask\")\n",
        "    if attention_mask is not None:\n",
        "        print(f\"‚úÖ Attention masks found:\")\n",
        "        print(f\"   Shape: {attention_mask.shape}\")\n",
        "        print(f\"   Dtype: {attention_mask.dtype}\")\n",
        "        print(f\"   Device: {attention_mask.device}\")\n",
        "        print(f\"   Sample values (first 5 tokens of first 3 samples):\")\n",
        "        print(f\"   {attention_mask[:3, :5].tolist()}\")\n",
        "    else:\n",
        "        print(\"‚ùå Attention mask not found\")\n",
        "else:\n",
        "    print(\"‚ùå Attention masks layer not found in saved data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Demonstrate matching activations with attention masks\n",
        "print(\"üîó Demonstrating activation-attention mask matching...\")\n",
        "print()\n",
        "\n",
        "# Load a batch\n",
        "batch_idx = 0\n",
        "retrieved_metadata, retrieved_tensors = lm.context.store.get_detector_metadata(run_name, batch_idx)\n",
        "\n",
        "# Get activations and attention masks\n",
        "activations = retrieved_tensors[str(LAYER_SIGNATURE)][\"activations\"]\n",
        "attention_mask = retrieved_tensors[\"attention_masks\"][\"attention_mask\"]\n",
        "\n",
        "print(f\"üìä Batch {batch_idx} data:\")\n",
        "print(f\"   Activations shape: {activations.shape}  # [batch_size, seq_len, d_model]\")\n",
        "print(f\"   Attention mask shape: {attention_mask.shape}  # [batch_size, seq_len]\")\n",
        "print()\n",
        "\n",
        "# Verify shapes match\n",
        "batch_size, seq_len, d_model = activations.shape\n",
        "mask_batch_size, mask_seq_len = attention_mask.shape\n",
        "\n",
        "if batch_size == mask_batch_size and seq_len == mask_seq_len:\n",
        "    print(\"‚úÖ Shapes match perfectly!\")\n",
        "    print()\n",
        "    \n",
        "    # Show how to filter activations using attention mask\n",
        "    print(\"üí° Example: Filtering activations for regular (non-padding) tokens:\")\n",
        "    print()\n",
        "    \n",
        "    # For first sample in batch\n",
        "    sample_idx = 0\n",
        "    sample_activations = activations[sample_idx]  # [seq_len, d_model]\n",
        "    sample_mask = attention_mask[sample_idx]  # [seq_len]\n",
        "    \n",
        "    # Count regular tokens\n",
        "    num_regular_tokens = sample_mask.sum().item()\n",
        "    print(f\"   Sample {sample_idx}:\")\n",
        "    print(f\"      Total tokens: {seq_len}\")\n",
        "    print(f\"      Regular tokens (attention_mask=1): {num_regular_tokens}\")\n",
        "    print(f\"      Padding tokens (attention_mask=0): {seq_len - num_regular_tokens}\")\n",
        "    print()\n",
        "    \n",
        "    # Filter activations to only regular tokens\n",
        "    regular_activations = sample_activations[sample_mask.bool()]  # [num_regular_tokens, d_model]\n",
        "    print(f\"   Filtered activations shape: {regular_activations.shape}\")\n",
        "    print(f\"   ‚úÖ Successfully filtered to {regular_activations.shape[0]} regular token activations\")\n",
        "    print()\n",
        "    \n",
        "    # Show how to apply mask across entire batch\n",
        "    print(\"üí° Example: Applying mask to entire batch:\")\n",
        "    # Expand mask to match activation dimensions\n",
        "    mask_expanded = attention_mask.unsqueeze(-1).expand_as(activations)  # [batch_size, seq_len, d_model]\n",
        "    # Masked activations (set padding positions to zero)\n",
        "    masked_activations = activations * mask_expanded\n",
        "    print(f\"   Masked activations shape: {masked_activations.shape}\")\n",
        "    print(f\"   ‚úÖ Padding positions are now zero\")\n",
        "else:\n",
        "    print(f\"‚ùå Shape mismatch!\")\n",
        "    print(f\"   Activations: batch={batch_size}, seq={seq_len}\")\n",
        "    print(f\"   Attention mask: batch={mask_batch_size}, seq={mask_seq_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Compare with and without attention masks\n",
        "print(\"üîÑ Comparing save with and without attention masks...\")\n",
        "print()\n",
        "\n",
        "# Save without attention masks\n",
        "run_name_no_mask = f\"{RUN_ID}_no_mask\"\n",
        "print(f\"üíæ Saving WITHOUT attention masks (run: {run_name_no_mask})...\")\n",
        "\n",
        "run_name_no_mask = lm.activations.save_activations_dataset(\n",
        "    dataset,\n",
        "    layer_signature=LAYER_SIGNATURE,\n",
        "    run_name=run_name_no_mask,\n",
        "    batch_size=BATCH_SIZE_SAVE,\n",
        "    max_length=MAX_LENGTH,\n",
        "    autocast=False,\n",
        "    save_attention_mask=False,  # Explicitly disable\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# Check if attention masks were saved\n",
        "retrieved_metadata_no_mask, retrieved_tensors_no_mask = lm.context.store.get_detector_metadata(run_name_no_mask, 0)\n",
        "\n",
        "if \"attention_masks\" in retrieved_tensors_no_mask:\n",
        "    print(\"‚ùå Attention masks found (unexpected!)\")\n",
        "else:\n",
        "    print(\"‚úÖ No attention masks saved (as expected)\")\n",
        "    print(f\"   Available layers: {list(retrieved_tensors_no_mask.keys())}\")\n",
        "print()\n",
        "\n",
        "# Compare with run that has attention masks\n",
        "print(f\"üìä Comparison:\")\n",
        "print(f\"   Run WITH masks ({run_name}):\")\n",
        "print(f\"      Layers: {list(retrieved_tensors.keys())}\")\n",
        "print(f\"   Run WITHOUT masks ({run_name_no_mask}):\")\n",
        "print(f\"      Layers: {list(retrieved_tensors_no_mask.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Access attention masks using the convenience method\n",
        "print(\"üîç Accessing attention masks using store convenience method...\")\n",
        "print()\n",
        "\n",
        "# Using get_detector_metadata_by_layer_by_key\n",
        "batch_idx = 0\n",
        "attention_mask = lm.context.store.get_detector_metadata_by_layer_by_key(\n",
        "    run_name, batch_idx, \"attention_masks\", \"attention_mask\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Retrieved attention mask directly:\")\n",
        "print(f\"   Shape: {attention_mask.shape}\")\n",
        "print(f\"   Dtype: {attention_mask.dtype}\")\n",
        "print()\n",
        "\n",
        "# Get activations the same way\n",
        "activations = lm.context.store.get_detector_metadata_by_layer_by_key(\n",
        "    run_name, batch_idx, str(LAYER_SIGNATURE), \"activations\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Retrieved activations directly:\")\n",
        "print(f\"   Shape: {activations.shape}\")\n",
        "print(f\"   Dtype: {activations.dtype}\")\n",
        "print()\n",
        "print(\"üí° Both can be easily accessed and matched per batch!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This example demonstrated:\n",
        "\n",
        "1. ‚úÖ **Saving activations with attention masks** - Using `save_attention_mask=True` (default) in `save_activations_dataset()`\n",
        "2. ‚úÖ **Automatic batch matching** - Attention masks are saved per batch, matching the activation batch structure\n",
        "3. ‚úÖ **Easy access** - Both activations and attention masks can be loaded from the same batch using the store API\n",
        "4. ‚úÖ **Shape verification** - Attention masks `[batch_size, seq_len]` match activation sequence dimensions `[batch_size, seq_len, d_model]`\n",
        "5. ‚úÖ **Practical usage** - Filtering activations to only regular (non-padding) tokens using attention masks\n",
        "\n",
        "**Key Benefits:**\n",
        "- No need to run separate inference to get attention masks\n",
        "- Attention masks are automatically matched to activation batches\n",
        "- Easy to filter activations to only regular tokens\n",
        "- Consistent API for accessing both activations and attention masks"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
