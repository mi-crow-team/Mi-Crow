{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Training TopKSAE Model\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a language model and dataset\n",
    "2. Save activations from a specific layer\n",
    "3. Train a TopK Sparse Autoencoder (TopKSAE) on those activations using the new `SaeTrainer` composite class\n",
    "4. Save the trained TopKSAE model\n",
    "\n",
    "The training uses overcomplete's `train_sae` functions via the `SaeTrainer` composite class, which is automatically available on all SAE instances via `sae.trainer`.\n",
    "\n",
    "All files (trained TopKSAE model, training metadata, activations) will be saved under `store/{model_id}/` for organized, model-specific storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:36:16.508511Z",
     "start_time": "2025-11-17T20:36:16.480176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from amber.datasets import TextDataset\n",
    "from amber.language_model.language_model import LanguageModel\n",
    "from amber.mechanistic.sae.modules.topk_sae import TopKSae, TopKSaeTrainingConfig\n",
    "from amber.store.local_store import LocalStore\n",
    "\n",
    "print(\"‚úÖ Imports completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:36:16.537189Z",
     "start_time": "2025-11-17T20:36:16.513670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting TopKSAE Training Example\n",
      "üì± Using device: mps\n",
      "üîß Model: sshleifer/tiny-gpt2\n",
      "üìä Dataset: roneneldan/TinyStories\n",
      "üéØ Target layer: gpt2lmheadmodel_transformer_h_0_attn_c_attn\n",
      "üî¢ TopK parameter: 8\n",
      "\n",
      "‚úÖ Output directories created\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"sshleifer/tiny-gpt2\"  # Small model for quick experimentation\n",
    "HF_DATASET = \"roneneldan/TinyStories\"\n",
    "DATA_SPLIT = \"train\"\n",
    "TEXT_FIELD = \"text\"\n",
    "DATA_LIMIT = 1000  # Number of text samples to use\n",
    "MAX_LENGTH = 64  # Maximum sequence length\n",
    "BATCH_SIZE_SAVE = 16  # Batch size for saving activations\n",
    "BATCH_SIZE_TRAIN = 32  # Batch size for SAE training\n",
    "\n",
    "# TopKSAE configuration\n",
    "TOP_K = 8  # Number of top activations to keep (sparsity parameter)\n",
    "\n",
    "# Choose which layer to hook - you can inspect available layers with model.layers.print_layer_names()\n",
    "LAYER_SIGNATURE = 'gpt2lmheadmodel_transformer_h_0_attn_c_attn'  # Attention layer (better activations)\n",
    "\n",
    "# Storage locations - will be updated after model loading to use model_id\n",
    "STORE_DIR = Path(\"store\")\n",
    "RUN_ID = f\"topk_sae_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# Model-specific paths will be set after loading the model\n",
    "SAE_MODEL_PATH = None  # Will be set to store/{model_id}/topk_sae_model.pt\n",
    "METADATA_PATH = None  # Will be set to store/{model_id}/training_metadata.json\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    DTYPE = torch.float16\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    DTYPE = None\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DTYPE = None\n",
    "\n",
    "print(\"üöÄ Starting TopKSAE Training Example\")\n",
    "print(f\"üì± Using device: {DEVICE}\")\n",
    "print(f\"üîß Model: {MODEL_ID}\")\n",
    "print(f\"üìä Dataset: {HF_DATASET}\")\n",
    "print(f\"üéØ Target layer: {LAYER_SIGNATURE}\")\n",
    "print(f\"üî¢ TopK parameter: {TOP_K}\")\n",
    "print()\n",
    "\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"‚úÖ Output directories created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:36:17.523064Z",
     "start_time": "2025-11-17T20:36:16.541300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading language model...\n",
      "üîç Available layers:\n",
      "gpt2lmheadmodel_transformer: No weight\n",
      "gpt2lmheadmodel_transformer_wte: torch.Size([50257, 2])\n",
      "gpt2lmheadmodel_transformer_wpe: torch.Size([1024, 2])\n",
      "gpt2lmheadmodel_transformer_drop: No weight\n",
      "gpt2lmheadmodel_transformer_h: No weight\n",
      "gpt2lmheadmodel_transformer_h_0: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_ln_1: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_0_attn: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_attn_c_attn: torch.Size([2, 6])\n",
      "gpt2lmheadmodel_transformer_h_0_attn_c_proj: torch.Size([2, 2])\n",
      "gpt2lmheadmodel_transformer_h_0_attn_attn_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_attn_resid_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_ln_2: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_0_mlp: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_c_fc: torch.Size([2, 8])\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_c_proj: torch.Size([8, 2])\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_act: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_1: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_ln_1: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_1_attn: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_attn_c_attn: torch.Size([2, 6])\n",
      "gpt2lmheadmodel_transformer_h_1_attn_c_proj: torch.Size([2, 2])\n",
      "gpt2lmheadmodel_transformer_h_1_attn_attn_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_attn_resid_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_ln_2: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_1_mlp: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_c_fc: torch.Size([2, 8])\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_c_proj: torch.Size([8, 2])\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_act: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_ln_f: torch.Size([2])\n",
      "gpt2lmheadmodel_lm_head: torch.Size([50257, 2])\n",
      "‚úÖ Model loaded: sshleifer_tiny-gpt2\n",
      "üì± Device: mps\n",
      "üìÅ Store base: store\n",
      "üìÅ Model directory: store/sshleifer_tiny-gpt2\n",
      "üìÅ Store location: store\n",
      "üíæ SAE model will be saved to: store/sshleifer_tiny-gpt2/topk_sae_model.pt\n",
      "üíæ Metadata will be saved to: store/sshleifer_tiny-gpt2/training_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load language model with context\n",
    "print(\"üì• Loading language model...\")\n",
    "\n",
    "store = LocalStore(STORE_DIR)\n",
    "# Load model first to get model_id\n",
    "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
    "lm.model.to(DEVICE)\n",
    "\n",
    "# Create model-specific directory for organizing all files\n",
    "MODEL_DIR = STORE_DIR / lm.model_id\n",
    "\n",
    "# Create store under model-specific directory (so activations are also organized by model)\n",
    "\n",
    "# Set the store we want to use (overrides default)\n",
    "lm.context.store = store\n",
    "\n",
    "# Update paths to use model-specific directory\n",
    "SAE_MODEL_PATH = MODEL_DIR / \"topk_sae_model.pt\"\n",
    "METADATA_PATH = MODEL_DIR / \"training_metadata.json\"\n",
    "\n",
    "# Print available layers for reference\n",
    "print(\"üîç Available layers:\")\n",
    "lm.layers.print_layer_names()\n",
    "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
    "print(f\"üì± Device: {DEVICE}\")\n",
    "print(f\"üìÅ Store base: {STORE_DIR}\")\n",
    "print(f\"üìÅ Model directory: {MODEL_DIR}\")\n",
    "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n",
    "print(f\"üíæ SAE model will be saved to: {SAE_MODEL_PATH}\")\n",
    "print(f\"üíæ Metadata will be saved to: {METADATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:36:19.454285Z",
     "start_time": "2025-11-17T20:36:17.539935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 471270.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1000 text samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load dataset\n",
    "print(\"üì• Loading dataset...\")\n",
    "dataset = TextDataset.from_huggingface(\n",
    "    HF_DATASET,\n",
    "    split=DATA_SPLIT,\n",
    "    store=store,\n",
    "    text_field=TEXT_FIELD,\n",
    "    limit=DATA_LIMIT,\n",
    ")\n",
    "print(f\"‚úÖ Loaded {len(dataset)} text samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-17T20:36:19.469492Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving activations...\n",
      "‚úÖ Saved 63 batches of activations\n",
      "üìÅ Run ID: topk_sae_training_20251217_190917\n",
      "üìÅ Store location: store\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Save activations\n",
    "print(\"üíæ Saving activations...\")\n",
    "\n",
    "# Use the store that was set on the language model\n",
    "lm.activations.save_activations_dataset(\n",
    "    dataset,\n",
    "    layer_signature=LAYER_SIGNATURE,\n",
    "    run_name=RUN_ID,\n",
    "    batch_size=BATCH_SIZE_SAVE,\n",
    "    autocast=False,  # Disable autocast for consistency\n",
    ")\n",
    "\n",
    "# Verify activations were saved\n",
    "batches = lm.context.store.list_run_batches(RUN_ID)\n",
    "print(f\"‚úÖ Saved {len(batches)} batches of activations\")\n",
    "print(f\"üìÅ Run ID: {RUN_ID}\")\n",
    "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:32:34.891065Z",
     "start_time": "2025-11-17T20:32:34.859980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating TopKSAE model...\n",
      "üìè Hidden dimension: 6\n",
      "üß† TopKSAE architecture: 6 ‚Üí 24 ‚Üí 6\n",
      "üî¢ TopK parameter: 8\n",
      "üîß Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create TopKSAE model\n",
    "print(\"üèóÔ∏è Creating TopKSAE model...\")\n",
    "\n",
    "# Get the hidden dimension from the first batch\n",
    "first_batch = lm.context.store.get_run_batch(RUN_ID, 0)\n",
    "if isinstance(first_batch, dict):\n",
    "    activations = first_batch[\"activations\"]\n",
    "else:\n",
    "    activations = first_batch[0]  # Assume first tensor is activations\n",
    "\n",
    "hidden_dim = activations.shape[-1]  # Last dimension is hidden size\n",
    "print(f\"üìè Hidden dimension: {hidden_dim}\")\n",
    "\n",
    "sae = TopKSae(\n",
    "    n_latents=hidden_dim * 4,\n",
    "    n_inputs=hidden_dim,\n",
    "    k=TOP_K,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(f\"üß† TopKSAE architecture: {hidden_dim} ‚Üí {sae.context.n_latents} ‚Üí {hidden_dim}\")\n",
    "print(f\"üî¢ TopK parameter: {sae.k}\")\n",
    "print(f\"üîß Device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:32:38.648030Z",
     "start_time": "2025-11-17T20:32:34.895789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 19:09:25,098 [INFO] amber.mechanistic.sae.sae_trainer: [SaeTrainer] Starting training run_id=topk_sae_training_20251217_190917 epochs=100 batch_size=32 device=mps use_amp=True\n",
      "2025-12-17 19:09:25,098 [INFO] amber.mechanistic.sae.sae_trainer: [SaeTrainer] Using train_sae with monitoring=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training TopKSAE...\n",
      "üìù Note: Training uses overcomplete's train_sae functions via the SaeTrainer composite class\n",
      "üîß Trainer available at: sae.trainer (type: SaeTrainer)\n",
      "\n",
      "Epoch[1/100], Loss: 0.0535, R2: -68.8907, L0: 8.0000, Dead Features: 54.2%, Time: 0.4259 seconds\n",
      "Epoch[2/100], Loss: 0.0124, R2: -15.1618, L0: 8.0000, Dead Features: 54.2%, Time: 0.2685 seconds\n",
      "Epoch[3/100], Loss: 0.0038, R2: -3.9057, L0: 8.0000, Dead Features: 54.2%, Time: 0.2904 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     10\u001b[39m config = TopKSaeTrainingConfig(\n\u001b[32m     11\u001b[39m     epochs=\u001b[32m100\u001b[39m,\n\u001b[32m     12\u001b[39m     batch_size=BATCH_SIZE_TRAIN,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     monitoring=\u001b[32m2\u001b[39m,  \u001b[38;5;66;03m# Detailed monitoring (0=silent, 1=basic, 2=detailed)\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Train using TopKSAE's train method (which delegates to sae.trainer.train())\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# The trainer uses overcomplete's train_sae_amp or train_sae functions internally\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m history = \u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRUN_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLAYER_SIGNATURE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/src/amber/mechanistic/sae/modules/topk_sae.py:116\u001b[39m, in \u001b[36mTopKSae.train\u001b[39m\u001b[34m(self, store, run_id, layer_signature, config, training_run_id)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    115\u001b[39m     config = TopKSaeTrainingConfig()\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_run_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/src/amber/mechanistic/sae/sae_trainer.py:86\u001b[39m, in \u001b[36mSaeTrainer.train\u001b[39m\u001b[34m(self, store, run_id, layer_signature, config, training_run_id)\u001b[39m\n\u001b[32m     83\u001b[39m dataloader = \u001b[38;5;28mself\u001b[39m._create_dataloader(store, run_id, layer_signature, cfg, device)\n\u001b[32m     84\u001b[39m monitoring = \u001b[38;5;28mself\u001b[39m._configure_logging(cfg, run_id)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m history = \u001b[38;5;28mself\u001b[39m._process_training_logs(logs, cfg)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg.memory_efficient:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/src/amber/mechanistic/sae/sae_trainer.py:250\u001b[39m, in \u001b[36mSaeTrainer._run_training\u001b[39m\u001b[34m(self, cfg, dataloader, criterion, optimizer, device, monitoring)\u001b[39m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cfg.verbose:\n\u001b[32m    249\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[SaeTrainer] Using train_sae with monitoring=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonitoring\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     logs = \u001b[43mtrain_sae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43msae_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnb_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmonitoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmonitoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_str\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg.verbose:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\n\u001b[32m    264\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[SaeTrainer] Overcomplete training function returned, processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(logs.get(\u001b[33m'\u001b[39m\u001b[33mavg_loss\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m[]))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epoch results...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/overcomplete/sae/train.py:180\u001b[39m, in \u001b[36mtrain_sae\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, scheduler, nb_epochs, clip_grad, monitoring, device)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m monitoring:\n\u001b[32m    179\u001b[39m     epoch_loss += loss.item()\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     epoch_error += \u001b[43m_compute_reconstruction_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     epoch_sparsity += l0_eps(z, \u001b[32m0\u001b[39m).sum().item()\n\u001b[32m    182\u001b[39m     _log_metrics(monitoring, logs, model, z, loss, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/overcomplete/sae/train.py:61\u001b[39m, in \u001b[36m_compute_reconstruction_error\u001b[39m\u001b[34m(x, x_hat)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m x.shape == x_hat.shape, \u001b[33m\"\u001b[39m\u001b[33mInput and output shapes must match.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     59\u001b[39m     x_flatten = x\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m r2 = \u001b[43mr2_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_flatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r2.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/overcomplete/metrics.py:338\u001b[39m, in \u001b[36mr2_score\u001b[39m\u001b[34m(x, x_hat)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x.shape) == \u001b[32m2\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mInput tensors must be 2D\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m ss_res = torch.mean((x - x_hat) ** \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m ss_tot = torch.mean((x - \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) ** \u001b[32m2\u001b[39m)\n\u001b[32m    340\u001b[39m r2 = \u001b[32m1\u001b[39m - (ss_res / (ss_tot + Epsilon))\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.mean(r2)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Step 5: Train TopKSAE using SaeTrainer\n",
    "print(\"üèãÔ∏è Training TopKSAE...\")\n",
    "print(\"üìù Note: Training uses overcomplete's train_sae functions via the SaeTrainer composite class\")\n",
    "print(f\"üîß Trainer available at: sae.trainer (type: {type(sae.trainer).__name__})\")\n",
    "print()\n",
    "\n",
    "# Configure training parameters\n",
    "# Note: TopKSaeTrainingConfig extends SaeTrainingConfig\n",
    "# You can also use SaeTrainingConfig directly from sae_trainer module\n",
    "config = TopKSaeTrainingConfig(\n",
    "    epochs=100,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    lr=1e-3,\n",
    "    l1_lambda=1e-4,  # L1 sparsity penalty\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    max_batches_per_epoch=50,  # Limit batches per epoch for quick training\n",
    "    verbose=True,  # Enable progress logging\n",
    "    use_amp=True,\n",
    "    amp_dtype=DTYPE,\n",
    "    clip_grad=1.0,  # Gradient clipping (overcomplete parameter)\n",
    "    monitoring=2,  # Detailed monitoring (0=silent, 1=basic, 2=detailed)\n",
    ")\n",
    "\n",
    "# Train using TopKSAE's train method (which delegates to sae.trainer.train())\n",
    "# The trainer uses overcomplete's train_sae_amp or train_sae functions internally\n",
    "history = sae.train(lm.context.store, RUN_ID, LAYER_SIGNATURE, config)\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(f\"üìà Final loss: {history['loss'][-1]:.6f}\")\n",
    "print(f\"üìà Final reconstruction MSE: {history['recon_mse'][-1]:.6f}\")\n",
    "print(f\"üìà Final L1 penalty: {history['l1'][-1]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:32:38.688922Z",
     "start_time": "2025-11-17T20:32:38.664080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 21:32:38,687 [INFO] amber.mechanistic.sae.modules.topk_sae: Saved TopKSAE to store/sshleifer_tiny-gpt2/topk_sae_model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving trained TopKSAE...\n",
      "‚úÖ TopKSAE saved to: store/sshleifer_tiny-gpt2/topk_sae_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Save trained TopKSAE\n",
    "print(\"üíæ Saving trained TopKSAE...\")\n",
    "\n",
    "# Save using TopKSAE's save method (saves overcomplete model + our metadata)\n",
    "sae.save(\n",
    "    name=\"topk_sae_model\",\n",
    "    path=SAE_MODEL_PATH.parent\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ TopKSAE saved to: {SAE_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:32:38.719563Z",
     "start_time": "2025-11-17T20:32:38.694367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training metadata saved to: store/sshleifer_tiny-gpt2/training_metadata.json\n",
      "\n",
      "üéâ TopKSAE training completed successfully!\n",
      "üìÅ All files saved under model directory: store/sshleifer_tiny-gpt2\n",
      "üìù Next: Run 02_attach_sae_and_save_texts.ipynb to attach the TopKSAE and collect top texts\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save run metadata for next example\n",
    "import json\n",
    "\n",
    "run_metadata = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"layer_signature\": LAYER_SIGNATURE,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"n_latents\": sae.context.n_latents,\n",
    "    \"k\": sae.k,\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"model_dir\": str(MODEL_DIR),\n",
    "    \"dataset\": HF_DATASET,\n",
    "    \"data_limit\": DATA_LIMIT,\n",
    "    \"sae_model_path\": str(SAE_MODEL_PATH),\n",
    "    \"store_dir\": str(STORE_DIR),\n",
    "    \"training_history\": history,\n",
    "}\n",
    "\n",
    "# Save metadata to model-specific directory\n",
    "with open(METADATA_PATH, \"w\") as f:\n",
    "    json.dump(run_metadata, f, indent=2)\n",
    "\n",
    "print(f\"üìã Training metadata saved to: {METADATA_PATH}\")\n",
    "print()\n",
    "print(\"üéâ TopKSAE training completed successfully!\")\n",
    "print(f\"üìÅ All files saved under model directory: {MODEL_DIR}\")\n",
    "print(\"üìù Next: Run 02_attach_sae_and_save_texts.ipynb to attach the TopKSAE and collect top texts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:32:38.724784Z",
     "start_time": "2025-11-17T20:32:38.723443Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
