{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 4: Save Model Inputs and Outputs\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load a language model\n",
        "2. Attach `ModelInputDetector` and `ModelOutputDetector` to capture tokenized inputs and model outputs\n",
        "3. Run inference and save the captured data\n",
        "4. Access and inspect the saved inputs and outputs\n",
        "\n",
        "The detectors are useful for:\n",
        "- Saving complete input-output pairs for analysis\n",
        "- Creating datasets for fine-tuning or training\n",
        "- Debugging model behavior\n",
        "- Analyzing model responses to specific inputs\n",
        "\n",
        "**Note:** `ModelInputDetector` uses PRE_FORWARD hook to capture inputs, while `ModelOutputDetector` uses FORWARD hook to capture outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "âœ… Imports completed\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from amber.hooks import ModelInputDetector, ModelOutputDetector\n",
        "from amber.language_model.language_model import LanguageModel\n",
        "from amber.store.local_store import LocalStore\n",
        "\n",
        "print(\"âœ… Imports completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting Input/Output Saving Example\n",
            "ğŸ“± Using device: cpu\n",
            "ğŸ”§ Model: sshleifer/tiny-gpt2\n",
            "ğŸ“ Number of test texts: 3\n",
            "\n",
            "âœ… Output directories created\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "MODEL_ID = \"sshleifer/tiny-gpt2\"  # Small model for quick experimentation\n",
        "STORE_DIR = Path(\"store\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Test texts to process\n",
        "TEST_TEXTS = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language.\",\n",
        "]\n",
        "\n",
        "print(\"ğŸš€ Starting Input/Output Saving Example\")\n",
        "print(f\"ğŸ“± Using device: {DEVICE}\")\n",
        "print(f\"ğŸ”§ Model: {MODEL_ID}\")\n",
        "print(f\"ğŸ“ Number of test texts: {len(TEST_TEXTS)}\")\n",
        "print()\n",
        "\n",
        "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"âœ… Output directories created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Loading language model...\n",
            "âœ… Model loaded: sshleifer_tiny-gpt2\n",
            "ğŸ“± Device: cpu\n",
            "ğŸ“ Store location: store\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load language model\n",
        "print(\"ğŸ“¥ Loading language model...\")\n",
        "\n",
        "store = LocalStore(STORE_DIR)\n",
        "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
        "lm.model.to(DEVICE)\n",
        "\n",
        "print(f\"âœ… Model loaded: {lm.model_id}\")\n",
        "print(f\"ğŸ“± Device: {DEVICE}\")\n",
        "print(f\"ğŸ“ Store location: {lm.context.store.base_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Creating ModelInputDetector and ModelOutputDetector...\n",
            "ğŸ“ Added 'model_inputs' to layers registry\n",
            "ğŸ“ Added 'model_outputs' to layers registry\n",
            "âœ… Detectors attached to model via layers system\n",
            "ğŸ†” Input detector ID: model_input_detector\n",
            "ğŸ†” Output detector ID: model_output_detector\n",
            "ğŸ’¾ Will save: input_ids, output_logits\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Create and attach separate input and output detectors\n",
        "print(\"ğŸ”§ Creating ModelInputDetector and ModelOutputDetector...\")\n",
        "\n",
        "# Use different layer signatures for input and output detectors\n",
        "# This ensures they save to separate subfolders in the store\n",
        "input_layer_signature = \"model_inputs\"\n",
        "output_layer_signature = \"model_outputs\"\n",
        "\n",
        "# Get the root model for hooking\n",
        "root_model = lm.model\n",
        "\n",
        "# Add both layer signatures to the layers registry\n",
        "if input_layer_signature not in lm.layers.name_to_layer:\n",
        "    lm.layers.name_to_layer[input_layer_signature] = root_model\n",
        "    print(f\"ğŸ“ Added '{input_layer_signature}' to layers registry\")\n",
        "\n",
        "if output_layer_signature not in lm.layers.name_to_layer:\n",
        "    lm.layers.name_to_layer[output_layer_signature] = root_model\n",
        "    print(f\"ğŸ“ Added '{output_layer_signature}' to layers registry\")\n",
        "\n",
        "# Create input detector (uses PRE_FORWARD hook)\n",
        "input_detector = ModelInputDetector(\n",
        "    layer_signature=input_layer_signature,\n",
        "    hook_id=\"model_input_detector\",\n",
        "    save_input_ids=True,\n",
        "    save_attention_mask=False,  # Set to True if you need attention masks\n",
        ")\n",
        "\n",
        "# Create output detector (uses FORWARD hook)\n",
        "output_detector = ModelOutputDetector(\n",
        "    layer_signature=output_layer_signature,\n",
        "    hook_id=\"model_output_detector\",\n",
        "    save_output_logits=True,\n",
        "    save_output_hidden_state=False,  # Set to True if you need hidden states\n",
        ")\n",
        "\n",
        "# Register hooks using the layers system\n",
        "input_hook_id = lm.layers.register_hook(input_layer_signature, input_detector)\n",
        "output_hook_id = lm.layers.register_hook(output_layer_signature, output_detector)\n",
        "\n",
        "print(f\"âœ… Detectors attached to model via layers system\")\n",
        "print(f\"ğŸ†” Input detector ID: {input_detector.id}\")\n",
        "print(f\"ğŸ†” Output detector ID: {output_detector.id}\")\n",
        "print(f\"ğŸ’¾ Will save: input_ids, output_logits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Running inference...\n",
            "ğŸ“ Processing 3 texts\n",
            "âœ… Inference completed\n",
            "ğŸ“Š Output type: <class 'transformers.modeling_outputs.CausalLMOutputWithCrossAttentions'>\n",
            "ğŸ“Š Encodings keys: ['input_ids', 'attention_mask']\n",
            "\n",
            "ğŸ’¡ Data captured in detectors - ready to save to store\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Run inference and capture inputs/outputs\n",
        "print(\"ğŸš€ Running inference...\")\n",
        "print(f\"ğŸ“ Processing {len(TEST_TEXTS)} texts\")\n",
        "\n",
        "# Clear any previous captures\n",
        "input_detector.clear_captured()\n",
        "output_detector.clear_captured()\n",
        "\n",
        "# Run forward pass\n",
        "output, encodings = lm.forwards(\n",
        "    TEST_TEXTS,\n",
        "    tok_kwargs={\"max_length\": 128, \"padding\": True, \"truncation\": True},\n",
        "    autocast=False,\n",
        ")\n",
        "\n",
        "# Note: For HuggingFace models called with **kwargs, the pre_forward hook\n",
        "# doesn't receive the inputs. We need to manually set them from encodings.\n",
        "input_detector.set_inputs_from_encodings(encodings)\n",
        "\n",
        "print(\"âœ… Inference completed\")\n",
        "print(f\"ğŸ“Š Output type: {type(output)}\")\n",
        "print(f\"ğŸ“Š Encodings keys: {list(encodings.keys()) if isinstance(encodings, dict) else 'N/A'}\")\n",
        "print()\n",
        "print(\"ğŸ’¡ Data captured in detectors - ready to save to store\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Inspecting captured data...\n",
            "âœ… Captured input_ids (from ModelInputDetector):\n",
            "   Shape: torch.Size([3, 10])\n",
            "   Dtype: torch.int64\n",
            "   Sample (first 10 tokens of first text): [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
            "\n",
            "âœ… Captured output_logits (from ModelOutputDetector):\n",
            "   Shape: torch.Size([3, 10, 50257])\n",
            "   Dtype: torch.float32\n",
            "   Vocabulary size: 50257\n",
            "\n",
            "ğŸ“‹ Input detector metadata:\n",
            "   input_ids_shape: (3, 10)\n",
            "\n",
            "ğŸ“‹ Output detector metadata:\n",
            "   output_logits_shape: (3, 10, 50257)\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Inspect captured inputs and outputs\n",
        "print(\"ğŸ” Inspecting captured data...\")\n",
        "\n",
        "# Get captured input_ids from input detector\n",
        "input_ids = input_detector.get_captured_input_ids()\n",
        "if input_ids is not None:\n",
        "    print(f\"âœ… Captured input_ids (from ModelInputDetector):\")\n",
        "    print(f\"   Shape: {input_ids.shape}\")\n",
        "    print(f\"   Dtype: {input_ids.dtype}\")\n",
        "    print(f\"   Sample (first 10 tokens of first text): {input_ids[0, :10].tolist()}\")\n",
        "    print()\n",
        "else:\n",
        "    print(\"âŒ No input_ids captured\")\n",
        "\n",
        "# Get captured output logits from output detector\n",
        "output_logits = output_detector.get_captured_output_logits()\n",
        "if output_logits is not None:\n",
        "    print(f\"âœ… Captured output_logits (from ModelOutputDetector):\")\n",
        "    print(f\"   Shape: {output_logits.shape}\")\n",
        "    print(f\"   Dtype: {output_logits.dtype}\")\n",
        "    print(f\"   Vocabulary size: {output_logits.shape[-1]}\")\n",
        "    print()\n",
        "else:\n",
        "    print(\"âŒ No output_logits captured\")\n",
        "\n",
        "# Show metadata from both detectors\n",
        "print(\"ğŸ“‹ Input detector metadata:\")\n",
        "for key, value in input_detector.metadata.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "print(\"\\nğŸ“‹ Output detector metadata:\")\n",
        "for key, value in output_detector.metadata.items():\n",
        "    print(f\"   {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¤ Decoding captured data...\n",
            "\n",
            "ğŸ“ Decoded input texts:\n",
            "   Text 1: The quick brown fox jumps over the lazy dog.\n",
            "   Decoded: The quick brown fox jumps over the lazy dog.\n",
            "\n",
            "   Text 2: Machine learning is a subset of artificial intelligence.\n",
            "   Decoded: Machine learning is a subset of artificial intelligence.<|endoftext|>\n",
            "\n",
            "   Text 3: Python is a popular programming language.\n",
            "   Decoded: Python is a popular programming language.<|endoftext|><|endoftext|><|endoftext|>\n",
            "\n",
            "ğŸ¯ Predicted tokens (argmax of logits):\n",
            "   Text 1 last predicted token:  factors\n",
            "   Text 2 last predicted token:  factors\n",
            "   Text 3 last predicted token:  factors\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Decode and verify the data\n",
        "print(\"ğŸ”¤ Decoding captured data...\")\n",
        "\n",
        "if input_ids is not None:\n",
        "    print(\"\\nğŸ“ Decoded input texts:\")\n",
        "    for i, text in enumerate(TEST_TEXTS):\n",
        "        decoded = lm.tokenizer.decode(input_ids[i], skip_special_tokens=False)\n",
        "        print(f\"   Text {i+1}: {text}\")\n",
        "        print(f\"   Decoded: {decoded}\")\n",
        "        print()\n",
        "\n",
        "if output_logits is not None:\n",
        "    print(\"ğŸ¯ Predicted tokens (argmax of logits):\")\n",
        "    predicted_token_ids = output_logits.argmax(dim=-1)\n",
        "    for i in range(len(TEST_TEXTS)):\n",
        "        # Get the last predicted token for each sequence\n",
        "        last_token_id = predicted_token_ids[i, -1].item()\n",
        "        predicted_token = lm.tokenizer.decode([last_token_id], skip_special_tokens=True)\n",
        "        print(f\"   Text {i+1} last predicted token: {predicted_token}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Accessing all captured tensors...\n",
            "Available tensors in input_detector.tensor_metadata:\n",
            "   input_ids: shape=torch.Size([3, 10]), dtype=torch.int64\n",
            "\n",
            "Available tensors in output_detector.tensor_metadata:\n",
            "   output_logits: shape=torch.Size([3, 10, 50257]), dtype=torch.float32\n",
            "\n",
            "âœ… All data is stored in detector.tensor_metadata\n",
            "   You can access it directly or save it to disk as needed\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Access tensor metadata directly\n",
        "print(\"ğŸ’¾ Accessing all captured tensors...\")\n",
        "\n",
        "print(\"Available tensors in input_detector.tensor_metadata:\")\n",
        "for key, tensor in input_detector.tensor_metadata.items():\n",
        "    print(f\"   {key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
        "\n",
        "print(\"\\nAvailable tensors in output_detector.tensor_metadata:\")\n",
        "for key, tensor in output_detector.tensor_metadata.items():\n",
        "    print(f\"   {key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
        "\n",
        "print(\"\\nâœ… All data is stored in detector.tensor_metadata\")\n",
        "print(\"   You can access it directly or save it to disk as needed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Saving detector outputs to store...\n",
            "âœ… Detector outputs saved to store\n",
            "ğŸ“ Run name: model_io_20251126_221443\n",
            "ğŸ“ Batch index: 0\n",
            "ğŸ“ Saved path: runs/model_io_20251126_221443/batch_0\n",
            "\n",
            "ğŸ’¡ The saved data includes:\n",
            "   - input_ids tensor (from ModelInputDetector)\n",
            "   - output_logits tensor (from ModelOutputDetector)\n",
            "   - All metadata (shapes, etc.)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Save detector outputs to store\n",
        "print(\"ğŸ’¾ Saving detector outputs to store...\")\n",
        "\n",
        "# Create a run name for this batch\n",
        "run_name = f\"model_io_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "batch_idx = 0\n",
        "\n",
        "# Save detector metadata (this saves both input and output detector data)\n",
        "saved_path = lm.save_detector_metadata(run_name, batch_idx)\n",
        "\n",
        "print(f\"âœ… Detector outputs saved to store\")\n",
        "print(f\"ğŸ“ Run name: {run_name}\")\n",
        "print(f\"ğŸ“ Batch index: {batch_idx}\")\n",
        "print(f\"ğŸ“ Saved path: {saved_path}\")\n",
        "print()\n",
        "print(\"ğŸ’¡ The saved data includes:\")\n",
        "print(\"   - input_ids tensor (from ModelInputDetector)\")\n",
        "print(\"   - output_logits tensor (from ModelOutputDetector)\")\n",
        "print(\"   - All metadata (shapes, etc.)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Verifying saved data...\n",
            "âœ… Loaded metadata for 2 layer(s)\n",
            "âœ… Loaded tensors for 2 layer(s)\n",
            "\n",
            "ğŸ“¦ Saved tensors:\n",
            "   Layer 'model_inputs':\n",
            "      input_ids: shape=torch.Size([3, 10]), dtype=torch.int64\n",
            "   Layer 'model_outputs':\n",
            "      output_logits: shape=torch.Size([3, 10, 50257]), dtype=torch.float32\n",
            "\n",
            "ğŸ“‹ Saved metadata:\n",
            "   Layer 'model_inputs':\n",
            "      input_ids_shape: [3, 10]\n",
            "   Layer 'model_outputs':\n",
            "      output_logits_shape: [3, 10, 50257]\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Verify saved data by loading it back\n",
        "print(\"ğŸ” Verifying saved data...\")\n",
        "\n",
        "# Load the saved detector metadata\n",
        "retrieved_metadata, retrieved_tensors = lm.store.get_detector_metadata(run_name, batch_idx)\n",
        "\n",
        "print(f\"âœ… Loaded metadata for {len(retrieved_metadata)} layer(s)\")\n",
        "print(f\"âœ… Loaded tensors for {len(retrieved_tensors)} layer(s)\")\n",
        "\n",
        "# Check what was saved for input and output layers\n",
        "print(f\"\\nğŸ“¦ Saved tensors:\")\n",
        "for layer_sig in [input_layer_signature, output_layer_signature]:\n",
        "    if layer_sig in retrieved_tensors:\n",
        "        print(f\"   Layer '{layer_sig}':\")\n",
        "        for tensor_key, tensor in retrieved_tensors[layer_sig].items():\n",
        "            print(f\"      {tensor_key}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
        "\n",
        "print(f\"\\nğŸ“‹ Saved metadata:\")\n",
        "for layer_sig in [input_layer_signature, output_layer_signature]:\n",
        "    if layer_sig in retrieved_metadata:\n",
        "        print(f\"   Layer '{layer_sig}':\")\n",
        "        for key, value in retrieved_metadata[layer_sig].items():\n",
        "            print(f\"      {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ Cleaning up...\n",
            "âœ… Hooks removed and data cleared\n",
            "\n",
            "ğŸ’¡ Tip: You can process multiple batches by:\n",
            "   1. Calling detector.clear_captured() on both detectors\n",
            "   2. Re-running inference\n",
            "   3. Calling lm.save_detector_metadata(run_name, batch_idx) for each batch\n",
            "\n",
            "ğŸ’¡ Note: You can use input and output detectors independently:\n",
            "   - Use only ModelInputDetector if you only need inputs\n",
            "   - Use only ModelOutputDetector if you only need outputs\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Cleanup - remove hooks\n",
        "print(\"ğŸ§¹ Cleaning up...\")\n",
        "\n",
        "# Unregister hooks using the layers system\n",
        "lm.layers.unregister_hook(input_hook_id)\n",
        "lm.layers.unregister_hook(output_hook_id)\n",
        "\n",
        "# Clear captured data\n",
        "input_detector.clear_captured()\n",
        "output_detector.clear_captured()\n",
        "\n",
        "print(\"âœ… Hooks removed and data cleared\")\n",
        "print(\"\\nğŸ’¡ Tip: You can process multiple batches by:\")\n",
        "print(\"   1. Calling detector.clear_captured() on both detectors\")\n",
        "print(\"   2. Re-running inference\")\n",
        "print(\"   3. Calling lm.save_detector_metadata(run_name, batch_idx) for each batch\")\n",
        "print(\"\\nğŸ’¡ Note: You can use input and output detectors independently:\")\n",
        "print(\"   - Use only ModelInputDetector if you only need inputs\")\n",
        "print(\"   - Use only ModelOutputDetector if you only need outputs\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
