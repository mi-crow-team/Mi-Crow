{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Load Concepts and Demonstrate Activation Manipulation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load the language model and trained SAE from previous examples\n",
    "2. Load curated concepts from the manual curation process\n",
    "3. Attach the concept dictionary to the SAE\n",
    "4. **Access per-item neuron activation metadata** - See which neurons fire for each input\n",
    "5. Analyze neuron activation patterns across different inputs\n",
    "6. Demonstrate inference with manipulated activations\n",
    "7. Create custom activation controllers to amplify or suppress specific concepts\n",
    "\n",
    "This example shows how to use curated concepts to understand and control what the model generates, and how to analyze neuron activations for individual inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:32:55.979556Z",
     "start_time": "2025-11-24T22:32:55.949994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "âœ… Imports completed\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from amber.store import LocalStore\n",
    "from amber.language_model.language_model import LanguageModel\n",
    "from amber.mechanistic.sae.modules.topk_sae import TopKSae\n",
    "from amber.mechanistic.sae.concepts.concept_dictionary import ConceptDictionary\n",
    "\n",
    "print(\"âœ… Imports completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:32:56.011117Z",
     "start_time": "2025-11-24T22:32:55.984915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Concept Loading and Neuron Manipulation Example\n",
      "âš ï¸ Warning: No curated concepts found!\n",
      "   Expected at: store/sshleifer_tiny-gpt2/curated_concepts.csv or store/sshleifer_tiny-gpt2/curated_concepts.json\n",
      "   Please run the manual curation process first\n",
      "   You can create a simple CSV with format: neuron_idx,concept_name,score\n",
      "ðŸ”§ Model: sshleifer/tiny-gpt2\n",
      "ðŸŽ¯ Target layer: gpt2lmheadmodel_transformer_h_0_attn_c_attn\n",
      "ðŸ§  SAE model: store/sshleifer_tiny-gpt2/topk_sae_model.pt\n",
      "ðŸ“Š Curated concepts: Not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "print(\"ðŸš€ Starting Concept Loading and Neuron Manipulation Example\")\n",
    "\n",
    "MODEL_ID_HF = \"sshleifer/tiny-gpt2\"\n",
    "STORE_DIR = Path(\"store\")\n",
    "MODEL_DIR = STORE_DIR / MODEL_ID_HF.replace(\"/\", \"_\")\n",
    "training_metadata_path = MODEL_DIR / \"training_metadata.json\"\n",
    "attachment_metadata_path = MODEL_DIR / \"attachment_metadata.json\"\n",
    "\n",
    "if not training_metadata_path.exists():\n",
    "    print(f\"âŒ Error: training_metadata.json not found at {training_metadata_path}!\")\n",
    "    print(\"   Please run 01_train_sae_model.ipynb first\")\n",
    "    raise FileNotFoundError(f\"training_metadata.json not found at {training_metadata_path}\")\n",
    "\n",
    "if not attachment_metadata_path.exists():\n",
    "    print(f\"âš ï¸ Warning: attachment_metadata.json not found at {attachment_metadata_path}\")\n",
    "    print(\"   This is optional - you can still load concepts without it\")\n",
    "\n",
    "# Load metadata\n",
    "with open(training_metadata_path, \"r\") as f:\n",
    "    training_metadata = json.load(f)\n",
    "\n",
    "attachment_metadata = {}\n",
    "if attachment_metadata_path.exists():\n",
    "    with open(attachment_metadata_path, \"r\") as f:\n",
    "        attachment_metadata = json.load(f)\n",
    "\n",
    "# Configuration from metadata\n",
    "MODEL_ID = training_metadata[\"model_id\"]\n",
    "LAYER_SIGNATURE = training_metadata[\"layer_signature\"]\n",
    "SAE_MODEL_PATH = Path(training_metadata[\"sae_model_path\"])\n",
    "DATASET_DIR = Path(training_metadata.get(\"dataset_dir\", MODEL_DIR / \"cache\"))\n",
    "STORE_DIR = Path(training_metadata.get(\"store_dir\", STORE_DIR))\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Check for curated concepts (saved under model directory)\n",
    "CURATED_CONCEPTS_CSV = MODEL_DIR / \"curated_concepts.csv\"\n",
    "CURATED_CONCEPTS_JSON = MODEL_DIR / \"curated_concepts.json\"\n",
    "\n",
    "if not CURATED_CONCEPTS_CSV.exists() and not CURATED_CONCEPTS_JSON.exists():\n",
    "    print(\"âš ï¸ Warning: No curated concepts found!\")\n",
    "    print(f\"   Expected at: {CURATED_CONCEPTS_CSV} or {CURATED_CONCEPTS_JSON}\")\n",
    "    print(\"   Please run the manual curation process first\")\n",
    "    print(\"   You can create a simple CSV with format: neuron_idx,concept_name,score\")\n",
    "\n",
    "print(f\"ðŸ”§ Model: {MODEL_ID}\")\n",
    "print(f\"ðŸŽ¯ Target layer: {LAYER_SIGNATURE}\")\n",
    "print(f\"ðŸ§  SAE model: {SAE_MODEL_PATH}\")\n",
    "print(\n",
    "    f\"ðŸ“Š Curated concepts: {CURATED_CONCEPTS_CSV if CURATED_CONCEPTS_CSV.exists() else (CURATED_CONCEPTS_JSON if CURATED_CONCEPTS_JSON.exists() else 'Not found')}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:32:57.094283Z",
     "start_time": "2025-11-24T22:32:56.015007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading language model...\n",
      "âœ… Model loaded: sshleifer_tiny-gpt2\n",
      "ðŸ“± Device: cpu\n",
      "ðŸ”§ Context: concept_manipulation/manipulation_20251124_233257\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load language model\n",
    "print(\"ðŸ“¥ Loading language model...\")\n",
    "\n",
    "# Create LocalStore for the model\n",
    "store = LocalStore(MODEL_DIR)\n",
    "\n",
    "# Load model and move to device\n",
    "model = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
    "model.model.to(DEVICE)\n",
    "\n",
    "# Optional: set experiment metadata\n",
    "model.context.experiment_name = \"concept_manipulation\"\n",
    "model.context.run_id = f\"manipulation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "model.context.max_length = 64\n",
    "\n",
    "print(f\"âœ… Model loaded: {model.model_id}\")\n",
    "print(f\"ðŸ“± Device: {DEVICE}\")\n",
    "print(f\"ðŸ”§ Context: {model.context.experiment_name}/{model.context.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:32:57.135787Z",
     "start_time": "2025-11-24T22:32:57.110215Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 23:32:57,134 [INFO] amber.mechanistic.sae.modules.topk_sae: \n",
      "Loaded TopKSAE from store/sshleifer_tiny-gpt2/topk_sae_model.pt\n",
      "n_latents=24, n_inputs=6, k=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading trained SAE...\n",
      "âœ… SAE loaded: 6 â†’ 24 â†’ 6\n",
      "ðŸ”§ Context: concept_manipulation/manipulation_20251124_233257\n",
      "ðŸ“Š TopK parameter: k=8\n",
      "âœ… Trained SAE loaded\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load trained SAE\n",
    "print(\"ðŸ“¥ Loading trained SAE...\")\n",
    "if not SAE_MODEL_PATH.exists():\n",
    "    print(f\"âŒ Error: SAE model not found at {SAE_MODEL_PATH}\")\n",
    "    print(\"   Please run 01_train_sae_model.ipynb first\")\n",
    "    raise FileNotFoundError(f\"SAE model not found at {SAE_MODEL_PATH}\")\n",
    "\n",
    "sae = TopKSae.load(SAE_MODEL_PATH)\n",
    "\n",
    "# Update SAE context with current experiment info\n",
    "sae.context.experiment_name = \"concept_manipulation\"\n",
    "sae.context.run_id = f\"manipulation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "print(\n",
    "    f\"âœ… SAE loaded: {training_metadata['hidden_dim']} â†’ {training_metadata['n_latents']} â†’ {training_metadata['hidden_dim']}\")\n",
    "print(f\"ðŸ”§ Context: {sae.context.experiment_name}/{sae.context.run_id}\")\n",
    "print(f\"ðŸ“Š TopK parameter: k={training_metadata.get('k', 'N/A')}\")\n",
    "print(\"âœ… Trained SAE loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:33:41.397316Z",
     "start_time": "2025-11-24T22:33:41.374616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading curated concepts...\n",
      "ðŸ“„ Loading from JSON: store/sshleifer_tiny-gpt2/curated_concepts.json\n",
      "âœ… Loaded concept dictionary with 24 neurons\n",
      "ðŸ“Š Total concepts: 7\n",
      "\n",
      "ðŸ” Sample concepts:\n",
      "   Neuron 0: 'subject_pronouns' (score: 0.850)\n",
      "   Neuron 1: 'verbs_action' (score: 0.780)\n",
      "   Neuron 2: 'prepositions_location' (score: 0.720)\n",
      "   Neuron 3: no concept\n",
      "   Neuron 4: no concept\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load curated concepts\n",
    "print(\"ðŸ“¥ Loading curated concepts...\")\n",
    "\n",
    "# Try to load from CSV first, then JSON\n",
    "concept_dict = None\n",
    "if CURATED_CONCEPTS_CSV.exists():\n",
    "    print(f\"ðŸ“„ Loading from CSV: {CURATED_CONCEPTS_CSV}\")\n",
    "    concept_dict = ConceptDictionary.from_csv(CURATED_CONCEPTS_CSV, n_size=training_metadata[\"n_latents\"])\n",
    "elif CURATED_CONCEPTS_JSON.exists():\n",
    "    print(f\"ðŸ“„ Loading from JSON: {CURATED_CONCEPTS_JSON}\")\n",
    "    concept_dict = ConceptDictionary.from_json(CURATED_CONCEPTS_JSON, n_size=training_metadata[\"n_latents\"])\n",
    "else:\n",
    "    print(\"âŒ Error: No curated concepts file found!\")\n",
    "    print(f\"   Expected at: {CURATED_CONCEPTS_CSV} or {CURATED_CONCEPTS_JSON}\")\n",
    "    raise FileNotFoundError(\"No curated concepts file found\")\n",
    "\n",
    "print(f\"âœ… Loaded concept dictionary with {concept_dict.n_size} neurons\")\n",
    "print(f\"ðŸ“Š Total concepts: {sum(1 for i in range(concept_dict.n_size) if concept_dict.get(i) is not None)}\")\n",
    "\n",
    "# Show some concepts\n",
    "print(\"\\nðŸ” Sample concepts:\")\n",
    "for neuron_idx in range(min(5, concept_dict.n_size)):\n",
    "    concept = concept_dict.get(neuron_idx)\n",
    "    if concept:\n",
    "        print(f\"   Neuron {neuron_idx}: '{concept.name}' (score: {concept.score:.3f})\")\n",
    "    else:\n",
    "        print(f\"   Neuron {neuron_idx}: no concept\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:32:57.181014Z",
     "start_time": "2025-11-10T20:20:49.829893Z"
    }
   },
   "outputs": [],
   "source": [
    "sae.attach_dictionary(concept_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Attach SAE and Run Inference with Metadata Collection\n",
    "\n",
    "Now we'll attach the SAE to the language model and run inference. The SAE will automatically collect metadata about neuron activations for each item in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach SAE to the language model\n",
    "print(\"ðŸ”— Attaching SAE to language model...\")\n",
    "model.layers.attach_controller(LAYER_SIGNATURE, sae)\n",
    "print(f\"âœ… SAE attached to layer: {LAYER_SIGNATURE}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a small batch to collect metadata\n",
    "print(\"ðŸš€ Running inference to collect neuron activation metadata...\")\n",
    "test_texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog ran in the park.\",\n",
    "    \"The bird flew in the sky.\"\n",
    "]\n",
    "\n",
    "outputs, encodings = model.inference.infer_texts(\n",
    "    test_texts,\n",
    "    run_name=None,  # Don't save metadata, just trigger SAE metadata collection\n",
    "    batch_size=None,\n",
    "    tok_kwargs={\n",
    "        \"max_length\": 64,\n",
    "        \"padding\": True,\n",
    "        \"truncation\": True,\n",
    "        \"add_special_tokens\": True\n",
    "    },\n",
    "    autocast=False,\n",
    ")\n",
    "print(f\"âœ… Ran inference on {len(test_texts)} texts\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Access Per-Item Neuron Activation Metadata\n",
    "\n",
    "The SAE automatically saves metadata for each item in the batch, including:\n",
    "- `nonzero_indices`: List of neuron indices that were active (nonzero)\n",
    "- `activations`: Dictionary mapping neuron index to activation value\n",
    "\n",
    "This allows you to analyze which neurons fired for each input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the batch_items metadata from the SAE\n",
    "print(\"ðŸ“Š Accessing per-item neuron activation metadata...\")\n",
    "print()\n",
    "\n",
    "if 'batch_items' in sae.metadata:\n",
    "    batch_items = sae.metadata['batch_items']\n",
    "    print(f\"âœ… Found metadata for {len(batch_items)} items\")\n",
    "    print()\n",
    "\n",
    "    # Display metadata for each item\n",
    "    for item_idx, (text, item_metadata) in enumerate(zip(test_texts, batch_items)):\n",
    "        print(f\"ðŸ“ Item {item_idx + 1}: {text[:50]}...\")\n",
    "        print(f\"   Active neurons: {len(item_metadata['nonzero_indices'])}\")\n",
    "        print(\n",
    "            f\"   Neuron indices: {item_metadata['nonzero_indices'][:10]}{'...' if len(item_metadata['nonzero_indices']) > 10 else ''}\")\n",
    "\n",
    "        # Show top activations\n",
    "        activations = item_metadata['activations']\n",
    "        if activations:\n",
    "            top_activations = sorted(activations.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "            print(f\"   Top 5 activations:\")\n",
    "            for neuron_idx, activation_value in top_activations:\n",
    "                # Get concept name if available\n",
    "                concept_info = \"\"\n",
    "                if sae.concepts.dictionary is not None:\n",
    "                    concept = sae.concepts.dictionary.get(neuron_idx)\n",
    "                    if concept:\n",
    "                        concept_info = f\" ({concept.name})\"\n",
    "                print(f\"      Neuron {neuron_idx}: {activation_value:.4f}{concept_info}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸ No batch_items metadata found. Make sure SAE is attached and inference was run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Neuron Patterns Across Items\n",
    "\n",
    "We can also analyze which neurons are commonly active across different inputs, or find neurons that are specific to certain types of text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze neuron activation patterns\n",
    "if 'batch_items' in sae.metadata:\n",
    "    batch_items = sae.metadata['batch_items']\n",
    "\n",
    "    # Count how many times each neuron is active\n",
    "    neuron_counts = {}\n",
    "    for item_metadata in batch_items:\n",
    "        for neuron_idx in item_metadata['nonzero_indices']:\n",
    "            neuron_counts[neuron_idx] = neuron_counts.get(neuron_idx, 0) + 1\n",
    "\n",
    "    # Find neurons that are active in all items (common patterns)\n",
    "    n_items = len(batch_items)\n",
    "    common_neurons = [idx for idx, count in neuron_counts.items() if count == n_items]\n",
    "\n",
    "    # Find neurons that are active in only one item (specific patterns)\n",
    "    specific_neurons = [idx for idx, count in neuron_counts.items() if count == 1]\n",
    "\n",
    "    print(\"ðŸ” Neuron Activation Analysis:\")\n",
    "    print(f\"   Total unique active neurons: {len(neuron_counts)}\")\n",
    "    print(f\"   Neurons active in all items: {len(common_neurons)}\")\n",
    "    print(f\"   Neurons active in only one item: {len(specific_neurons)}\")\n",
    "    print()\n",
    "\n",
    "    if common_neurons:\n",
    "        print(\"ðŸ§  Common neurons (active in all items):\")\n",
    "        for neuron_idx in common_neurons[:10]:\n",
    "            concept_info = \"\"\n",
    "            if sae.concepts.dictionary is not None:\n",
    "                concept = sae.concepts.dictionary.get(neuron_idx)\n",
    "                if concept:\n",
    "                    concept_info = f\" - {concept.name}\"\n",
    "            print(f\"   Neuron {neuron_idx}{concept_info}\")\n",
    "        if len(common_neurons) > 10:\n",
    "            print(f\"   ... and {len(common_neurons) - 10} more\")\n",
    "        print()\n",
    "\n",
    "    if specific_neurons:\n",
    "        print(\"ðŸŽ¯ Specific neurons (active in only one item):\")\n",
    "        for neuron_idx in specific_neurons[:10]:\n",
    "            concept_info = \"\"\n",
    "            if sae.concepts.dictionary is not None:\n",
    "                concept = sae.concepts.dictionary.get(neuron_idx)\n",
    "                if concept:\n",
    "                    concept_info = f\" - {concept.name}\"\n",
    "            print(f\"   Neuron {neuron_idx}{concept_info}\")\n",
    "        if len(specific_neurons) > 10:\n",
    "            print(f\"   ... and {len(specific_neurons) - 10} more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber.datasets import TextDataset\n",
    "\n",
    "HF_DATASET = \"roneneldan/TinyStories\"\n",
    "DATA_SPLIT = \"train\"\n",
    "TEXT_FIELD = \"text\"\n",
    "DATA_LIMIT = 500\n",
    "MAX_LENGTH = 64\n",
    "\n",
    "dataset = TextDataset.from_huggingface(\n",
    "    HF_DATASET,\n",
    "    split=DATA_SPLIT,\n",
    "    dataset_dir=str(DATASET_DIR),\n",
    "    text_field=TEXT_FIELD,\n",
    "    limit=DATA_LIMIT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index, texts in enumerate(dataset.iter_batches(32)):\n",
    "    output, encodings = model.inference.infer_texts(\n",
    "        texts,\n",
    "        run_name=None,  # Don't save metadata, just trigger SAE metadata collection\n",
    "        batch_size=None,\n",
    "        tok_kwargs={\n",
    "            \"max_length\": MAX_LENGTH,\n",
    "            \"padding\": True,\n",
    "            \"truncation\": True,\n",
    "            \"add_special_tokens\": True\n",
    "        },\n",
    "        autocast=False,\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T22:32:57.182842Z",
     "start_time": "2025-11-10T20:20:51.824042Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
