{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GadziJƒôzyk Dataset\n",
    "\n",
    "This notebook demonstrates how to load the **GadziJƒôzyk** dataset using Amber's dataset abstractions.\n",
    "\n",
    "## ‚ö†Ô∏è Dataset Information Needed\n",
    "\n",
    "**Note:** The exact source and structure of the GadziJƒôzyk dataset could not be automatically determined. Please update this notebook with the correct:\n",
    "\n",
    "1. **Dataset source:** HuggingFace repo ID, local path, CSV file, or other source\n",
    "2. **Dataset structure:** Field names, splits, and data format\n",
    "3. **Dataset type:** Text dataset, classification dataset, or other\n",
    "\n",
    "Once you have this information, update the configuration section below and adjust the loading code accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.446554Z",
     "start_time": "2025-11-11T18:23:06.319243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from amber.adapters import TextDataset, ClassificationDataset\n",
    "from amber.adapters.loading_strategy import LoadingStrategy\n",
    "from amber.store.local_store import LocalStore\n",
    "\n",
    "print(\"‚úÖ Imports completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "**‚ö†Ô∏è Update these values based on the actual dataset source and structure:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.469716Z",
     "start_time": "2025-11-11T18:23:07.449588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Please update the configuration above with the correct dataset source!\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "# Option 1: HuggingFace Hub (if available)\n",
    "DATASET_REPO_ID = None  # e.g., \"username/gadzijezyk\" or \"organization/gadzijezyk\"\n",
    "SPLIT = \"train\"  # Usually \"train\", \"test\", \"validation\", etc.\n",
    "\n",
    "# Option 2: Local CSV file path\n",
    "CSV_PATH = None  # e.g., Path(\"../data/gadzijezyk.csv\")\n",
    "\n",
    "# Option 3: Local JSON/JSONL file\n",
    "JSON_PATH = None  # e.g., Path(\"../data/gadzijezyk.json\")\n",
    "\n",
    "# Option 4: Local directory (for .txt files)\n",
    "LOCAL_DIR = None  # e.g., Path(\"../data/gadzijezyk\")\n",
    "\n",
    "# Loading strategy\n",
    "LOADING_STRATEGY = LoadingStrategy.MEMORY  # Use STREAM for large datasets\n",
    "\n",
    "# Field names (adjust based on actual dataset structure)\n",
    "TEXT_FIELD = \"text\"  # Column name containing text content\n",
    "CATEGORY_FIELD = \"label\"  # Column name containing category/label (if classification dataset)\n",
    "\n",
    "# Storage configuration\n",
    "STORE_DIR = Path(\"../store\")  # Relative to examples directory\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: Limit number of samples for quick testing\n",
    "LIMIT = None  # Set to a number (e.g., 100) to limit samples\n",
    "\n",
    "print(\"‚ö†Ô∏è  Please update the configuration above with the correct dataset source!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Load from CSV File\n",
    "\n",
    "If the dataset is stored as a CSV file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.484999Z",
     "start_time": "2025-11-11T18:23:07.472212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  CSV_PATH not set. Skipping CSV load.\n"
     ]
    }
   ],
   "source": [
    "if CSV_PATH:\n",
    "    # Create store instance\n",
    "    store = LocalStore(STORE_DIR)\n",
    "    \n",
    "    print(f\"üì• Loading from CSV: {CSV_PATH}...\")\n",
    "    \n",
    "    # If it's a text-only dataset:\n",
    "    # dataset = TextDataset.from_csv(\n",
    "    #     source=CSV_PATH,\n",
    "    #     store=store,\n",
    "    #     loading_strategy=LOADING_STRATEGY,\n",
    "    #     text_field=TEXT_FIELD,\n",
    "    # )\n",
    "    \n",
    "    # Or, if it's a classification dataset:\n",
    "    dataset = ClassificationDataset.from_csv(\n",
    "        source=CSV_PATH,\n",
    "        store=store,\n",
    "        loading_strategy=LOADING_STRATEGY,\n",
    "        text_field=TEXT_FIELD,\n",
    "        category_field=CATEGORY_FIELD,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Number of samples: {len(dataset)}\")\n",
    "    if hasattr(dataset, 'get_categories'):\n",
    "        print(f\"üè∑Ô∏è  Categories: {dataset.get_categories()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CSV_PATH not set. Skipping CSV load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Load from HuggingFace Hub\n",
    "\n",
    "If the dataset is available on HuggingFace Hub:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.503426Z",
     "start_time": "2025-11-11T18:23:07.490568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  DATASET_REPO_ID not set. Skipping HuggingFace load.\n"
     ]
    }
   ],
   "source": [
    "if DATASET_REPO_ID:\n",
    "    # Create store instance\n",
    "    store = LocalStore(STORE_DIR)\n",
    "    \n",
    "    # Determine if it's a classification or text dataset\n",
    "    # If you have labels/categories, use ClassificationDataset\n",
    "    # Otherwise, use TextDataset\n",
    "    \n",
    "    # Example: Load as TextDataset (if no labels)\n",
    "    print(f\"üì• Loading {DATASET_REPO_ID} from HuggingFace...\")\n",
    "    # dataset = TextDataset.from_huggingface(\n",
    "    #     repo_id=DATASET_REPO_ID,\n",
    "    #     store=store,\n",
    "    #     split=SPLIT,\n",
    "    #     loading_strategy=LOADING_STRATEGY,\n",
    "    #     text_field=TEXT_FIELD,\n",
    "    #     limit=LIMIT,\n",
    "    # )\n",
    "    \n",
    "    # Or, if it's a classification dataset:\n",
    "    # dataset = ClassificationDataset.from_huggingface(\n",
    "    #     repo_id=DATASET_REPO_ID,\n",
    "    #     store=store,\n",
    "    #     split=SPLIT,\n",
    "    #     loading_strategy=LOADING_STRATEGY,\n",
    "    #     text_field=TEXT_FIELD,\n",
    "    #     category_field=CATEGORY_FIELD,\n",
    "    #     limit=LIMIT,\n",
    "    # )\n",
    "    \n",
    "    # print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    # print(f\"üìä Number of samples: {len(dataset)}\")\n",
    "    print(\"‚ö†Ô∏è  Uncomment and adjust the code above based on your dataset type.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DATASET_REPO_ID not set. Skipping HuggingFace load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Load from Local Directory\n",
    "\n",
    "If the dataset is stored locally as a directory of text files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.521344Z",
     "start_time": "2025-11-11T18:23:07.509615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  LOCAL_DIR not set. Skipping local directory load.\n"
     ]
    }
   ],
   "source": [
    "if LOCAL_DIR:\n",
    "    # Create store instance\n",
    "    store = LocalStore(STORE_DIR)\n",
    "    \n",
    "    print(f\"üì• Loading from local directory: {LOCAL_DIR}...\")\n",
    "    dataset = TextDataset.from_local(\n",
    "        source=LOCAL_DIR,\n",
    "        store=store,\n",
    "        loading_strategy=LOADING_STRATEGY,\n",
    "        text_field=TEXT_FIELD,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Number of samples: {len(dataset)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LOCAL_DIR not set. Skipping local directory load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 4: Load from JSON/JSONL File\n",
    "\n",
    "If the dataset is stored as a JSON or JSONL file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.535323Z",
     "start_time": "2025-11-11T18:23:07.523871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  JSON_PATH not set. Skipping JSON load.\n"
     ]
    }
   ],
   "source": [
    "if JSON_PATH:\n",
    "    # Create store instance\n",
    "    store = LocalStore(STORE_DIR)\n",
    "    \n",
    "    print(f\"üì• Loading from JSON: {JSON_PATH}...\")\n",
    "    \n",
    "    # If it's a text-only dataset:\n",
    "    # dataset = TextDataset.from_json(\n",
    "    #     source=JSON_PATH,\n",
    "    #     store=store,\n",
    "    #     loading_strategy=LOADING_STRATEGY,\n",
    "    #     text_field=TEXT_FIELD,\n",
    "    # )\n",
    "    \n",
    "    # Or, if it's a classification dataset:\n",
    "    # dataset = ClassificationDataset.from_json(\n",
    "    #     source=JSON_PATH,\n",
    "    #     store=store,\n",
    "    #     loading_strategy=LOADING_STRATEGY,\n",
    "    #     text_field=TEXT_FIELD,\n",
    "    #     category_field=CATEGORY_FIELD,\n",
    "    # )\n",
    "    \n",
    "    # print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    # print(f\"üìä Number of samples: {len(dataset)}\")\n",
    "    print(\"‚ö†Ô∏è  Uncomment and adjust the code above based on your dataset type.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  JSON_PATH not set. Skipping JSON load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset Structure (if loading from HuggingFace)\n",
    "\n",
    "If you're loading from HuggingFace and unsure about the structure, inspect it first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:23:07.554530Z",
     "start_time": "2025-11-11T18:23:07.541138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  DATASET_REPO_ID not set. Skipping inspection.\n"
     ]
    }
   ],
   "source": [
    "if DATASET_REPO_ID:\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"üîç Inspecting dataset structure...\")\n",
    "    raw_dataset = load_dataset(\n",
    "        DATASET_REPO_ID,\n",
    "        SPLIT,\n",
    "        streaming=False,\n",
    "    )\n",
    "    \n",
    "    # Get first example\n",
    "    if hasattr(raw_dataset, '__getitem__'):\n",
    "        first_example = raw_dataset[0]\n",
    "    else:\n",
    "        first_example = next(iter(raw_dataset))\n",
    "    \n",
    "    print(\"\\nüìã Dataset columns:\")\n",
    "    print(raw_dataset.column_names if hasattr(raw_dataset, 'column_names') else list(first_example.keys()))\n",
    "    \n",
    "    print(\"\\nüìù First example:\")\n",
    "    for key, value in first_example.items():\n",
    "        if isinstance(value, str) and len(value) > 200:\n",
    "            print(f\"  {key}: {value[:200]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  DATASET_REPO_ID not set. Skipping inspection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **Update Configuration:** Make sure to update the configuration section with the correct dataset source and field names\n",
    "- **Dataset Type:** Choose the appropriate dataset class (`TextDataset` or `ClassificationDataset`) based on your data\n",
    "- **Field Names:** Adjust `TEXT_FIELD` and `CATEGORY_FIELD` to match your dataset's actual column names\n",
    "- **Caching:** The dataset is cached locally in the store directory for faster subsequent loads\n",
    "- **Streaming:** Use `LoadingStrategy.STREAM` for very large datasets to avoid loading everything into memory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
