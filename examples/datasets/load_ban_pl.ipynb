{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading BAN-PL Dataset\n",
    "\n",
    "This notebook demonstrates how to load the **BAN-PL** dataset using Amber's dataset abstractions.\n",
    "\n",
    "## About BAN-PL\n",
    "\n",
    "BAN-PL is a Polish dataset containing harmful and offensive content from Wykop.pl. The anonymized subset consists of 24,000 examples evenly split between \"harmful\" and \"neutral\" classes.\n",
    "\n",
    "**Source:** Available as CSV file from GitHub repository\n",
    "\n",
    "**GitHub:** [NASK-PIB/BAN-PL](https://github.com/NASK-PIB/BAN-PL)\n",
    "\n",
    "**Paper:** [BAN-PL: A Polish Dataset of Banned Harmful and Offensive Content](https://aclanthology.org/2024.lrec-main.190/)\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "This is a classification dataset with:\n",
    "- Text content (potentially harmful or neutral)\n",
    "- Labels: \"harmful\" or \"neutral\"\n",
    "\n",
    "**Note:** The dataset contains potentially offensive content. Use responsibly.\n",
    "\n",
    "**Important:** BAN-PL is not available on HuggingFace Hub. You need to download the CSV file from the GitHub repository first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:09:06.669897Z",
     "start_time": "2025-11-11T18:09:05.550153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from amber.adapters import ClassificationDataset\n",
    "from amber.adapters.loading_strategy import LoadingStrategy\n",
    "from amber.store.local_store import LocalStore\n",
    "\n",
    "print(\"‚úÖ Imports completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:09:06.695142Z",
     "start_time": "2025-11-11T18:09:06.673264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset: NASK-PIB/BAN-PL\n",
      "üìÅ Store directory: ../store\n",
      "üîß Loading strategy: LoadingStrategy.MEMORY\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "# Option 1: Local CSV file path (download from GitHub first)\n",
    "CSV_PATH = Path(\"../data/BAN-PL.csv\")  # Update this path to your CSV file location\n",
    "\n",
    "# Option 2: Download from GitHub (uncomment to use)\n",
    "# CSV_URL = \"https://raw.githubusercontent.com/NASK-PIB/BAN-PL/main/data/BAN-PL.csv\"\n",
    "\n",
    "LOADING_STRATEGY = LoadingStrategy.MEMORY  # Use STREAM for large datasets\n",
    "\n",
    "# Field names (adjust if the dataset uses different column names)\n",
    "# Common column names in BAN-PL CSV: \"text\", \"label\" or \"harmful\"\n",
    "TEXT_FIELD = \"text\"  # Column name containing text content\n",
    "CATEGORY_FIELD = \"label\"  # Column name containing category/label (may be \"harmful\" or \"label\")\n",
    "\n",
    "# Storage configuration\n",
    "STORE_DIR = Path(\"../store\")  # Relative to examples directory\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optional: Limit number of samples for quick testing\n",
    "LIMIT = None  # Set to a number (e.g., 100) to limit samples\n",
    "\n",
    "print(f\"üìä Dataset: BAN-PL (CSV)\")\n",
    "print(f\"üìÅ CSV path: {CSV_PATH}\")\n",
    "print(f\"üìÅ Store directory: {STORE_DIR}\")\n",
    "print(f\"üîß Loading strategy: {LOADING_STRATEGY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset (if needed)\n",
    "\n",
    "If you haven't downloaded the CSV file yet, you can download it from GitHub:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:09:07.082019Z",
     "start_time": "2025-11-11T18:09:06.711226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading NASK-PIB/BAN-PL...\n"
     ]
    },
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'NASK-PIB/BAN-PL' doesn't exist on the Hub or cannot be accessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDatasetNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load dataset from HuggingFace\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müì• Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_REPO_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dataset = \u001b[43mClassificationDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_huggingface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATASET_REPO_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloading_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOADING_STRATEGY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_field\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEXT_FIELD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategory_field\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCATEGORY_FIELD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLIMIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Number of samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/src/amber/adapters/classification_dataset.py:166\u001b[39m, in \u001b[36mClassificationDataset.from_huggingface\u001b[39m\u001b[34m(cls, repo_id, store, split, loading_strategy, revision, text_field, category_field, filters, limit, streaming, **kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load classification dataset from HuggingFace Hub.\"\"\"\u001b[39;00m\n\u001b[32m    164\u001b[39m use_streaming = streaming \u001b[38;5;28;01mif\u001b[39;00m streaming \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (loading_strategy == LoadingStrategy.STREAM)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m ds = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_streaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_streaming:\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filters:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/datasets/load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/datasets/load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/datasets/load.py:1025\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1027\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1028\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1030\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/datasets/load.py:980\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[32m    977\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRevision \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist for dataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    978\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist on the Hub or cannot be accessed.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    982\u001b[39m     api.hf_hub_download(\n\u001b[32m    983\u001b[39m         repo_id=path,\n\u001b[32m    984\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    987\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    988\u001b[39m     )\n",
      "\u001b[31mDatasetNotFoundError\u001b[39m: Dataset 'NASK-PIB/BAN-PL' doesn't exist on the Hub or cannot be accessed."
     ]
    }
   ],
   "source": [
    "# Download CSV from GitHub if not already present\n",
    "import urllib.request\n",
    "\n",
    "if not CSV_PATH.exists():\n",
    "    print(f\"üì• Downloading BAN-PL dataset from GitHub...\")\n",
    "    CSV_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Try to download from GitHub\n",
    "    try:\n",
    "        url = \"https://raw.githubusercontent.com/NASK-PIB/BAN-PL/main/data/BAN-PL.csv\"\n",
    "        urllib.request.urlretrieve(url, CSV_PATH)\n",
    "        print(f\"‚úÖ Downloaded to: {CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading: {e}\")\n",
    "        print(f\"üí° Please download the CSV file manually from:\")\n",
    "        print(f\"   https://github.com/NASK-PIB/BAN-PL\")\n",
    "        print(f\"   And place it at: {CSV_PATH}\")\n",
    "else:\n",
    "    print(f\"‚úÖ CSV file found at: {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create store instance\n",
    "store = LocalStore(STORE_DIR)\n",
    "\n",
    "# Load dataset from CSV\n",
    "print(f\"üì• Loading BAN-PL from CSV: {CSV_PATH}...\")\n",
    "\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"CSV file not found at {CSV_PATH}. \"\n",
    "        f\"Please download it from https://github.com/NASK-PIB/BAN-PL \"\n",
    "        f\"or update CSV_PATH in the configuration.\"\n",
    "    )\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ClassificationDataset.from_csv(\n",
    "    source=CSV_PATH,\n",
    "    store=store,\n",
    "    loading_strategy=LOADING_STRATEGY,\n",
    "    text_field=TEXT_FIELD,\n",
    "    category_field=CATEGORY_FIELD,\n",
    ")\n",
    "\n",
    "# Apply limit if specified (before loading, we'd need to filter the CSV)\n",
    "# For now, we'll load the full dataset and you can slice it later\n",
    "# If you need to limit, you can use: dataset = dataset[:LIMIT] after loading\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Number of samples: {len(dataset)}\")\n",
    "print(f\"üè∑Ô∏è  Categories: {dataset.get_categories()}\")\n",
    "\n",
    "# Apply limit if specified (slice the dataset)\n",
    "if LIMIT is not None and not dataset.is_streaming:\n",
    "    print(f\"üìâ Limiting to first {LIMIT} samples...\")\n",
    "    # Note: This creates a new dataset with limited samples\n",
    "    from datasets import Dataset\n",
    "    limited_texts = dataset.get_texts()[:LIMIT]\n",
    "    limited_categories = dataset.get_categories_for_texts(limited_texts)\n",
    "    limited_ds = Dataset.from_dict({\n",
    "        TEXT_FIELD: limited_texts,\n",
    "        CATEGORY_FIELD: limited_categories\n",
    "    })\n",
    "    dataset = ClassificationDataset(\n",
    "        limited_ds,\n",
    "        store=store,\n",
    "        loading_strategy=LOADING_STRATEGY,\n",
    "        text_field=TEXT_FIELD,\n",
    "        category_field=CATEGORY_FIELD,\n",
    "    )\n",
    "    print(f\"üìä Limited dataset size: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Inspect CSV Structure First\n",
    "\n",
    "If you're unsure about the column names, inspect the CSV file first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect CSV structure (optional)\n",
    "import pandas as pd\n",
    "\n",
    "if CSV_PATH.exists():\n",
    "    print(\"üîç Inspecting CSV structure...\")\n",
    "    df_sample = pd.read_csv(CSV_PATH, nrows=5)\n",
    "    print(\"\\nüìã Column names:\")\n",
    "    print(df_sample.columns.tolist())\n",
    "    print(\"\\nüìù First few rows:\")\n",
    "    print(df_sample.head())\n",
    "    print(f\"\\nüí° Update TEXT_FIELD and CATEGORY_FIELD if needed based on the column names above.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CSV file not found. Please download it first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample item\n",
    "sample = dataset[0]\n",
    "print(\"üìù Sample item:\")\n",
    "print(f\"Text: {sample['text'][:200]}...\" if len(sample['text']) > 200 else f\"Text: {sample['text']}\")\n",
    "print(f\"Category: {sample['category']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple samples\n",
    "samples = dataset[:5]\n",
    "print(f\"üì¶ Retrieved {len(samples)} samples:\")\n",
    "for i, item in enumerate(samples):\n",
    "    print(f\"\\n{i+1}. Category: {item['category']}\")\n",
    "    print(f\"   Text preview: {item['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Over Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over items one by one\n",
    "print(\"üîÑ Iterating over first 3 items:\")\n",
    "for i, item in enumerate(dataset.iter_items()):\n",
    "    if i >= 3:\n",
    "        break\n",
    "    print(f\"\\nItem {i+1}:\")\n",
    "    print(f\"  Category: {item['category']}\")\n",
    "    print(f\"  Text: {item['text'][:80]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate in batches\n",
    "print(\"üì¶ Iterating in batches of 10:\")\n",
    "batch_count = 0\n",
    "for batch in dataset.iter_batches(batch_size=10):\n",
    "    batch_count += 1\n",
    "    print(f\"\\nBatch {batch_count} ({len(batch)} items):\")\n",
    "    categories = [item['category'] for item in batch]\n",
    "    print(f\"  Categories: {categories}\")\n",
    "    if batch_count >= 2:  # Show only first 2 batches\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get category distribution\n",
    "categories = dataset.get_categories()\n",
    "print(f\"üè∑Ô∏è  Available categories: {categories}\")\n",
    "\n",
    "# Count items per category (for non-streaming datasets)\n",
    "if not dataset.is_streaming:\n",
    "    category_counts = {}\n",
    "    for item in dataset.iter_items():\n",
    "        cat = item['category']\n",
    "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
    "    \n",
    "    print(\"\\nüìä Category distribution:\")\n",
    "    for cat, count in sorted(category_counts.items()):\n",
    "        print(f\"  {cat}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- The dataset is cached locally in the store directory for faster subsequent loads\n",
    "- Use `LoadingStrategy.STREAM` for very large datasets to avoid loading everything into memory\n",
    "- Adjust `TEXT_FIELD` and `CATEGORY_FIELD` if the dataset uses different column names\n",
    "- The dataset contains potentially harmful content - use responsibly and in accordance with ethical guidelines\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
