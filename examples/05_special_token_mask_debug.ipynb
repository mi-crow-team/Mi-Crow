{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example 5: Special Token Mask Detection\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Load a language model\n",
        "2. Create `ModelInputDetector` with special token mask detection enabled\n",
        "3. Use model's automatic special token detection OR provide custom special token IDs\n",
        "4. Run inference and capture special token masks\n",
        "5. Visualize and verify the mask correctness\n",
        "6. Save the mask to store\n",
        "\n",
        "The special token mask is a binary mask (1 for special tokens, 0 for regular tokens) that has the same shape as `input_ids`. This is useful for:\n",
        "- Filtering out special tokens during analysis\n",
        "- Understanding tokenization behavior\n",
        "- Creating attention masks that exclude special tokens\n",
        "- Analyzing model behavior on special vs regular tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/SanDiskData/Inzynierka/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Imports completed\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "from amber.hooks import ModelInputDetector\n",
        "from amber.language_model.language_model import LanguageModel\n",
        "from amber.store.local_store import LocalStore\n",
        "\n",
        "print(\"‚úÖ Imports completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Special Token Mask Example\n",
            "üì± Using device: cpu\n",
            "üîß Model: speakleash/Bielik-1.5B-v3.0-Instruct\n",
            "üìù Number of test texts: 3\n",
            "\n",
            "‚úÖ Output directories created\n"
          ]
        }
      ],
      "source": [
        "# MODEL_ID = \"sshleifer/tiny-gpt2\"\n",
        "MODEL_ID = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
        "STORE_DIR = Path(\"store\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "TEST_TEXTS = [\n",
        "    \"Hello world! This is a test.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is fascinating.\",\n",
        "]\n",
        "\n",
        "print(\"üöÄ Starting Special Token Mask Example\")\n",
        "print(f\"üì± Using device: {DEVICE}\")\n",
        "print(f\"üîß Model: {MODEL_ID}\")\n",
        "print(f\"üìù Number of test texts: {len(TEST_TEXTS)}\")\n",
        "print()\n",
        "\n",
        "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"‚úÖ Output directories created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading language model...\n",
            "‚úÖ Model loaded: speakleash_Bielik-1.5B-v3.0-Instruct\n",
            "üì± Device: cpu\n",
            "üìÅ Store location: store\n",
            "\n",
            "üîç Tokenizer special tokens:\n",
            "  pad_token_id: 2\n",
            "  eos_token_id: 4\n",
            "  bos_token_id: 1\n",
            "  unk_token_id: 0\n",
            "  all_special_ids: [1, 4, 0, 2, 3, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "print(\"üì• Loading language model...\")\n",
        "\n",
        "store = LocalStore(STORE_DIR)\n",
        "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
        "lm.model.to(DEVICE)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
        "print(f\"üì± Device: {DEVICE}\")\n",
        "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n",
        "print()\n",
        "\n",
        "tokenizer = lm.tokenizer\n",
        "print(\"üîç Tokenizer special tokens:\")\n",
        "special_token_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id', \n",
        "                       'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
        "special_tokens = {}\n",
        "for attr in special_token_attrs:\n",
        "    token_id = getattr(tokenizer, attr, None)\n",
        "    if token_id is not None:\n",
        "        special_tokens[attr] = token_id\n",
        "        print(f\"  {attr}: {token_id}\")\n",
        "\n",
        "if hasattr(tokenizer, 'all_special_ids'):\n",
        "    print(f\"  all_special_ids: {tokenizer.all_special_ids}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Debug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At the top of the demo we check token_ids in lm.tokenizer\n",
        "```\n",
        "tokenizer = lm.tokenizer\n",
        "...\n",
        "token_id = getattr(tokenizer, attr, None)\n",
        "```\n",
        "\n",
        "Then in ModelInputDetector we pass lm.model\n",
        "\n",
        "```\n",
        "lm.layers.name_to_layer[layer_signature] = lm.model\n",
        "```\n",
        "\n",
        "and then in _get_special_token_ids check for module.tokenizer (does not exist) and module.config (exists but sometimes contains lists as token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pad_token_id\n",
            "2\n",
            "eos_token_id\n",
            "[4, 2]\n",
            "bos_token_id\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "module = lm.model\n",
        "if hasattr(module, 'config'):\n",
        "    config = module.config\n",
        "    token_id_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id',\n",
        "                            'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
        "    for attr in token_id_attrs:\n",
        "        token_id = getattr(config, attr, None)\n",
        "        if token_id is not None:\n",
        "            print(attr)\n",
        "            print(token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "if hasattr(module, 'tokenizer'):\n",
        "    tokenizer = module.tokenizer\n",
        "    token_id_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id', \n",
        "                    'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
        "    for attr in token_id_attrs:\n",
        "        token_id = getattr(tokenizer, attr, None)\n",
        "        if token_id is not None:\n",
        "            print(attr)\n",
        "            print(token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Auto-detect Special Tokens from Model\n",
        "\n",
        "The detector will automatically extract special token IDs from the model's tokenizer or config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating ModelInputDetector with auto-detection of special tokens...\n",
            "üìù Added 'model_inputs_with_mask' to layers registry\n",
            "‚úÖ Detector attached to model\n",
            "üÜî Detector ID: model_input_detector_with_mask\n",
            "üíæ Will save: input_ids, special_token_mask\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß Creating ModelInputDetector with auto-detection of special tokens...\")\n",
        "\n",
        "layer_signature = \"model_inputs_with_mask\"\n",
        "if layer_signature not in lm.layers.name_to_layer:\n",
        "    lm.layers.name_to_layer[layer_signature] = lm.model\n",
        "    print(f\"üìù Added '{layer_signature}' to layers registry\")\n",
        "\n",
        "input_detector = ModelInputDetector(\n",
        "    layer_signature=layer_signature,\n",
        "    hook_id=\"model_input_detector_with_mask\",\n",
        "    save_input_ids=True,\n",
        "    save_attention_mask=False,\n",
        "    save_special_token_mask=True,\n",
        "    special_token_ids=None,\n",
        ")\n",
        "\n",
        "hook_id = lm.layers.register_hook(layer_signature, input_detector)\n",
        "\n",
        "print(f\"‚úÖ Detector attached to model\")\n",
        "print(f\"üÜî Detector ID: {input_detector.id}\")\n",
        "print(f\"üíæ Will save: input_ids, special_token_mask\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Running inference...\n",
            "üìù Processing 3 texts\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error setting inputs from encodings in ModelInputDetector model_input_detector_with_mask: unhashable type: 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/SanDiskData/Inzynierka/src/amber/hooks/implementations/model_input_detector.py:207\u001b[39m, in \u001b[36mModelInputDetector.set_inputs_from_encodings\u001b[39m\u001b[34m(self, encodings, module)\u001b[39m\n\u001b[32m    205\u001b[39m     module = DummyModule()\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m special_token_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_special_token_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m.tensor_metadata[\u001b[33m'\u001b[39m\u001b[33mspecial_token_mask\u001b[39m\u001b[33m'\u001b[39m] = special_token_mask.detach().to(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/SanDiskData/Inzynierka/src/amber/hooks/implementations/model_input_detector.py:160\u001b[39m, in \u001b[36mModelInputDetector._create_special_token_mask\u001b[39m\u001b[34m(self, input_ids, module)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03mCreate a binary mask indicating special token positions.\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    158\u001b[39m \u001b[33;03m    Binary mask tensor with same shape as input_ids (1 for special tokens, 0 otherwise)\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m special_token_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_special_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m special_token_ids:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/SanDiskData/Inzynierka/src/amber/hooks/implementations/model_input_detector.py:140\u001b[39m, in \u001b[36mModelInputDetector._get_special_token_ids\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m             \u001b[43mspecial_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module.tokenizer, \u001b[33m'\u001b[39m\u001b[33mall_special_ids\u001b[39m\u001b[33m'\u001b[39m):\n",
            "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m input_detector.clear_captured()\n\u001b[32m      6\u001b[39m output, encodings = lm.forwards(\n\u001b[32m      7\u001b[39m     TEST_TEXTS,\n\u001b[32m      8\u001b[39m     tok_kwargs={\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m128\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpadding\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33madd_special_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m      9\u001b[39m     autocast=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43minput_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_inputs_from_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Inference completed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Encodings keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encodings.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/SanDiskData/Inzynierka/src/amber/hooks/implementations/model_input_detector.py:211\u001b[39m, in \u001b[36mModelInputDetector.set_inputs_from_encodings\u001b[39m\u001b[34m(self, encodings, module)\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[38;5;28mself\u001b[39m.metadata[\u001b[33m'\u001b[39m\u001b[33mspecial_token_mask_shape\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mtuple\u001b[39m(special_token_mask.shape)\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    212\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError setting inputs from encodings in ModelInputDetector \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mRuntimeError\u001b[39m: Error setting inputs from encodings in ModelInputDetector model_input_detector_with_mask: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "print(\"üöÄ Running inference...\")\n",
        "print(f\"üìù Processing {len(TEST_TEXTS)} texts\")\n",
        "\n",
        "input_detector.clear_captured()\n",
        "\n",
        "output, encodings = lm.forwards(\n",
        "    TEST_TEXTS,\n",
        "    tok_kwargs={\"max_length\": 128, \"padding\": True, \"truncation\": True, \"add_special_tokens\": True},\n",
        "    autocast=False,\n",
        ")\n",
        "\n",
        "input_detector.set_inputs_from_encodings(encodings, module=lm.model)\n",
        "\n",
        "print(\"‚úÖ Inference completed\")\n",
        "print(f\"üìä Encodings keys: {list(encodings.keys())}\")\n",
        "print()\n",
        "print(\"üí° Data captured in detector - ready to inspect\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     1, 10404,\n",
              "            397, 22299, 31964, 22382,  3707,   322,  6291, 31917],\n",
              "         [    1,  2091,  9108, 23156, 31225, 31892,  2228, 31967,   590,  4742,\n",
              "          31896, 17419,  1226,  1347,   395,   303, 31908, 31917],\n",
              "         [    2,     2,     2,     2,     2,     1,   739,  1437,   289,   568,\n",
              "            300,  4957,  3707,  1075, 31896,  5910, 19217, 31917]]),\n",
              " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 18, 32000])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.to_tuple()[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = input_detector.get_captured_input_ids()\n",
        "special_token_mask = input_detector.get_captured_special_token_mask()\n",
        "\n",
        "print(\"üìä Captured Data:\")\n",
        "print(f\"  input_ids shape: {input_ids.shape}\")\n",
        "print(f\"  special_token_mask shape: {special_token_mask.shape}\")\n",
        "print(f\"  special_token_mask dtype: {special_token_mask.dtype}\")\n",
        "print()\n",
        "\n",
        "assert input_ids.shape == special_token_mask.shape, \"Shapes must match!\"\n",
        "print(\"‚úÖ Shapes match!\")\n",
        "print()\n",
        "\n",
        "print(\"üîç Special Token Mask Analysis:\")\n",
        "for i, text in enumerate(TEST_TEXTS):\n",
        "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
        "    print(f\"  input_ids: {input_ids[i].tolist()}\")\n",
        "    print(f\"  mask:      {special_token_mask[i].int().tolist()}\")\n",
        "    \n",
        "    num_special = special_token_mask[i].sum().item()\n",
        "    num_total = len(input_ids[i])\n",
        "    print(f\"  Special tokens: {num_special}/{num_total} ({100*num_special/num_total:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Visualize mask\n",
        "print(\"üìà Visualizing Special Token Mask:\")\n",
        "print()\n",
        "\n",
        "for i, text in enumerate(TEST_TEXTS):\n",
        "    print(f\"Text {i+1}: {text}\")\n",
        "    print(f\"  Token IDs:     {input_ids[i].tolist()}\")\n",
        "    print(f\"  Special Mask:  {' '.join(['‚ñà' if m else '‚ñë' for m in special_token_mask[i].tolist()])}\")\n",
        "    print(f\"  Values:        {' '.join(['1' if m else '0' for m in special_token_mask[i].tolist()])}\")\n",
        "    print()\n",
        "    \n",
        "    # Decode tokens to verify\n",
        "    token_ids_list = input_ids[i].tolist()\n",
        "    mask_list = special_token_mask[i].tolist()\n",
        "    \n",
        "    print(\"  Token breakdown:\")\n",
        "    for j, (token_id, is_special) in enumerate(zip(token_ids_list, mask_list)):\n",
        "        token_str = tokenizer.decode([token_id])\n",
        "        special_marker = \"[SPECIAL]\" if is_special else \"\"\n",
        "        print(f\"    [{j:2d}] ID={token_id:4d} | {token_str:20s} {special_marker}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Provide Custom Special Token IDs\n",
        "\n",
        "You can also explicitly provide special token IDs instead of auto-detecting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Create detector with custom special token IDs\n",
        "print(\"üîß Creating ModelInputDetector with custom special token IDs...\")\n",
        "\n",
        "# Example: Only mark pad_token and eos_token as special\n",
        "custom_special_ids = []\n",
        "if tokenizer.pad_token_id is not None:\n",
        "    custom_special_ids.append(tokenizer.pad_token_id)\n",
        "if tokenizer.eos_token_id is not None:\n",
        "    custom_special_ids.append(tokenizer.eos_token_id)\n",
        "\n",
        "print(f\"  Custom special token IDs: {custom_special_ids}\")\n",
        "\n",
        "# Create new detector with custom IDs\n",
        "layer_signature_custom = \"model_inputs_custom_mask\"\n",
        "if layer_signature_custom not in lm.layers.name_to_layer:\n",
        "    lm.layers.name_to_layer[layer_signature_custom] = lm.model\n",
        "\n",
        "custom_detector = ModelInputDetector(\n",
        "    layer_signature=layer_signature_custom,\n",
        "    hook_id=\"model_input_detector_custom_mask\",\n",
        "    save_input_ids=True,\n",
        "    save_special_token_mask=True,\n",
        "    special_token_ids=custom_special_ids,  # Custom special token IDs\n",
        ")\n",
        "\n",
        "lm.layers.register_hook(layer_signature_custom, custom_detector)\n",
        "print(\"‚úÖ Custom detector created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Test custom detector\n",
        "print(\"üöÄ Testing custom detector...\")\n",
        "\n",
        "custom_detector.clear_captured()\n",
        "\n",
        "output, encodings = lm.forwards(\n",
        "    TEST_TEXTS,\n",
        "    tok_kwargs={\"max_length\": 128, \"padding\": True, \"truncation\": True, \"add_special_tokens\": True},\n",
        "    autocast=False,\n",
        ")\n",
        "\n",
        "custom_detector.set_inputs_from_encodings(encodings, module=lm.model)\n",
        "\n",
        "custom_mask = custom_detector.get_captured_special_token_mask()\n",
        "print(\"‚úÖ Custom mask captured\")\n",
        "print(f\"\\nüìä Comparison:\")\n",
        "# print(f\"  Auto-detected mask sum: {special_token_mask.sum().item()}\")\n",
        "print(f\"  Custom mask sum:        {custom_mask.sum().item()}\")\n",
        "# print(f\"\\n  Auto-detected mask (first text): {special_token_mask[0].int().tolist()}\")\n",
        "print(f\"  Custom mask (first text):         {custom_mask[0].int().tolist()}\")\n",
        "\n",
        "custom_mask[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['‚ñÅThe', '‚ñÅqu', 'ick', '‚ñÅbrow', 'n', '‚ñÅfo', 'x', '‚ñÅju', 'mp', 's', '‚ñÅover', '‚ñÅthe', '‚ñÅla', 'zy', '‚ñÅdo', 'g', '.']\n"
          ]
        }
      ],
      "source": [
        "print(lm.tokenizer.tokenize(\"The quick brown fox jumps over the lazy dog.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Verify mask correctness\n",
        "print(\"‚úÖ Verifying mask correctness...\")\n",
        "print()\n",
        "\n",
        "# Get special token IDs that were used\n",
        "special_ids_used = input_detector._get_special_token_ids(lm.model)\n",
        "print(f\"Special token IDs used: {sorted(special_ids_used)}\")\n",
        "print()\n",
        "\n",
        "# Verify: for each position where mask is True, the token ID should be in special_ids\n",
        "all_correct = True\n",
        "for i in range(input_ids.shape[0]):\n",
        "    for j in range(input_ids.shape[1]):\n",
        "        token_id = input_ids[i, j].item()\n",
        "        is_special = special_token_mask[i, j].item()\n",
        "        \n",
        "        # Check if mask correctly identifies special tokens\n",
        "        should_be_special = token_id in special_ids_used\n",
        "        \n",
        "        if is_special != should_be_special:\n",
        "            print(f\"‚ùå Mismatch at position [{i}, {j}]: token_id={token_id}, \"\n",
        "                  f\"mask={is_special}, should_be_special={should_be_special}\")\n",
        "            all_correct = False\n",
        "\n",
        "if all_correct:\n",
        "    print(\"‚úÖ All mask values are correct!\")\n",
        "    print(f\"   Mask correctly identifies {special_token_mask.sum().item()} special token positions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Save to store (optional)\n",
        "print(\"üíæ Saving to store...\")\n",
        "\n",
        "# Create a run name\n",
        "run_name = \"special_token_mask_demo\"\n",
        "batch_index = 0\n",
        "\n",
        "# Save detector metadata\n",
        "lm.save_detector_metadata(run_name, batch_index)\n",
        "\n",
        "print(f\"‚úÖ Saved to store: {run_name}/batch_{batch_index}\")\n",
        "print(f\"üìÅ Store location: {store.base_path}\")\n",
        "print()\n",
        "print(\"üéâ Example completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
