{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Special Token Mask Detection\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a language model\n",
    "2. Create `ModelInputDetector` with special token mask detection enabled\n",
    "3. Use model's automatic special token detection OR provide custom special token IDs\n",
    "4. Run inference and capture special token masks\n",
    "5. Visualize and verify the mask correctness\n",
    "6. Save the mask to store\n",
    "\n",
    "The special token mask is a binary mask (1 for special tokens, 0 for regular tokens) that has the same shape as `input_ids`. This is useful for:\n",
    "- Filtering out special tokens during analysis\n",
    "- Understanding tokenization behavior\n",
    "- Creating attention masks that exclude special tokens\n",
    "- Analyzing model behavior on special vs regular tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:26:07.185294Z",
     "start_time": "2025-12-08T23:26:07.156541Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/Projects/Inzynierka/codebase/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from amber.hooks import ModelInputDetector\n",
    "from amber.language_model.language_model import LanguageModel\n",
    "from amber.store.local_store import LocalStore\n",
    "\n",
    "print(\"‚úÖ Imports completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:26:07.534296Z",
     "start_time": "2025-12-08T23:26:07.508170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Special Token Mask Example\n",
      "üì± Using device: cpu\n",
      "üîß Model: speakleash/Bielik-1.5B-v3.0-Instruct\n",
      "üìù Number of test texts: 3\n",
      "\n",
      "‚úÖ Output directories created\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"sshleifer/tiny-gpt2\"\n",
    "MODEL_ID = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
    "STORE_DIR = Path(\"store\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "TEST_TEXTS = [\n",
    "    \"Hello world! This is a test.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "]\n",
    "\n",
    "print(\"üöÄ Starting Special Token Mask Example\")\n",
    "print(f\"üì± Using device: {DEVICE}\")\n",
    "print(f\"üîß Model: {MODEL_ID}\")\n",
    "print(f\"üìù Number of test texts: {len(TEST_TEXTS)}\")\n",
    "print()\n",
    "\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"‚úÖ Output directories created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:26:52.955986Z",
     "start_time": "2025-12-08T23:26:08.130578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading language model...\n",
      "‚úÖ Model loaded: speakleash_Bielik-1.5B-v3.0-Instruct\n",
      "üì± Device: cpu\n",
      "üìÅ Store location: store\n",
      "\n",
      "üîç Tokenizer special tokens:\n",
      "  pad_token_id: 2\n",
      "  eos_token_id: 4\n",
      "  bos_token_id: 1\n",
      "  unk_token_id: 0\n",
      "  all_special_ids: [1, 4, 0, 2, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• Loading language model...\")\n",
    "\n",
    "store = LocalStore(STORE_DIR)\n",
    "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
    "lm.model.to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
    "print(f\"üì± Device: {DEVICE}\")\n",
    "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n",
    "print()\n",
    "\n",
    "tokenizer = lm.tokenizer\n",
    "print(\"üîç Tokenizer special tokens:\")\n",
    "special_token_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id', \n",
    "                       'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
    "special_tokens = {}\n",
    "for attr in special_token_attrs:\n",
    "    token_id = getattr(tokenizer, attr, None)\n",
    "    if token_id is not None:\n",
    "        special_tokens[attr] = token_id\n",
    "        print(f\"  {attr}: {token_id}\")\n",
    "\n",
    "if hasattr(tokenizer, 'all_special_ids'):\n",
    "    print(f\"  all_special_ids: {tokenizer.all_special_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the top of the demo we check token_ids in lm.tokenizer\n",
    "```\n",
    "tokenizer = lm.tokenizer\n",
    "...\n",
    "token_id = getattr(tokenizer, attr, None)\n",
    "```\n",
    "\n",
    "Then in ModelInputDetector we pass lm.model\n",
    "\n",
    "```\n",
    "lm.layers.name_to_layer[layer_signature] = lm.model\n",
    "```\n",
    "\n",
    "and then in _get_special_token_ids check for module.tokenizer (does not exist) and module.config (exists but sometimes contains lists as token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id\n",
      "2\n",
      "eos_token_id\n",
      "[4, 2]\n",
      "bos_token_id\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "module = lm.model\n",
    "if hasattr(module, 'config'):\n",
    "    config = module.config\n",
    "    token_id_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id',\n",
    "                            'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
    "    for attr in token_id_attrs:\n",
    "        token_id = getattr(config, attr, None)\n",
    "        if token_id is not None:\n",
    "            print(attr)\n",
    "            print(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(module, 'tokenizer'):\n",
    "    tokenizer = module.tokenizer\n",
    "    token_id_attrs = ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id', \n",
    "                    'cls_token_id', 'sep_token_id', 'mask_token_id']\n",
    "    for attr in token_id_attrs:\n",
    "        token_id = getattr(tokenizer, attr, None)\n",
    "        if token_id is not None:\n",
    "            print(attr)\n",
    "            print(token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Auto-detect Special Tokens from Model\n",
    "\n",
    "The detector will automatically extract special token IDs from the model's tokenizer or config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T23:27:31.306439Z",
     "start_time": "2025-12-08T23:27:31.274167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating ModelInputDetector with auto-detection of special tokens...\n",
      "üìù Added 'model_inputs_with_mask' to layers registry\n",
      "‚úÖ Detector attached to model\n",
      "üÜî Detector ID: model_input_detector_with_mask\n",
      "üíæ Will save: input_ids, special_token_mask\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Creating ModelInputDetector with auto-detection of special tokens...\")\n",
    "\n",
    "layer_signature = \"model_inputs_with_mask\"\n",
    "if layer_signature not in lm.layers.name_to_layer:\n",
    "    lm.layers.name_to_layer[layer_signature] = lm.model\n",
    "    print(f\"üìù Added '{layer_signature}' to layers registry\")\n",
    "\n",
    "input_detector = ModelInputDetector(\n",
    "    layer_signature=layer_signature,\n",
    "    hook_id=\"model_input_detector_with_mask\",\n",
    "    save_input_ids=True,\n",
    "    save_attention_mask=False,\n",
    "    save_special_token_mask=True,\n",
    "    special_token_ids=None,\n",
    ")\n",
    "\n",
    "hook_id = lm.layers.register_hook(layer_signature, input_detector)\n",
    "\n",
    "print(f\"‚úÖ Detector attached to model\")\n",
    "print(f\"üÜî Detector ID: {input_detector.id}\")\n",
    "print(f\"üíæ Will save: input_ids, special_token_mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running inference...\n",
      "üìù Processing 3 texts\n",
      "‚úÖ Inference completed\n",
      "üìä Encodings keys: ['input_ids', 'attention_mask']\n",
      "\n",
      "üí° Data captured in detector - ready to inspect\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Running inference...\")\n",
    "print(f\"üìù Processing {len(TEST_TEXTS)} texts\")\n",
    "\n",
    "input_detector.clear_captured()\n",
    "\n",
    "output, encodings = lm.forwards(\n",
    "    TEST_TEXTS,\n",
    "    tok_kwargs={\"max_length\": 128, \"padding\": True, \"truncation\": True, \"add_special_tokens\": True},\n",
    "    autocast=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Inference completed\")\n",
    "print(f\"üìä Encodings keys: {list(encodings.keys())}\")\n",
    "print()\n",
    "print(\"üí° Data captured in detector - ready to inspect\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     1, 10404,\n",
       "            397, 22299, 31964, 22382,  3707,   322,  6291, 31917],\n",
       "         [    1,  2091,  9108, 23156, 31225, 31892,  2228, 31967,   590,  4742,\n",
       "          31896, 17419,  1226,  1347,   395,   303, 31908, 31917],\n",
       "         [    2,     2,     2,     2,     2,     1,   739,  1437,   289,   568,\n",
       "            300,  4957,  3707,  1075, 31896,  5910, 19217, 31917]]),\n",
       " 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 18, 32000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.to_tuple()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Captured Data:\n",
      "  input_ids shape: torch.Size([3, 18])\n",
      "  special_token_mask shape: torch.Size([3, 18])\n",
      "  special_token_mask dtype: torch.bool\n",
      "\n",
      "‚úÖ Shapes match!\n",
      "\n",
      "üîç Special Token Mask Analysis:\n",
      "\n",
      "Text 1: Hello world! This is a test....\n",
      "  input_ids: [2, 2, 2, 2, 2, 2, 2, 2, 1, 10404, 397, 22299, 31964, 22382, 3707, 322, 6291, 31917]\n",
      "  mask:      [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Special tokens: 9/18 (50.0%)\n",
      "\n",
      "Text 2: The quick brown fox jumps over the lazy dog....\n",
      "  input_ids: [1, 2091, 9108, 23156, 31225, 31892, 2228, 31967, 590, 4742, 31896, 17419, 1226, 1347, 395, 303, 31908, 31917]\n",
      "  mask:      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Special tokens: 1/18 (5.6%)\n",
      "\n",
      "Text 3: Machine learning is fascinating....\n",
      "  input_ids: [2, 2, 2, 2, 2, 1, 739, 1437, 289, 568, 300, 4957, 3707, 1075, 31896, 5910, 19217, 31917]\n",
      "  mask:      [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Special tokens: 6/18 (33.3%)\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_detector.get_captured_input_ids()\n",
    "special_token_mask = input_detector.get_captured_special_token_mask()\n",
    "\n",
    "print(\"üìä Captured Data:\")\n",
    "print(f\"  input_ids shape: {input_ids.shape}\")\n",
    "print(f\"  special_token_mask shape: {special_token_mask.shape}\")\n",
    "print(f\"  special_token_mask dtype: {special_token_mask.dtype}\")\n",
    "print()\n",
    "\n",
    "assert input_ids.shape == special_token_mask.shape, \"Shapes must match!\"\n",
    "print(\"‚úÖ Shapes match!\")\n",
    "print()\n",
    "\n",
    "print(\"üîç Special Token Mask Analysis:\")\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"\\nText {i+1}: {text[:50]}...\")\n",
    "    print(f\"  input_ids: {input_ids[i].tolist()}\")\n",
    "    print(f\"  mask:      {special_token_mask[i].int().tolist()}\")\n",
    "    \n",
    "    num_special = special_token_mask[i].sum().item()\n",
    "    num_total = len(input_ids[i])\n",
    "    print(f\"  Special tokens: {num_special}/{num_total} ({100*num_special/num_total:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Visualizing Special Token Mask:\n",
      "\n",
      "Text 1: Hello world! This is a test.\n",
      "  Token IDs:     [2, 2, 2, 2, 2, 2, 2, 2, 1, 10404, 397, 22299, 31964, 22382, 3707, 322, 6291, 31917]\n",
      "  Special Mask:  ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë\n",
      "  Values:        1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "  Token breakdown:\n",
      "    [ 0] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 1] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 2] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 3] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 4] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 5] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 6] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 7] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 8] ID=   1 | <s>                  [SPECIAL]\n",
      "    [ 9] ID=10404 | Hel                  \n",
      "    [10] ID= 397 | lo                   \n",
      "    [11] ID=22299 | world                \n",
      "    [12] ID=31964 | !                    \n",
      "    [13] ID=22382 | This                 \n",
      "    [14] ID=3707 | is                   \n",
      "    [15] ID= 322 | a                    \n",
      "    [16] ID=6291 | test                 \n",
      "    [17] ID=31917 | .                    \n",
      "\n",
      "Text 2: The quick brown fox jumps over the lazy dog.\n",
      "  Token IDs:     [1, 2091, 9108, 23156, 31225, 31892, 2228, 31967, 590, 4742, 31896, 17419, 1226, 1347, 395, 303, 31908, 31917]\n",
      "  Special Mask:  ‚ñà ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë\n",
      "  Values:        1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "  Token breakdown:\n",
      "    [ 0] ID=   1 | <s>                  [SPECIAL]\n",
      "    [ 1] ID=2091 | The                  \n",
      "    [ 2] ID=9108 | qu                   \n",
      "    [ 3] ID=23156 | ick                  \n",
      "    [ 4] ID=31225 | brow                 \n",
      "    [ 5] ID=31892 | n                    \n",
      "    [ 6] ID=2228 | fo                   \n",
      "    [ 7] ID=31967 | x                    \n",
      "    [ 8] ID= 590 | ju                   \n",
      "    [ 9] ID=4742 | mp                   \n",
      "    [10] ID=31896 | s                    \n",
      "    [11] ID=17419 | over                 \n",
      "    [12] ID=1226 | the                  \n",
      "    [13] ID=1347 | la                   \n",
      "    [14] ID= 395 | zy                   \n",
      "    [15] ID= 303 | do                   \n",
      "    [16] ID=31908 | g                    \n",
      "    [17] ID=31917 | .                    \n",
      "\n",
      "Text 3: Machine learning is fascinating.\n",
      "  Token IDs:     [2, 2, 2, 2, 2, 1, 739, 1437, 289, 568, 300, 4957, 3707, 1075, 31896, 5910, 19217, 31917]\n",
      "  Special Mask:  ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñà ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë ‚ñë\n",
      "  Values:        1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "\n",
      "  Token breakdown:\n",
      "    [ 0] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 1] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 2] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 3] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 4] ID=   2 | </s>                 [SPECIAL]\n",
      "    [ 5] ID=   1 | <s>                  [SPECIAL]\n",
      "    [ 6] ID= 739 | Ma                   \n",
      "    [ 7] ID=1437 | chi                  \n",
      "    [ 8] ID= 289 | ne                   \n",
      "    [ 9] ID= 568 | le                   \n",
      "    [10] ID= 300 | ar                   \n",
      "    [11] ID=4957 | ning                 \n",
      "    [12] ID=3707 | is                   \n",
      "    [13] ID=1075 | fa                   \n",
      "    [14] ID=31896 | s                    \n",
      "    [15] ID=5910 | cina                 \n",
      "    [16] ID=19217 | ting                 \n",
      "    [17] ID=31917 | .                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Visualize mask\n",
    "print(\"üìà Visualizing Special Token Mask:\")\n",
    "print()\n",
    "\n",
    "for i, text in enumerate(TEST_TEXTS):\n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "    print(f\"  Token IDs:     {input_ids[i].tolist()}\")\n",
    "    print(f\"  Special Mask:  {' '.join(['‚ñà' if m else '‚ñë' for m in special_token_mask[i].tolist()])}\")\n",
    "    print(f\"  Values:        {' '.join(['1' if m else '0' for m in special_token_mask[i].tolist()])}\")\n",
    "    print()\n",
    "    \n",
    "    # Decode tokens to verify\n",
    "    token_ids_list = input_ids[i].tolist()\n",
    "    mask_list = special_token_mask[i].tolist()\n",
    "    \n",
    "    print(\"  Token breakdown:\")\n",
    "    for j, (token_id, is_special) in enumerate(zip(token_ids_list, mask_list)):\n",
    "        token_str = tokenizer.decode([token_id])\n",
    "        special_marker = \"[SPECIAL]\" if is_special else \"\"\n",
    "        print(f\"    [{j:2d}] ID={token_id:4d} | {token_str:20s} {special_marker}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Provide Custom Special Token IDs\n",
    "\n",
    "You can also explicitly provide special token IDs instead of auto-detecting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating ModelInputDetector with custom special token IDs...\n",
      "  Custom special token IDs: [2, 4]\n",
      "‚úÖ Custom detector created\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create detector with custom special token IDs\n",
    "print(\"üîß Creating ModelInputDetector with custom special token IDs...\")\n",
    "\n",
    "# Example: Only mark pad_token and eos_token as special\n",
    "custom_special_ids = []\n",
    "if tokenizer.pad_token_id is not None:\n",
    "    custom_special_ids.append(tokenizer.pad_token_id)\n",
    "if tokenizer.eos_token_id is not None:\n",
    "    custom_special_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "print(f\"  Custom special token IDs: {custom_special_ids}\")\n",
    "\n",
    "# Create new detector with custom IDs\n",
    "layer_signature_custom = \"model_inputs_custom_mask\"\n",
    "if layer_signature_custom not in lm.layers.name_to_layer:\n",
    "    lm.layers.name_to_layer[layer_signature_custom] = lm.model\n",
    "\n",
    "custom_detector = ModelInputDetector(\n",
    "    layer_signature=layer_signature_custom,\n",
    "    hook_id=\"model_input_detector_custom_mask\",\n",
    "    save_input_ids=True,\n",
    "    save_special_token_mask=True,\n",
    "    special_token_ids=custom_special_ids,  # Custom special token IDs\n",
    ")\n",
    "\n",
    "lm.layers.register_hook(layer_signature_custom, custom_detector)\n",
    "print(\"‚úÖ Custom detector created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing custom detector...\n",
      "‚úÖ Custom mask captured\n",
      "\n",
      "üìä Comparison:\n",
      "  Custom mask sum:        13\n",
      "  Custom mask (first text):         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Test custom detector\n",
    "print(\"üöÄ Testing custom detector...\")\n",
    "\n",
    "custom_detector.clear_captured()\n",
    "\n",
    "output, encodings = lm.forwards(\n",
    "    TEST_TEXTS,\n",
    "    tok_kwargs={\"max_length\": 128, \"padding\": True, \"truncation\": True, \"add_special_tokens\": True},\n",
    "    autocast=False,\n",
    ")\n",
    "\n",
    "custom_detector.set_inputs_from_encodings(encodings, module=lm.model)\n",
    "\n",
    "custom_mask = custom_detector.get_captured_special_token_mask()\n",
    "print(\"‚úÖ Custom mask captured\")\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "# print(f\"  Auto-detected mask sum: {special_token_mask.sum().item()}\")\n",
    "print(f\"  Custom mask sum:        {custom_mask.sum().item()}\")\n",
    "# print(f\"\\n  Auto-detected mask (first text): {special_token_mask[0].int().tolist()}\")\n",
    "print(f\"  Custom mask (first text):         {custom_mask[0].int().tolist()}\")\n",
    "\n",
    "custom_mask[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅThe', '‚ñÅqu', 'ick', '‚ñÅbrow', 'n', '‚ñÅfo', 'x', '‚ñÅju', 'mp', 's', '‚ñÅover', '‚ñÅthe', '‚ñÅla', 'zy', '‚ñÅdo', 'g', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lm.tokenizer.tokenize(\"The quick brown fox jumps over the lazy dog.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Verifying mask correctness...\n",
      "\n",
      "Special token IDs used: [0, 1, 2, 3, 4, 5, 6]\n",
      "\n",
      "‚úÖ All mask values are correct!\n",
      "   Mask correctly identifies 16 special token positions\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Verify mask correctness\n",
    "print(\"‚úÖ Verifying mask correctness...\")\n",
    "print()\n",
    "\n",
    "# Get special token IDs that were used\n",
    "special_ids_used = input_detector._get_special_token_ids(lm.model)\n",
    "print(f\"Special token IDs used: {sorted(special_ids_used)}\")\n",
    "print()\n",
    "\n",
    "# Verify: for each position where mask is True, the token ID should be in special_ids\n",
    "all_correct = True\n",
    "for i in range(input_ids.shape[0]):\n",
    "    for j in range(input_ids.shape[1]):\n",
    "        token_id = input_ids[i, j].item()\n",
    "        is_special = special_token_mask[i, j].item()\n",
    "        \n",
    "        # Check if mask correctly identifies special tokens\n",
    "        should_be_special = token_id in special_ids_used\n",
    "        \n",
    "        if is_special != should_be_special:\n",
    "            print(f\"‚ùå Mismatch at position [{i}, {j}]: token_id={token_id}, \"\n",
    "                  f\"mask={is_special}, should_be_special={should_be_special}\")\n",
    "            all_correct = False\n",
    "\n",
    "if all_correct:\n",
    "    print(\"‚úÖ All mask values are correct!\")\n",
    "    print(f\"   Mask correctly identifies {special_token_mask.sum().item()} special token positions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to store...\n",
      "‚úÖ Saved to store: special_token_mask_demo/batch_0\n",
      "üìÅ Store location: store\n",
      "\n",
      "üéâ Example completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Save to store (optional)\n",
    "print(\"üíæ Saving to store...\")\n",
    "\n",
    "# Create a run name\n",
    "run_name = \"special_token_mask_demo\"\n",
    "batch_index = 0\n",
    "\n",
    "# Save detector metadata\n",
    "lm.save_detector_metadata(run_name, batch_index)\n",
    "\n",
    "print(f\"‚úÖ Saved to store: {run_name}/batch_{batch_index}\")\n",
    "print(f\"üìÅ Store location: {store.base_path}\")\n",
    "print()\n",
    "print(\"üéâ Example completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
