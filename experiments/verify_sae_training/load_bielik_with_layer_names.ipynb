{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0d9c0d70",
      "metadata": {},
      "source": [
        "# Load Bielik-1.5B-v3.0-Instruct with Layer Names\n",
        "\n",
        "This notebook loads the Bielik-1.5B-v3.0-Instruct model from HuggingFace and displays all available layer names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "070d4f93",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from mi_crow.language_model.language_model import LanguageModel\n",
        "from mi_crow.store.local_store import LocalStore\n",
        "\n",
        "MODEL_ID = \"speakleash/Bielik-1.5B-v3.0-Instruct\"\n",
        "STORE_DIR = Path(\"./store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "49ec64fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Loading Bielik model from HuggingFace...\n",
            "Model: speakleash/Bielik-1.5B-v3.0-Instruct\n",
            "\n",
            "âœ… Model loaded successfully!\n",
            "ðŸ“± Device: cpu\n",
            "ðŸ†” Model ID: speakleash_Bielik-1.5B-v3.0-Instruct\n",
            "ðŸ“ Store location: store\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ“¥ Loading Bielik model from HuggingFace...\")\n",
        "print(f\"Model: {MODEL_ID}\\n\")\n",
        "\n",
        "store = LocalStore(base_path=STORE_DIR)\n",
        "lm = LanguageModel.from_huggingface(MODEL_ID, store=store)\n",
        "\n",
        "print(f\"âœ… Model loaded successfully!\")\n",
        "print(f\"ðŸ“± Device: {lm.context.device}\")\n",
        "print(f\"ðŸ†” Model ID: {lm.model_id}\")\n",
        "print(f\"ðŸ“ Store location: {lm.context.store.base_path}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a1114a25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Found 422 layers in the model\n",
            "\n",
            "================================================================================\n",
            "All Layer Names:\n",
            "================================================================================\n",
            "   0: llamaforcausallm_model\n",
            "   1: llamaforcausallm_model_embed_tokens\n",
            "   2: llamaforcausallm_model_layers\n",
            "   3: llamaforcausallm_model_layers_0\n",
            "   4: llamaforcausallm_model_layers_0_self_attn\n",
            "   5: llamaforcausallm_model_layers_0_self_attn_q_proj\n",
            "   6: llamaforcausallm_model_layers_0_self_attn_k_proj\n",
            "   7: llamaforcausallm_model_layers_0_self_attn_v_proj\n",
            "   8: llamaforcausallm_model_layers_0_self_attn_o_proj\n",
            "   9: llamaforcausallm_model_layers_0_mlp\n",
            "  10: llamaforcausallm_model_layers_0_mlp_gate_proj\n",
            "  11: llamaforcausallm_model_layers_0_mlp_up_proj\n",
            "  12: llamaforcausallm_model_layers_0_mlp_down_proj\n",
            "  13: llamaforcausallm_model_layers_0_mlp_act_fn\n",
            "  14: llamaforcausallm_model_layers_0_input_layernorm\n",
            "  15: llamaforcausallm_model_layers_0_post_attention_layernorm\n",
            "  16: llamaforcausallm_model_layers_1\n",
            "  17: llamaforcausallm_model_layers_1_self_attn\n",
            "  18: llamaforcausallm_model_layers_1_self_attn_q_proj\n",
            "  19: llamaforcausallm_model_layers_1_self_attn_k_proj\n",
            "  20: llamaforcausallm_model_layers_1_self_attn_v_proj\n",
            "  21: llamaforcausallm_model_layers_1_self_attn_o_proj\n",
            "  22: llamaforcausallm_model_layers_1_mlp\n",
            "  23: llamaforcausallm_model_layers_1_mlp_gate_proj\n",
            "  24: llamaforcausallm_model_layers_1_mlp_up_proj\n",
            "  25: llamaforcausallm_model_layers_1_mlp_down_proj\n",
            "  26: llamaforcausallm_model_layers_1_mlp_act_fn\n",
            "  27: llamaforcausallm_model_layers_1_input_layernorm\n",
            "  28: llamaforcausallm_model_layers_1_post_attention_layernorm\n",
            "  29: llamaforcausallm_model_layers_2\n",
            "  30: llamaforcausallm_model_layers_2_self_attn\n",
            "  31: llamaforcausallm_model_layers_2_self_attn_q_proj\n",
            "  32: llamaforcausallm_model_layers_2_self_attn_k_proj\n",
            "  33: llamaforcausallm_model_layers_2_self_attn_v_proj\n",
            "  34: llamaforcausallm_model_layers_2_self_attn_o_proj\n",
            "  35: llamaforcausallm_model_layers_2_mlp\n",
            "  36: llamaforcausallm_model_layers_2_mlp_gate_proj\n",
            "  37: llamaforcausallm_model_layers_2_mlp_up_proj\n",
            "  38: llamaforcausallm_model_layers_2_mlp_down_proj\n",
            "  39: llamaforcausallm_model_layers_2_mlp_act_fn\n",
            "  40: llamaforcausallm_model_layers_2_input_layernorm\n",
            "  41: llamaforcausallm_model_layers_2_post_attention_layernorm\n",
            "  42: llamaforcausallm_model_layers_3\n",
            "  43: llamaforcausallm_model_layers_3_self_attn\n",
            "  44: llamaforcausallm_model_layers_3_self_attn_q_proj\n",
            "  45: llamaforcausallm_model_layers_3_self_attn_k_proj\n",
            "  46: llamaforcausallm_model_layers_3_self_attn_v_proj\n",
            "  47: llamaforcausallm_model_layers_3_self_attn_o_proj\n",
            "  48: llamaforcausallm_model_layers_3_mlp\n",
            "  49: llamaforcausallm_model_layers_3_mlp_gate_proj\n",
            "  50: llamaforcausallm_model_layers_3_mlp_up_proj\n",
            "  51: llamaforcausallm_model_layers_3_mlp_down_proj\n",
            "  52: llamaforcausallm_model_layers_3_mlp_act_fn\n",
            "  53: llamaforcausallm_model_layers_3_input_layernorm\n",
            "  54: llamaforcausallm_model_layers_3_post_attention_layernorm\n",
            "  55: llamaforcausallm_model_layers_4\n",
            "  56: llamaforcausallm_model_layers_4_self_attn\n",
            "  57: llamaforcausallm_model_layers_4_self_attn_q_proj\n",
            "  58: llamaforcausallm_model_layers_4_self_attn_k_proj\n",
            "  59: llamaforcausallm_model_layers_4_self_attn_v_proj\n",
            "  60: llamaforcausallm_model_layers_4_self_attn_o_proj\n",
            "  61: llamaforcausallm_model_layers_4_mlp\n",
            "  62: llamaforcausallm_model_layers_4_mlp_gate_proj\n",
            "  63: llamaforcausallm_model_layers_4_mlp_up_proj\n",
            "  64: llamaforcausallm_model_layers_4_mlp_down_proj\n",
            "  65: llamaforcausallm_model_layers_4_mlp_act_fn\n",
            "  66: llamaforcausallm_model_layers_4_input_layernorm\n",
            "  67: llamaforcausallm_model_layers_4_post_attention_layernorm\n",
            "  68: llamaforcausallm_model_layers_5\n",
            "  69: llamaforcausallm_model_layers_5_self_attn\n",
            "  70: llamaforcausallm_model_layers_5_self_attn_q_proj\n",
            "  71: llamaforcausallm_model_layers_5_self_attn_k_proj\n",
            "  72: llamaforcausallm_model_layers_5_self_attn_v_proj\n",
            "  73: llamaforcausallm_model_layers_5_self_attn_o_proj\n",
            "  74: llamaforcausallm_model_layers_5_mlp\n",
            "  75: llamaforcausallm_model_layers_5_mlp_gate_proj\n",
            "  76: llamaforcausallm_model_layers_5_mlp_up_proj\n",
            "  77: llamaforcausallm_model_layers_5_mlp_down_proj\n",
            "  78: llamaforcausallm_model_layers_5_mlp_act_fn\n",
            "  79: llamaforcausallm_model_layers_5_input_layernorm\n",
            "  80: llamaforcausallm_model_layers_5_post_attention_layernorm\n",
            "  81: llamaforcausallm_model_layers_6\n",
            "  82: llamaforcausallm_model_layers_6_self_attn\n",
            "  83: llamaforcausallm_model_layers_6_self_attn_q_proj\n",
            "  84: llamaforcausallm_model_layers_6_self_attn_k_proj\n",
            "  85: llamaforcausallm_model_layers_6_self_attn_v_proj\n",
            "  86: llamaforcausallm_model_layers_6_self_attn_o_proj\n",
            "  87: llamaforcausallm_model_layers_6_mlp\n",
            "  88: llamaforcausallm_model_layers_6_mlp_gate_proj\n",
            "  89: llamaforcausallm_model_layers_6_mlp_up_proj\n",
            "  90: llamaforcausallm_model_layers_6_mlp_down_proj\n",
            "  91: llamaforcausallm_model_layers_6_mlp_act_fn\n",
            "  92: llamaforcausallm_model_layers_6_input_layernorm\n",
            "  93: llamaforcausallm_model_layers_6_post_attention_layernorm\n",
            "  94: llamaforcausallm_model_layers_7\n",
            "  95: llamaforcausallm_model_layers_7_self_attn\n",
            "  96: llamaforcausallm_model_layers_7_self_attn_q_proj\n",
            "  97: llamaforcausallm_model_layers_7_self_attn_k_proj\n",
            "  98: llamaforcausallm_model_layers_7_self_attn_v_proj\n",
            "  99: llamaforcausallm_model_layers_7_self_attn_o_proj\n",
            " 100: llamaforcausallm_model_layers_7_mlp\n",
            " 101: llamaforcausallm_model_layers_7_mlp_gate_proj\n",
            " 102: llamaforcausallm_model_layers_7_mlp_up_proj\n",
            " 103: llamaforcausallm_model_layers_7_mlp_down_proj\n",
            " 104: llamaforcausallm_model_layers_7_mlp_act_fn\n",
            " 105: llamaforcausallm_model_layers_7_input_layernorm\n",
            " 106: llamaforcausallm_model_layers_7_post_attention_layernorm\n",
            " 107: llamaforcausallm_model_layers_8\n",
            " 108: llamaforcausallm_model_layers_8_self_attn\n",
            " 109: llamaforcausallm_model_layers_8_self_attn_q_proj\n",
            " 110: llamaforcausallm_model_layers_8_self_attn_k_proj\n",
            " 111: llamaforcausallm_model_layers_8_self_attn_v_proj\n",
            " 112: llamaforcausallm_model_layers_8_self_attn_o_proj\n",
            " 113: llamaforcausallm_model_layers_8_mlp\n",
            " 114: llamaforcausallm_model_layers_8_mlp_gate_proj\n",
            " 115: llamaforcausallm_model_layers_8_mlp_up_proj\n",
            " 116: llamaforcausallm_model_layers_8_mlp_down_proj\n",
            " 117: llamaforcausallm_model_layers_8_mlp_act_fn\n",
            " 118: llamaforcausallm_model_layers_8_input_layernorm\n",
            " 119: llamaforcausallm_model_layers_8_post_attention_layernorm\n",
            " 120: llamaforcausallm_model_layers_9\n",
            " 121: llamaforcausallm_model_layers_9_self_attn\n",
            " 122: llamaforcausallm_model_layers_9_self_attn_q_proj\n",
            " 123: llamaforcausallm_model_layers_9_self_attn_k_proj\n",
            " 124: llamaforcausallm_model_layers_9_self_attn_v_proj\n",
            " 125: llamaforcausallm_model_layers_9_self_attn_o_proj\n",
            " 126: llamaforcausallm_model_layers_9_mlp\n",
            " 127: llamaforcausallm_model_layers_9_mlp_gate_proj\n",
            " 128: llamaforcausallm_model_layers_9_mlp_up_proj\n",
            " 129: llamaforcausallm_model_layers_9_mlp_down_proj\n",
            " 130: llamaforcausallm_model_layers_9_mlp_act_fn\n",
            " 131: llamaforcausallm_model_layers_9_input_layernorm\n",
            " 132: llamaforcausallm_model_layers_9_post_attention_layernorm\n",
            " 133: llamaforcausallm_model_layers_10\n",
            " 134: llamaforcausallm_model_layers_10_self_attn\n",
            " 135: llamaforcausallm_model_layers_10_self_attn_q_proj\n",
            " 136: llamaforcausallm_model_layers_10_self_attn_k_proj\n",
            " 137: llamaforcausallm_model_layers_10_self_attn_v_proj\n",
            " 138: llamaforcausallm_model_layers_10_self_attn_o_proj\n",
            " 139: llamaforcausallm_model_layers_10_mlp\n",
            " 140: llamaforcausallm_model_layers_10_mlp_gate_proj\n",
            " 141: llamaforcausallm_model_layers_10_mlp_up_proj\n",
            " 142: llamaforcausallm_model_layers_10_mlp_down_proj\n",
            " 143: llamaforcausallm_model_layers_10_mlp_act_fn\n",
            " 144: llamaforcausallm_model_layers_10_input_layernorm\n",
            " 145: llamaforcausallm_model_layers_10_post_attention_layernorm\n",
            " 146: llamaforcausallm_model_layers_11\n",
            " 147: llamaforcausallm_model_layers_11_self_attn\n",
            " 148: llamaforcausallm_model_layers_11_self_attn_q_proj\n",
            " 149: llamaforcausallm_model_layers_11_self_attn_k_proj\n",
            " 150: llamaforcausallm_model_layers_11_self_attn_v_proj\n",
            " 151: llamaforcausallm_model_layers_11_self_attn_o_proj\n",
            " 152: llamaforcausallm_model_layers_11_mlp\n",
            " 153: llamaforcausallm_model_layers_11_mlp_gate_proj\n",
            " 154: llamaforcausallm_model_layers_11_mlp_up_proj\n",
            " 155: llamaforcausallm_model_layers_11_mlp_down_proj\n",
            " 156: llamaforcausallm_model_layers_11_mlp_act_fn\n",
            " 157: llamaforcausallm_model_layers_11_input_layernorm\n",
            " 158: llamaforcausallm_model_layers_11_post_attention_layernorm\n",
            " 159: llamaforcausallm_model_layers_12\n",
            " 160: llamaforcausallm_model_layers_12_self_attn\n",
            " 161: llamaforcausallm_model_layers_12_self_attn_q_proj\n",
            " 162: llamaforcausallm_model_layers_12_self_attn_k_proj\n",
            " 163: llamaforcausallm_model_layers_12_self_attn_v_proj\n",
            " 164: llamaforcausallm_model_layers_12_self_attn_o_proj\n",
            " 165: llamaforcausallm_model_layers_12_mlp\n",
            " 166: llamaforcausallm_model_layers_12_mlp_gate_proj\n",
            " 167: llamaforcausallm_model_layers_12_mlp_up_proj\n",
            " 168: llamaforcausallm_model_layers_12_mlp_down_proj\n",
            " 169: llamaforcausallm_model_layers_12_mlp_act_fn\n",
            " 170: llamaforcausallm_model_layers_12_input_layernorm\n",
            " 171: llamaforcausallm_model_layers_12_post_attention_layernorm\n",
            " 172: llamaforcausallm_model_layers_13\n",
            " 173: llamaforcausallm_model_layers_13_self_attn\n",
            " 174: llamaforcausallm_model_layers_13_self_attn_q_proj\n",
            " 175: llamaforcausallm_model_layers_13_self_attn_k_proj\n",
            " 176: llamaforcausallm_model_layers_13_self_attn_v_proj\n",
            " 177: llamaforcausallm_model_layers_13_self_attn_o_proj\n",
            " 178: llamaforcausallm_model_layers_13_mlp\n",
            " 179: llamaforcausallm_model_layers_13_mlp_gate_proj\n",
            " 180: llamaforcausallm_model_layers_13_mlp_up_proj\n",
            " 181: llamaforcausallm_model_layers_13_mlp_down_proj\n",
            " 182: llamaforcausallm_model_layers_13_mlp_act_fn\n",
            " 183: llamaforcausallm_model_layers_13_input_layernorm\n",
            " 184: llamaforcausallm_model_layers_13_post_attention_layernorm\n",
            " 185: llamaforcausallm_model_layers_14\n",
            " 186: llamaforcausallm_model_layers_14_self_attn\n",
            " 187: llamaforcausallm_model_layers_14_self_attn_q_proj\n",
            " 188: llamaforcausallm_model_layers_14_self_attn_k_proj\n",
            " 189: llamaforcausallm_model_layers_14_self_attn_v_proj\n",
            " 190: llamaforcausallm_model_layers_14_self_attn_o_proj\n",
            " 191: llamaforcausallm_model_layers_14_mlp\n",
            " 192: llamaforcausallm_model_layers_14_mlp_gate_proj\n",
            " 193: llamaforcausallm_model_layers_14_mlp_up_proj\n",
            " 194: llamaforcausallm_model_layers_14_mlp_down_proj\n",
            " 195: llamaforcausallm_model_layers_14_mlp_act_fn\n",
            " 196: llamaforcausallm_model_layers_14_input_layernorm\n",
            " 197: llamaforcausallm_model_layers_14_post_attention_layernorm\n",
            " 198: llamaforcausallm_model_layers_15\n",
            " 199: llamaforcausallm_model_layers_15_self_attn\n",
            " 200: llamaforcausallm_model_layers_15_self_attn_q_proj\n",
            " 201: llamaforcausallm_model_layers_15_self_attn_k_proj\n",
            " 202: llamaforcausallm_model_layers_15_self_attn_v_proj\n",
            " 203: llamaforcausallm_model_layers_15_self_attn_o_proj\n",
            " 204: llamaforcausallm_model_layers_15_mlp\n",
            " 205: llamaforcausallm_model_layers_15_mlp_gate_proj\n",
            " 206: llamaforcausallm_model_layers_15_mlp_up_proj\n",
            " 207: llamaforcausallm_model_layers_15_mlp_down_proj\n",
            " 208: llamaforcausallm_model_layers_15_mlp_act_fn\n",
            " 209: llamaforcausallm_model_layers_15_input_layernorm\n",
            " 210: llamaforcausallm_model_layers_15_post_attention_layernorm\n",
            " 211: llamaforcausallm_model_layers_16\n",
            " 212: llamaforcausallm_model_layers_16_self_attn\n",
            " 213: llamaforcausallm_model_layers_16_self_attn_q_proj\n",
            " 214: llamaforcausallm_model_layers_16_self_attn_k_proj\n",
            " 215: llamaforcausallm_model_layers_16_self_attn_v_proj\n",
            " 216: llamaforcausallm_model_layers_16_self_attn_o_proj\n",
            " 217: llamaforcausallm_model_layers_16_mlp\n",
            " 218: llamaforcausallm_model_layers_16_mlp_gate_proj\n",
            " 219: llamaforcausallm_model_layers_16_mlp_up_proj\n",
            " 220: llamaforcausallm_model_layers_16_mlp_down_proj\n",
            " 221: llamaforcausallm_model_layers_16_mlp_act_fn\n",
            " 222: llamaforcausallm_model_layers_16_input_layernorm\n",
            " 223: llamaforcausallm_model_layers_16_post_attention_layernorm\n",
            " 224: llamaforcausallm_model_layers_17\n",
            " 225: llamaforcausallm_model_layers_17_self_attn\n",
            " 226: llamaforcausallm_model_layers_17_self_attn_q_proj\n",
            " 227: llamaforcausallm_model_layers_17_self_attn_k_proj\n",
            " 228: llamaforcausallm_model_layers_17_self_attn_v_proj\n",
            " 229: llamaforcausallm_model_layers_17_self_attn_o_proj\n",
            " 230: llamaforcausallm_model_layers_17_mlp\n",
            " 231: llamaforcausallm_model_layers_17_mlp_gate_proj\n",
            " 232: llamaforcausallm_model_layers_17_mlp_up_proj\n",
            " 233: llamaforcausallm_model_layers_17_mlp_down_proj\n",
            " 234: llamaforcausallm_model_layers_17_mlp_act_fn\n",
            " 235: llamaforcausallm_model_layers_17_input_layernorm\n",
            " 236: llamaforcausallm_model_layers_17_post_attention_layernorm\n",
            " 237: llamaforcausallm_model_layers_18\n",
            " 238: llamaforcausallm_model_layers_18_self_attn\n",
            " 239: llamaforcausallm_model_layers_18_self_attn_q_proj\n",
            " 240: llamaforcausallm_model_layers_18_self_attn_k_proj\n",
            " 241: llamaforcausallm_model_layers_18_self_attn_v_proj\n",
            " 242: llamaforcausallm_model_layers_18_self_attn_o_proj\n",
            " 243: llamaforcausallm_model_layers_18_mlp\n",
            " 244: llamaforcausallm_model_layers_18_mlp_gate_proj\n",
            " 245: llamaforcausallm_model_layers_18_mlp_up_proj\n",
            " 246: llamaforcausallm_model_layers_18_mlp_down_proj\n",
            " 247: llamaforcausallm_model_layers_18_mlp_act_fn\n",
            " 248: llamaforcausallm_model_layers_18_input_layernorm\n",
            " 249: llamaforcausallm_model_layers_18_post_attention_layernorm\n",
            " 250: llamaforcausallm_model_layers_19\n",
            " 251: llamaforcausallm_model_layers_19_self_attn\n",
            " 252: llamaforcausallm_model_layers_19_self_attn_q_proj\n",
            " 253: llamaforcausallm_model_layers_19_self_attn_k_proj\n",
            " 254: llamaforcausallm_model_layers_19_self_attn_v_proj\n",
            " 255: llamaforcausallm_model_layers_19_self_attn_o_proj\n",
            " 256: llamaforcausallm_model_layers_19_mlp\n",
            " 257: llamaforcausallm_model_layers_19_mlp_gate_proj\n",
            " 258: llamaforcausallm_model_layers_19_mlp_up_proj\n",
            " 259: llamaforcausallm_model_layers_19_mlp_down_proj\n",
            " 260: llamaforcausallm_model_layers_19_mlp_act_fn\n",
            " 261: llamaforcausallm_model_layers_19_input_layernorm\n",
            " 262: llamaforcausallm_model_layers_19_post_attention_layernorm\n",
            " 263: llamaforcausallm_model_layers_20\n",
            " 264: llamaforcausallm_model_layers_20_self_attn\n",
            " 265: llamaforcausallm_model_layers_20_self_attn_q_proj\n",
            " 266: llamaforcausallm_model_layers_20_self_attn_k_proj\n",
            " 267: llamaforcausallm_model_layers_20_self_attn_v_proj\n",
            " 268: llamaforcausallm_model_layers_20_self_attn_o_proj\n",
            " 269: llamaforcausallm_model_layers_20_mlp\n",
            " 270: llamaforcausallm_model_layers_20_mlp_gate_proj\n",
            " 271: llamaforcausallm_model_layers_20_mlp_up_proj\n",
            " 272: llamaforcausallm_model_layers_20_mlp_down_proj\n",
            " 273: llamaforcausallm_model_layers_20_mlp_act_fn\n",
            " 274: llamaforcausallm_model_layers_20_input_layernorm\n",
            " 275: llamaforcausallm_model_layers_20_post_attention_layernorm\n",
            " 276: llamaforcausallm_model_layers_21\n",
            " 277: llamaforcausallm_model_layers_21_self_attn\n",
            " 278: llamaforcausallm_model_layers_21_self_attn_q_proj\n",
            " 279: llamaforcausallm_model_layers_21_self_attn_k_proj\n",
            " 280: llamaforcausallm_model_layers_21_self_attn_v_proj\n",
            " 281: llamaforcausallm_model_layers_21_self_attn_o_proj\n",
            " 282: llamaforcausallm_model_layers_21_mlp\n",
            " 283: llamaforcausallm_model_layers_21_mlp_gate_proj\n",
            " 284: llamaforcausallm_model_layers_21_mlp_up_proj\n",
            " 285: llamaforcausallm_model_layers_21_mlp_down_proj\n",
            " 286: llamaforcausallm_model_layers_21_mlp_act_fn\n",
            " 287: llamaforcausallm_model_layers_21_input_layernorm\n",
            " 288: llamaforcausallm_model_layers_21_post_attention_layernorm\n",
            " 289: llamaforcausallm_model_layers_22\n",
            " 290: llamaforcausallm_model_layers_22_self_attn\n",
            " 291: llamaforcausallm_model_layers_22_self_attn_q_proj\n",
            " 292: llamaforcausallm_model_layers_22_self_attn_k_proj\n",
            " 293: llamaforcausallm_model_layers_22_self_attn_v_proj\n",
            " 294: llamaforcausallm_model_layers_22_self_attn_o_proj\n",
            " 295: llamaforcausallm_model_layers_22_mlp\n",
            " 296: llamaforcausallm_model_layers_22_mlp_gate_proj\n",
            " 297: llamaforcausallm_model_layers_22_mlp_up_proj\n",
            " 298: llamaforcausallm_model_layers_22_mlp_down_proj\n",
            " 299: llamaforcausallm_model_layers_22_mlp_act_fn\n",
            " 300: llamaforcausallm_model_layers_22_input_layernorm\n",
            " 301: llamaforcausallm_model_layers_22_post_attention_layernorm\n",
            " 302: llamaforcausallm_model_layers_23\n",
            " 303: llamaforcausallm_model_layers_23_self_attn\n",
            " 304: llamaforcausallm_model_layers_23_self_attn_q_proj\n",
            " 305: llamaforcausallm_model_layers_23_self_attn_k_proj\n",
            " 306: llamaforcausallm_model_layers_23_self_attn_v_proj\n",
            " 307: llamaforcausallm_model_layers_23_self_attn_o_proj\n",
            " 308: llamaforcausallm_model_layers_23_mlp\n",
            " 309: llamaforcausallm_model_layers_23_mlp_gate_proj\n",
            " 310: llamaforcausallm_model_layers_23_mlp_up_proj\n",
            " 311: llamaforcausallm_model_layers_23_mlp_down_proj\n",
            " 312: llamaforcausallm_model_layers_23_mlp_act_fn\n",
            " 313: llamaforcausallm_model_layers_23_input_layernorm\n",
            " 314: llamaforcausallm_model_layers_23_post_attention_layernorm\n",
            " 315: llamaforcausallm_model_layers_24\n",
            " 316: llamaforcausallm_model_layers_24_self_attn\n",
            " 317: llamaforcausallm_model_layers_24_self_attn_q_proj\n",
            " 318: llamaforcausallm_model_layers_24_self_attn_k_proj\n",
            " 319: llamaforcausallm_model_layers_24_self_attn_v_proj\n",
            " 320: llamaforcausallm_model_layers_24_self_attn_o_proj\n",
            " 321: llamaforcausallm_model_layers_24_mlp\n",
            " 322: llamaforcausallm_model_layers_24_mlp_gate_proj\n",
            " 323: llamaforcausallm_model_layers_24_mlp_up_proj\n",
            " 324: llamaforcausallm_model_layers_24_mlp_down_proj\n",
            " 325: llamaforcausallm_model_layers_24_mlp_act_fn\n",
            " 326: llamaforcausallm_model_layers_24_input_layernorm\n",
            " 327: llamaforcausallm_model_layers_24_post_attention_layernorm\n",
            " 328: llamaforcausallm_model_layers_25\n",
            " 329: llamaforcausallm_model_layers_25_self_attn\n",
            " 330: llamaforcausallm_model_layers_25_self_attn_q_proj\n",
            " 331: llamaforcausallm_model_layers_25_self_attn_k_proj\n",
            " 332: llamaforcausallm_model_layers_25_self_attn_v_proj\n",
            " 333: llamaforcausallm_model_layers_25_self_attn_o_proj\n",
            " 334: llamaforcausallm_model_layers_25_mlp\n",
            " 335: llamaforcausallm_model_layers_25_mlp_gate_proj\n",
            " 336: llamaforcausallm_model_layers_25_mlp_up_proj\n",
            " 337: llamaforcausallm_model_layers_25_mlp_down_proj\n",
            " 338: llamaforcausallm_model_layers_25_mlp_act_fn\n",
            " 339: llamaforcausallm_model_layers_25_input_layernorm\n",
            " 340: llamaforcausallm_model_layers_25_post_attention_layernorm\n",
            " 341: llamaforcausallm_model_layers_26\n",
            " 342: llamaforcausallm_model_layers_26_self_attn\n",
            " 343: llamaforcausallm_model_layers_26_self_attn_q_proj\n",
            " 344: llamaforcausallm_model_layers_26_self_attn_k_proj\n",
            " 345: llamaforcausallm_model_layers_26_self_attn_v_proj\n",
            " 346: llamaforcausallm_model_layers_26_self_attn_o_proj\n",
            " 347: llamaforcausallm_model_layers_26_mlp\n",
            " 348: llamaforcausallm_model_layers_26_mlp_gate_proj\n",
            " 349: llamaforcausallm_model_layers_26_mlp_up_proj\n",
            " 350: llamaforcausallm_model_layers_26_mlp_down_proj\n",
            " 351: llamaforcausallm_model_layers_26_mlp_act_fn\n",
            " 352: llamaforcausallm_model_layers_26_input_layernorm\n",
            " 353: llamaforcausallm_model_layers_26_post_attention_layernorm\n",
            " 354: llamaforcausallm_model_layers_27\n",
            " 355: llamaforcausallm_model_layers_27_self_attn\n",
            " 356: llamaforcausallm_model_layers_27_self_attn_q_proj\n",
            " 357: llamaforcausallm_model_layers_27_self_attn_k_proj\n",
            " 358: llamaforcausallm_model_layers_27_self_attn_v_proj\n",
            " 359: llamaforcausallm_model_layers_27_self_attn_o_proj\n",
            " 360: llamaforcausallm_model_layers_27_mlp\n",
            " 361: llamaforcausallm_model_layers_27_mlp_gate_proj\n",
            " 362: llamaforcausallm_model_layers_27_mlp_up_proj\n",
            " 363: llamaforcausallm_model_layers_27_mlp_down_proj\n",
            " 364: llamaforcausallm_model_layers_27_mlp_act_fn\n",
            " 365: llamaforcausallm_model_layers_27_input_layernorm\n",
            " 366: llamaforcausallm_model_layers_27_post_attention_layernorm\n",
            " 367: llamaforcausallm_model_layers_28\n",
            " 368: llamaforcausallm_model_layers_28_self_attn\n",
            " 369: llamaforcausallm_model_layers_28_self_attn_q_proj\n",
            " 370: llamaforcausallm_model_layers_28_self_attn_k_proj\n",
            " 371: llamaforcausallm_model_layers_28_self_attn_v_proj\n",
            " 372: llamaforcausallm_model_layers_28_self_attn_o_proj\n",
            " 373: llamaforcausallm_model_layers_28_mlp\n",
            " 374: llamaforcausallm_model_layers_28_mlp_gate_proj\n",
            " 375: llamaforcausallm_model_layers_28_mlp_up_proj\n",
            " 376: llamaforcausallm_model_layers_28_mlp_down_proj\n",
            " 377: llamaforcausallm_model_layers_28_mlp_act_fn\n",
            " 378: llamaforcausallm_model_layers_28_input_layernorm\n",
            " 379: llamaforcausallm_model_layers_28_post_attention_layernorm\n",
            " 380: llamaforcausallm_model_layers_29\n",
            " 381: llamaforcausallm_model_layers_29_self_attn\n",
            " 382: llamaforcausallm_model_layers_29_self_attn_q_proj\n",
            " 383: llamaforcausallm_model_layers_29_self_attn_k_proj\n",
            " 384: llamaforcausallm_model_layers_29_self_attn_v_proj\n",
            " 385: llamaforcausallm_model_layers_29_self_attn_o_proj\n",
            " 386: llamaforcausallm_model_layers_29_mlp\n",
            " 387: llamaforcausallm_model_layers_29_mlp_gate_proj\n",
            " 388: llamaforcausallm_model_layers_29_mlp_up_proj\n",
            " 389: llamaforcausallm_model_layers_29_mlp_down_proj\n",
            " 390: llamaforcausallm_model_layers_29_mlp_act_fn\n",
            " 391: llamaforcausallm_model_layers_29_input_layernorm\n",
            " 392: llamaforcausallm_model_layers_29_post_attention_layernorm\n",
            " 393: llamaforcausallm_model_layers_30\n",
            " 394: llamaforcausallm_model_layers_30_self_attn\n",
            " 395: llamaforcausallm_model_layers_30_self_attn_q_proj\n",
            " 396: llamaforcausallm_model_layers_30_self_attn_k_proj\n",
            " 397: llamaforcausallm_model_layers_30_self_attn_v_proj\n",
            " 398: llamaforcausallm_model_layers_30_self_attn_o_proj\n",
            " 399: llamaforcausallm_model_layers_30_mlp\n",
            " 400: llamaforcausallm_model_layers_30_mlp_gate_proj\n",
            " 401: llamaforcausallm_model_layers_30_mlp_up_proj\n",
            " 402: llamaforcausallm_model_layers_30_mlp_down_proj\n",
            " 403: llamaforcausallm_model_layers_30_mlp_act_fn\n",
            " 404: llamaforcausallm_model_layers_30_input_layernorm\n",
            " 405: llamaforcausallm_model_layers_30_post_attention_layernorm\n",
            " 406: llamaforcausallm_model_layers_31\n",
            " 407: llamaforcausallm_model_layers_31_self_attn\n",
            " 408: llamaforcausallm_model_layers_31_self_attn_q_proj\n",
            " 409: llamaforcausallm_model_layers_31_self_attn_k_proj\n",
            " 410: llamaforcausallm_model_layers_31_self_attn_v_proj\n",
            " 411: llamaforcausallm_model_layers_31_self_attn_o_proj\n",
            " 412: llamaforcausallm_model_layers_31_mlp\n",
            " 413: llamaforcausallm_model_layers_31_mlp_gate_proj\n",
            " 414: llamaforcausallm_model_layers_31_mlp_up_proj\n",
            " 415: llamaforcausallm_model_layers_31_mlp_down_proj\n",
            " 416: llamaforcausallm_model_layers_31_mlp_act_fn\n",
            " 417: llamaforcausallm_model_layers_31_input_layernorm\n",
            " 418: llamaforcausallm_model_layers_31_post_attention_layernorm\n",
            " 419: llamaforcausallm_model_norm\n",
            " 420: llamaforcausallm_model_rotary_emb\n",
            " 421: llamaforcausallm_lm_head\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "layer_names = lm.layers.get_layer_names()\n",
        "print(f\"ðŸ” Found {len(layer_names)} layers in the model\\n\")\n",
        "print(\"=\" * 80)\n",
        "print(\"All Layer Names:\")\n",
        "print(\"=\" * 80)\n",
        "for i, name in enumerate(layer_names):\n",
        "    print(f\"{i:4d}: {name}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c6da39bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ“Š Layer Information Summary:\n",
            "   Total layers: 422\n",
            "   First layer: llamaforcausallm_model\n",
            "   Last layer: llamaforcausallm_lm_head\n",
            "\n",
            "ðŸŽ¯ Found 417 transformer-related layers:\n",
            "   - llamaforcausallm_model_layers\n",
            "   - llamaforcausallm_model_layers_0\n",
            "   - llamaforcausallm_model_layers_0_self_attn\n",
            "   - llamaforcausallm_model_layers_0_self_attn_q_proj\n",
            "   - llamaforcausallm_model_layers_0_self_attn_k_proj\n",
            "   - llamaforcausallm_model_layers_0_self_attn_v_proj\n",
            "   - llamaforcausallm_model_layers_0_self_attn_o_proj\n",
            "   - llamaforcausallm_model_layers_0_mlp\n",
            "   - llamaforcausallm_model_layers_0_mlp_gate_proj\n",
            "   - llamaforcausallm_model_layers_0_mlp_up_proj\n",
            "   ... and 407 more\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nðŸ“Š Layer Information Summary:\")\n",
        "print(f\"   Total layers: {len(layer_names)}\")\n",
        "print(f\"   First layer: {layer_names[0] if layer_names else 'N/A'}\")\n",
        "print(f\"   Last layer: {layer_names[-1] if layer_names else 'N/A'}\")\n",
        "\n",
        "transformer_layers = [name for name in layer_names if 'transformer' in name.lower() or 'layer' in name.lower() or 'h_' in name.lower()]\n",
        "if transformer_layers:\n",
        "    print(f\"\\nðŸŽ¯ Found {len(transformer_layers)} transformer-related layers:\")\n",
        "    for layer in transformer_layers[:10]:\n",
        "        print(f\"   - {layer}\")\n",
        "    if len(transformer_layers) > 10:\n",
        "        print(f\"   ... and {len(transformer_layers) - 10} more\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "01f0e28f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llamaforcausallm_model: No weight\n",
            "llamaforcausallm_model_embed_tokens: torch.Size([32000, 1536])\n",
            "llamaforcausallm_model_layers: No weight\n",
            "llamaforcausallm_model_layers_0: No weight\n",
            "llamaforcausallm_model_layers_0_self_attn: No weight\n",
            "llamaforcausallm_model_layers_0_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_0_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_0_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_0_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_0_mlp: No weight\n",
            "llamaforcausallm_model_layers_0_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_0_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_0_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_0_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_0_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_0_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_1: No weight\n",
            "llamaforcausallm_model_layers_1_self_attn: No weight\n",
            "llamaforcausallm_model_layers_1_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_1_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_1_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_1_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_1_mlp: No weight\n",
            "llamaforcausallm_model_layers_1_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_1_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_1_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_1_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_1_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_1_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_2: No weight\n",
            "llamaforcausallm_model_layers_2_self_attn: No weight\n",
            "llamaforcausallm_model_layers_2_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_2_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_2_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_2_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_2_mlp: No weight\n",
            "llamaforcausallm_model_layers_2_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_2_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_2_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_2_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_2_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_2_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_3: No weight\n",
            "llamaforcausallm_model_layers_3_self_attn: No weight\n",
            "llamaforcausallm_model_layers_3_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_3_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_3_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_3_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_3_mlp: No weight\n",
            "llamaforcausallm_model_layers_3_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_3_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_3_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_3_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_3_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_3_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_4: No weight\n",
            "llamaforcausallm_model_layers_4_self_attn: No weight\n",
            "llamaforcausallm_model_layers_4_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_4_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_4_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_4_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_4_mlp: No weight\n",
            "llamaforcausallm_model_layers_4_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_4_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_4_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_4_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_4_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_4_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_5: No weight\n",
            "llamaforcausallm_model_layers_5_self_attn: No weight\n",
            "llamaforcausallm_model_layers_5_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_5_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_5_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_5_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_5_mlp: No weight\n",
            "llamaforcausallm_model_layers_5_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_5_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_5_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_5_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_5_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_5_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_6: No weight\n",
            "llamaforcausallm_model_layers_6_self_attn: No weight\n",
            "llamaforcausallm_model_layers_6_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_6_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_6_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_6_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_6_mlp: No weight\n",
            "llamaforcausallm_model_layers_6_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_6_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_6_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_6_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_6_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_6_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_7: No weight\n",
            "llamaforcausallm_model_layers_7_self_attn: No weight\n",
            "llamaforcausallm_model_layers_7_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_7_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_7_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_7_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_7_mlp: No weight\n",
            "llamaforcausallm_model_layers_7_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_7_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_7_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_7_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_7_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_7_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_8: No weight\n",
            "llamaforcausallm_model_layers_8_self_attn: No weight\n",
            "llamaforcausallm_model_layers_8_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_8_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_8_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_8_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_8_mlp: No weight\n",
            "llamaforcausallm_model_layers_8_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_8_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_8_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_8_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_8_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_8_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_9: No weight\n",
            "llamaforcausallm_model_layers_9_self_attn: No weight\n",
            "llamaforcausallm_model_layers_9_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_9_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_9_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_9_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_9_mlp: No weight\n",
            "llamaforcausallm_model_layers_9_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_9_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_9_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_9_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_9_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_9_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_10: No weight\n",
            "llamaforcausallm_model_layers_10_self_attn: No weight\n",
            "llamaforcausallm_model_layers_10_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_10_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_10_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_10_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_10_mlp: No weight\n",
            "llamaforcausallm_model_layers_10_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_10_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_10_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_10_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_10_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_10_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_11: No weight\n",
            "llamaforcausallm_model_layers_11_self_attn: No weight\n",
            "llamaforcausallm_model_layers_11_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_11_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_11_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_11_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_11_mlp: No weight\n",
            "llamaforcausallm_model_layers_11_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_11_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_11_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_11_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_11_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_11_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_12: No weight\n",
            "llamaforcausallm_model_layers_12_self_attn: No weight\n",
            "llamaforcausallm_model_layers_12_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_12_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_12_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_12_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_12_mlp: No weight\n",
            "llamaforcausallm_model_layers_12_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_12_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_12_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_12_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_12_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_12_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_13: No weight\n",
            "llamaforcausallm_model_layers_13_self_attn: No weight\n",
            "llamaforcausallm_model_layers_13_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_13_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_13_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_13_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_13_mlp: No weight\n",
            "llamaforcausallm_model_layers_13_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_13_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_13_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_13_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_13_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_13_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_14: No weight\n",
            "llamaforcausallm_model_layers_14_self_attn: No weight\n",
            "llamaforcausallm_model_layers_14_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_14_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_14_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_14_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_14_mlp: No weight\n",
            "llamaforcausallm_model_layers_14_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_14_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_14_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_14_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_14_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_14_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_15: No weight\n",
            "llamaforcausallm_model_layers_15_self_attn: No weight\n",
            "llamaforcausallm_model_layers_15_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_15_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_15_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_15_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_15_mlp: No weight\n",
            "llamaforcausallm_model_layers_15_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_15_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_15_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_15_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_15_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_15_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_16: No weight\n",
            "llamaforcausallm_model_layers_16_self_attn: No weight\n",
            "llamaforcausallm_model_layers_16_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_16_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_16_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_16_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_16_mlp: No weight\n",
            "llamaforcausallm_model_layers_16_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_16_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_16_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_16_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_16_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_16_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_17: No weight\n",
            "llamaforcausallm_model_layers_17_self_attn: No weight\n",
            "llamaforcausallm_model_layers_17_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_17_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_17_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_17_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_17_mlp: No weight\n",
            "llamaforcausallm_model_layers_17_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_17_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_17_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_17_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_17_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_17_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_18: No weight\n",
            "llamaforcausallm_model_layers_18_self_attn: No weight\n",
            "llamaforcausallm_model_layers_18_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_18_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_18_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_18_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_18_mlp: No weight\n",
            "llamaforcausallm_model_layers_18_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_18_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_18_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_18_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_18_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_18_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_19: No weight\n",
            "llamaforcausallm_model_layers_19_self_attn: No weight\n",
            "llamaforcausallm_model_layers_19_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_19_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_19_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_19_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_19_mlp: No weight\n",
            "llamaforcausallm_model_layers_19_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_19_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_19_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_19_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_19_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_19_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_20: No weight\n",
            "llamaforcausallm_model_layers_20_self_attn: No weight\n",
            "llamaforcausallm_model_layers_20_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_20_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_20_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_20_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_20_mlp: No weight\n",
            "llamaforcausallm_model_layers_20_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_20_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_20_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_20_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_20_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_20_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_21: No weight\n",
            "llamaforcausallm_model_layers_21_self_attn: No weight\n",
            "llamaforcausallm_model_layers_21_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_21_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_21_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_21_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_21_mlp: No weight\n",
            "llamaforcausallm_model_layers_21_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_21_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_21_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_21_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_21_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_21_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_22: No weight\n",
            "llamaforcausallm_model_layers_22_self_attn: No weight\n",
            "llamaforcausallm_model_layers_22_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_22_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_22_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_22_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_22_mlp: No weight\n",
            "llamaforcausallm_model_layers_22_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_22_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_22_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_22_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_22_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_22_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_23: No weight\n",
            "llamaforcausallm_model_layers_23_self_attn: No weight\n",
            "llamaforcausallm_model_layers_23_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_23_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_23_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_23_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_23_mlp: No weight\n",
            "llamaforcausallm_model_layers_23_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_23_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_23_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_23_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_23_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_23_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_24: No weight\n",
            "llamaforcausallm_model_layers_24_self_attn: No weight\n",
            "llamaforcausallm_model_layers_24_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_24_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_24_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_24_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_24_mlp: No weight\n",
            "llamaforcausallm_model_layers_24_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_24_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_24_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_24_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_24_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_24_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_25: No weight\n",
            "llamaforcausallm_model_layers_25_self_attn: No weight\n",
            "llamaforcausallm_model_layers_25_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_25_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_25_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_25_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_25_mlp: No weight\n",
            "llamaforcausallm_model_layers_25_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_25_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_25_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_25_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_25_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_25_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_26: No weight\n",
            "llamaforcausallm_model_layers_26_self_attn: No weight\n",
            "llamaforcausallm_model_layers_26_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_26_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_26_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_26_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_26_mlp: No weight\n",
            "llamaforcausallm_model_layers_26_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_26_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_26_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_26_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_26_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_26_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_27: No weight\n",
            "llamaforcausallm_model_layers_27_self_attn: No weight\n",
            "llamaforcausallm_model_layers_27_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_27_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_27_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_27_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_27_mlp: No weight\n",
            "llamaforcausallm_model_layers_27_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_27_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_27_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_27_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_27_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_27_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_28: No weight\n",
            "llamaforcausallm_model_layers_28_self_attn: No weight\n",
            "llamaforcausallm_model_layers_28_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_28_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_28_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_28_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_28_mlp: No weight\n",
            "llamaforcausallm_model_layers_28_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_28_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_28_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_28_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_28_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_28_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_29: No weight\n",
            "llamaforcausallm_model_layers_29_self_attn: No weight\n",
            "llamaforcausallm_model_layers_29_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_29_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_29_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_29_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_29_mlp: No weight\n",
            "llamaforcausallm_model_layers_29_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_29_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_29_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_29_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_29_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_29_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_30: No weight\n",
            "llamaforcausallm_model_layers_30_self_attn: No weight\n",
            "llamaforcausallm_model_layers_30_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_30_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_30_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_30_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_30_mlp: No weight\n",
            "llamaforcausallm_model_layers_30_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_30_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_30_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_30_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_30_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_30_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_31: No weight\n",
            "llamaforcausallm_model_layers_31_self_attn: No weight\n",
            "llamaforcausallm_model_layers_31_self_attn_q_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_31_self_attn_k_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_31_self_attn_v_proj: torch.Size([256, 1536])\n",
            "llamaforcausallm_model_layers_31_self_attn_o_proj: torch.Size([1536, 1536])\n",
            "llamaforcausallm_model_layers_31_mlp: No weight\n",
            "llamaforcausallm_model_layers_31_mlp_gate_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_31_mlp_up_proj: torch.Size([8960, 1536])\n",
            "llamaforcausallm_model_layers_31_mlp_down_proj: torch.Size([1536, 8960])\n",
            "llamaforcausallm_model_layers_31_mlp_act_fn: No weight\n",
            "llamaforcausallm_model_layers_31_input_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_layers_31_post_attention_layernorm: torch.Size([1536])\n",
            "llamaforcausallm_model_norm: torch.Size([1536])\n",
            "llamaforcausallm_model_rotary_emb: No weight\n",
            "llamaforcausallm_lm_head: torch.Size([32000, 1536])\n"
          ]
        }
      ],
      "source": [
        "lm.layers.print_layer_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "23484b6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ’¾ Layer names saved to variable 'layer_names'\n",
            "   Access with: layer_names\n",
            "   Example: layer_names[0] = 'llamaforcausallm_model'\n",
            "\n",
            "âœ… Model ready for use!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nðŸ’¾ Layer names saved to variable 'layer_names'\")\n",
        "print(f\"   Access with: layer_names\")\n",
        "print(f\"   Example: layer_names[0] = '{layer_names[0] if layer_names else 'N/A'}'\")\n",
        "print(f\"\\nâœ… Model ready for use!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c9a3304",
      "metadata": {},
      "source": [
        "## Filter: Post-Attention LayerNorm Layers\n",
        "\n",
        "Filter and display only the `_post_attention_layernorm` layers (resid_mid - residual stream after attention, before MLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ac1cb0ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ Found 32 post-attention layernorm layers:\n",
            "\n",
            "================================================================================\n",
            "  0: llamaforcausallm_model_layers_0_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  1: llamaforcausallm_model_layers_1_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  2: llamaforcausallm_model_layers_2_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  3: llamaforcausallm_model_layers_3_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  4: llamaforcausallm_model_layers_4_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  5: llamaforcausallm_model_layers_5_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  6: llamaforcausallm_model_layers_6_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  7: llamaforcausallm_model_layers_7_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  8: llamaforcausallm_model_layers_8_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "  9: llamaforcausallm_model_layers_9_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 10: llamaforcausallm_model_layers_10_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 11: llamaforcausallm_model_layers_11_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 12: llamaforcausallm_model_layers_12_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 13: llamaforcausallm_model_layers_13_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 14: llamaforcausallm_model_layers_14_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 15: llamaforcausallm_model_layers_15_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 16: llamaforcausallm_model_layers_16_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 17: llamaforcausallm_model_layers_17_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 18: llamaforcausallm_model_layers_18_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 19: llamaforcausallm_model_layers_19_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 20: llamaforcausallm_model_layers_20_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 21: llamaforcausallm_model_layers_21_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 22: llamaforcausallm_model_layers_22_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 23: llamaforcausallm_model_layers_23_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 24: llamaforcausallm_model_layers_24_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 25: llamaforcausallm_model_layers_25_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 26: llamaforcausallm_model_layers_26_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 27: llamaforcausallm_model_layers_27_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 28: llamaforcausallm_model_layers_28_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 29: llamaforcausallm_model_layers_29_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 30: llamaforcausallm_model_layers_30_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            " 31: llamaforcausallm_model_layers_31_post_attention_layernorm\n",
            "     Weight shape: torch.Size([1536])\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "post_attention_layers = [name for name in layer_names if \"_post_attention_layernorm\" in name]\n",
        "\n",
        "print(f\"ðŸŽ¯ Found {len(post_attention_layers)} post-attention layernorm layers:\\n\")\n",
        "print(\"=\" * 80)\n",
        "for i, name in enumerate(post_attention_layers):\n",
        "    layer = lm.layers.name_to_layer[name]\n",
        "    weight_shape = getattr(layer, 'weight', None)\n",
        "    weight_info = weight_shape.shape if weight_shape is not None else 'No weight'\n",
        "    print(f\"{i:3d}: {name}\")\n",
        "    print(f\"     Weight shape: {weight_info}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "af26a7a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ’¾ Post-attention layernorm layer names saved to variable 'post_attention_layers'\n",
            "   Total count: 32\n",
            "   Example: post_attention_layers[0] = 'llamaforcausallm_model_layers_0_post_attention_layernorm'\n",
            "   Example: post_attention_layers[-1] = 'llamaforcausallm_model_layers_31_post_attention_layernorm'\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nðŸ’¾ Post-attention layernorm layer names saved to variable 'post_attention_layers'\")\n",
        "print(f\"   Total count: {len(post_attention_layers)}\")\n",
        "print(f\"   Example: post_attention_layers[0] = '{post_attention_layers[0] if post_attention_layers else 'N/A'}'\")\n",
        "print(f\"   Example: post_attention_layers[-1] = '{post_attention_layers[-1] if post_attention_layers else 'N/A'}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db8d02f",
      "metadata": {},
      "source": [
        "## Layer Selection Recommendations (Based on Research)\n",
        "\n",
        "Based on recent SAE research (LLaMA-Scope, Kissane et al., Olson et al.):\n",
        "\n",
        "**Key Findings:**\n",
        "- **Residual stream (post-MLP) is optimal** - highest explained variance, lowest Î”LM loss\n",
        "- **Mid-to-upper layers (15-24) are best** - balance expressivity and abstraction\n",
        "- **Avoid very early layers (0-6)** - too little structure\n",
        "- **Avoid very late layers (30-31)** - layer 31 showed anomalous behavior\n",
        "- **Validated layers**: 7, 15, 23 showed robust performance\n",
        "\n",
        "**For Bielik-1.5B (32 layers):**\n",
        "- **Recommended range**: Layers 15-24 (mid-to-upper third)\n",
        "- **Also validated**: Layers 7, 15, 23\n",
        "- **Avoid**: Very early (0-6) and very late (30-31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9ad1d0d2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Layer Analysis for Bielik-1.5B (32 layers total)\n",
            "\n",
            "================================================================================\n",
            "âœ… RECOMMENDED (Mid-to-upper layers, 15-24):\n",
            "   Layer 15: llamaforcausallm_model_layers_15_post_attention_layernorm\n",
            "   Layer 16: llamaforcausallm_model_layers_16_post_attention_layernorm\n",
            "   Layer 17: llamaforcausallm_model_layers_17_post_attention_layernorm\n",
            "   Layer 18: llamaforcausallm_model_layers_18_post_attention_layernorm\n",
            "   Layer 19: llamaforcausallm_model_layers_19_post_attention_layernorm\n",
            "   Layer 20: llamaforcausallm_model_layers_20_post_attention_layernorm\n",
            "   Layer 21: llamaforcausallm_model_layers_21_post_attention_layernorm\n",
            "   Layer 22: llamaforcausallm_model_layers_22_post_attention_layernorm\n",
            "   Layer 23: llamaforcausallm_model_layers_23_post_attention_layernorm\n",
            "   Layer 24: llamaforcausallm_model_layers_24_post_attention_layernorm\n",
            "   Total: 10 layers\n",
            "\n",
            "âœ… VALIDATED (Tested in research, layers 7, 15, 23):\n",
            "   Layer  7: llamaforcausallm_model_layers_7_post_attention_layernorm\n",
            "   Layer 15: llamaforcausallm_model_layers_15_post_attention_layernorm\n",
            "   Layer 23: llamaforcausallm_model_layers_23_post_attention_layernorm\n",
            "\n",
            "âš ï¸  AVOID - Very Early (0-6, too little structure):\n",
            "   Layer  0: llamaforcausallm_model_layers_0_post_attention_layernorm\n",
            "   Layer  1: llamaforcausallm_model_layers_1_post_attention_layernorm\n",
            "   Layer  2: llamaforcausallm_model_layers_2_post_attention_layernorm\n",
            "   Layer  3: llamaforcausallm_model_layers_3_post_attention_layernorm\n",
            "   Layer  4: llamaforcausallm_model_layers_4_post_attention_layernorm\n",
            "   Layer  5: llamaforcausallm_model_layers_5_post_attention_layernorm\n",
            "   Layer  6: llamaforcausallm_model_layers_6_post_attention_layernorm\n",
            "   Total: 7 layers\n",
            "\n",
            "âš ï¸  AVOID - Very Late (30-31, anomalous behavior):\n",
            "   Layer 30: llamaforcausallm_model_layers_30_post_attention_layernorm\n",
            "   Layer 31: llamaforcausallm_model_layers_31_post_attention_layernorm\n",
            "   Total: 2 layers\n",
            "\n",
            "================================================================================\n",
            "\n",
            "ðŸ’¡ RECOMMENDATION: Use layers 15-24 for best results\n",
            "   Example: llamaforcausallm_model_layers_16_post_attention_layernorm\n",
            "   (Layer 16 is in the middle of the recommended range)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_layer_number(layer_name):\n",
        "    \"\"\"Extract layer number from layer name.\"\"\"\n",
        "    match = re.search(r'_layers_(\\d+)_post_attention_layernorm', layer_name)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "layer_numbers = {}\n",
        "for name in post_attention_layers:\n",
        "    layer_num = extract_layer_number(name)\n",
        "    if layer_num is not None:\n",
        "        layer_numbers[layer_num] = name\n",
        "\n",
        "total_layers = len(post_attention_layers)\n",
        "print(f\"ðŸ“Š Layer Analysis for Bielik-1.5B ({total_layers} layers total)\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "recommended_layers = [num for num in layer_numbers.keys() if 15 <= num <= 24]\n",
        "validated_layers = [7, 15, 23]\n",
        "avoid_early = [num for num in layer_numbers.keys() if 0 <= num <= 6]\n",
        "avoid_late = [num for num in layer_numbers.keys() if 30 <= num <= 31]\n",
        "\n",
        "print(\"âœ… RECOMMENDED (Mid-to-upper layers, 15-24):\")\n",
        "for num in sorted(recommended_layers):\n",
        "    print(f\"   Layer {num:2d}: {layer_numbers[num]}\")\n",
        "print(f\"   Total: {len(recommended_layers)} layers\\n\")\n",
        "\n",
        "print(\"âœ… VALIDATED (Tested in research, layers 7, 15, 23):\")\n",
        "for num in sorted([n for n in validated_layers if n in layer_numbers]):\n",
        "    print(f\"   Layer {num:2d}: {layer_numbers[num]}\")\n",
        "print()\n",
        "\n",
        "print(\"âš ï¸  AVOID - Very Early (0-6, too little structure):\")\n",
        "for num in sorted(avoid_early):\n",
        "    print(f\"   Layer {num:2d}: {layer_numbers[num]}\")\n",
        "print(f\"   Total: {len(avoid_early)} layers\\n\")\n",
        "\n",
        "print(\"âš ï¸  AVOID - Very Late (30-31, anomalous behavior):\")\n",
        "for num in sorted(avoid_late):\n",
        "    print(f\"   Layer {num:2d}: {layer_numbers[num]}\")\n",
        "print(f\"   Total: {len(avoid_late)} layers\\n\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nðŸ’¡ RECOMMENDATION: Use layers 15-24 for best results\")\n",
        "print(f\"   Example: {layer_numbers[16] if 16 in layer_numbers else 'N/A'}\")\n",
        "print(f\"   (Layer 16 is in the middle of the recommended range)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e87083f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Recommended layer names saved to variables:\n",
            "   'recommended_layer_names' = 10 layers (15-24)\n",
            "   'validated_layer_names' = 3 layers (7, 15, 23)\n",
            "\n",
            "ðŸ“ Quick access examples:\n",
            "   Recommended: recommended_layer_names[0] = 'llamaforcausallm_model_layers_15_post_attention_layernorm'\n",
            "   Middle layer: recommended_layer_names[5] = 'llamaforcausallm_model_layers_20_post_attention_layernorm'\n",
            "   Validated: validated_layer_names[1] = 'llamaforcausallm_model_layers_15_post_attention_layernorm'\n"
          ]
        }
      ],
      "source": [
        "recommended_layer_names = [layer_numbers[num] for num in sorted(recommended_layers)]\n",
        "validated_layer_names = [layer_numbers[num] for num in sorted([n for n in validated_layers if n in layer_numbers])]\n",
        "\n",
        "print(\"ðŸ’¾ Recommended layer names saved to variables:\")\n",
        "print(f\"   'recommended_layer_names' = {len(recommended_layer_names)} layers (15-24)\")\n",
        "print(f\"   'validated_layer_names' = {len(validated_layer_names)} layers (7, 15, 23)\")\n",
        "print()\n",
        "print(\"ðŸ“ Quick access examples:\")\n",
        "if recommended_layer_names:\n",
        "    print(f\"   Recommended: recommended_layer_names[0] = '{recommended_layer_names[0]}'\")\n",
        "    print(f\"   Middle layer: recommended_layer_names[{len(recommended_layer_names)//2}] = '{recommended_layer_names[len(recommended_layer_names)//2]}'\")\n",
        "if validated_layer_names:\n",
        "    print(f\"   Validated: validated_layer_names[1] = '{validated_layer_names[1] if len(validated_layer_names) > 1 else validated_layer_names[0]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b0ca0f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
