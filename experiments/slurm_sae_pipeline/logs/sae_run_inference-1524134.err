Audited 188 packages in 121ms
2026-01-19 22:53:11,466 [INFO] __main__: ğŸš€ Starting SAE Inference with Text Tracking
2026-01-19 22:53:11,467 [INFO] __main__: ğŸ“± Using device: cuda
2026-01-19 22:53:11,467 [INFO] __main__: ğŸ”§ Model: speakleash/Bielik-4.5B-v3.0-Instruct
2026-01-19 22:53:11,467 [INFO] __main__: ğŸ“Š Dataset: clarin-pl/polemo2-official
2026-01-19 22:53:11,467 [INFO] __main__: ğŸ“Š Data limit: None
2026-01-19 22:53:11,467 [INFO] __main__: ğŸ“ Store directory: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store
2026-01-19 22:53:11,468 [INFO] __main__: ğŸ¯ Layers (2): llamaforcausallm_model_layers_28, llamaforcausallm_model_layers_38
2026-01-19 22:53:11,468 [INFO] __main__: ğŸ“¦ SAE paths (2):
2026-01-19 22:53:11,468 [INFO] __main__:    0: experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_28_20260118_144434/model.pt
2026-01-19 22:53:11,468 [INFO] __main__:    1: experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_38_20260118_144434/model.pt
2026-01-19 22:53:11,468 [INFO] __main__: ğŸ”¢ Top-K texts per neuron: 15
2026-01-19 22:53:11,468 [INFO] __main__: ğŸ“Š Tracking: both positive (top) and negative (bottom) activations
2026-01-19 22:53:11,468 [INFO] __main__: ğŸ“¦ Batch size: 32
2026-01-19 22:53:11,943 [INFO] __main__: ğŸ”Œ CUDA available: NVIDIA H100 PCIe
2026-01-19 22:53:11,943 [INFO] __main__: ğŸ’¾ CUDA memory: 85.03 GB
2026-01-19 22:53:11,943 [INFO] __main__: ğŸ“¥ Loading language model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:12<00:12, 12.97s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 12.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 12.35s/it]
2026-01-19 22:53:43,911 [INFO] __main__: âœ… Model loaded: speakleash_Bielik-4.5B-v3.0-Instruct
2026-01-19 22:53:43,911 [INFO] __main__: ğŸ“¥ Loading SAEs...
2026-01-19 22:53:43,911 [INFO] __main__:    Loading SAE 1/2 from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_28_20260118_144434/model.pt...
2026-01-19 22:53:44,711 [INFO] mi_crow.mechanistic.sae.modules.l1_sae: 
Loaded L1SAE from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_28_20260118_144434/model.pt
n_latents=8192, n_inputs=2048
2026-01-19 22:53:44,711 [INFO] __main__:    âœ… Loaded L1Sae: 2048 -> 8192
2026-01-19 22:53:44,714 [INFO] __main__:    âœ… Attached L1Sae to llamaforcausallm_model_layers_28
2026-01-19 22:53:44,714 [INFO] __main__:    Loading SAE 2/2 from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_38_20260118_144434/model.pt...
2026-01-19 22:53:45,694 [INFO] mi_crow.mechanistic.sae.modules.l1_sae: 
Loaded L1SAE from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_38_20260118_144434/model.pt
n_latents=8192, n_inputs=2048
2026-01-19 22:53:45,695 [INFO] __main__:    âœ… Loaded L1Sae: 2048 -> 8192
2026-01-19 22:53:45,695 [INFO] __main__:    âœ… Attached L1Sae to llamaforcausallm_model_layers_38
2026-01-19 22:53:45,695 [INFO] __main__: ğŸ”§ Attaching ModelInputDetector...
2026-01-19 22:53:45,695 [INFO] __main__: âœ… ModelInputDetector attached (hook ID: model_input_detector)
2026-01-19 22:53:45,695 [INFO] __main__: ğŸ“¥ Loading dataset...
2026-01-19 22:53:45,695 [INFO] __main__: ğŸ“¥ Using HuggingFace datasets server API for polemo2-official...
Saving the dataset (0/1 shards):   0%|          | 0/71827 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71827/71827 [00:00<00:00, 1525805.91 examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71827/71827 [00:00<00:00, 1515299.52 examples/s]
2026-01-19 22:53:46,287 [INFO] __main__: âœ… Loaded 71827 text samples from dataset
2026-01-19 22:53:46,288 [INFO] __main__: ğŸ“Š Using all 71827 available rows (no data limit)
2026-01-19 22:53:46,288 [INFO] __main__: ğŸš€ Running inference to collect texts...
2026-01-19 22:53:46,289 [INFO] __main__:    Tracking both positive (top) and negative (bottom) activations
2026-01-19 22:53:46,289 [INFO] __main__:    Processing 71827 samples in batches of 32
2026-01-19 22:53:46,289 [INFO] __main__: [DEBUG] Starting batch loop...
2026-01-19 22:53:46,289 [INFO] __main__: [DEBUG] Got batch 1, extracting texts...
2026-01-19 22:53:46,289 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:55:48,156 [INFO] __main__: [DEBUG] Inference completed in 121.87s
2026-01-19 22:55:51,717 [INFO] __main__: [DEBUG] Got batch 2, extracting texts...
2026-01-19 22:55:51,717 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:56:53,199 [INFO] __main__: [DEBUG] Inference completed in 61.48s
2026-01-19 22:56:54,566 [INFO] __main__: [DEBUG] Got batch 3, extracting texts...
2026-01-19 22:56:54,566 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:58:03,955 [INFO] __main__: [DEBUG] Inference completed in 69.39s
2026-01-19 22:58:05,230 [INFO] __main__: [DEBUG] Got batch 4, extracting texts...
2026-01-19 22:58:05,231 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:59:57,597 [INFO] __main__: [DEBUG] Inference completed in 112.37s
2026-01-19 23:00:00,267 [INFO] __main__: [DEBUG] Got batch 5, extracting texts...
2026-01-19 23:00:00,267 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:01:22,727 [INFO] __main__: [DEBUG] Inference completed in 82.46s
2026-01-19 23:01:24,882 [INFO] __main__: [DEBUG] Got batch 6, extracting texts...
2026-01-19 23:01:24,883 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:03:03,112 [INFO] __main__: [DEBUG] Inference completed in 98.23s
2026-01-19 23:03:05,929 [INFO] __main__: [DEBUG] Got batch 7, extracting texts...
2026-01-19 23:03:05,930 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:04:21,610 [INFO] __main__: [DEBUG] Inference completed in 75.68s
2026-01-19 23:04:23,553 [INFO] __main__: [DEBUG] Got batch 8, extracting texts...
2026-01-19 23:04:23,554 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:05:31,094 [INFO] __main__: [DEBUG] Inference completed in 67.54s
2026-01-19 23:05:32,958 [INFO] __main__: [DEBUG] Got batch 9, extracting texts...
2026-01-19 23:05:32,959 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:07:16,049 [INFO] __main__: [DEBUG] Inference completed in 103.09s
2026-01-19 23:07:18,927 [INFO] __main__: [DEBUG] Got batch 10, extracting texts...
2026-01-19 23:07:18,927 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:08:18,479 [INFO] __main__: [DEBUG] Inference completed in 59.55s
2026-01-19 23:08:18,480 [INFO] __main__:    Processed 10 batches...
2026-01-19 23:08:18,480 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 10 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-19 23:08:34,374 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.41 GB reserved
2026-01-19 23:08:34,374 [INFO] __main__: [DEBUG] Got batch 11, extracting texts...
2026-01-19 23:08:34,374 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:10:24,535 [INFO] __main__: [DEBUG] Inference completed in 110.16s
2026-01-19 23:10:27,631 [INFO] __main__: [DEBUG] Got batch 12, extracting texts...
2026-01-19 23:10:27,632 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:12:53,802 [INFO] __main__: [DEBUG] Inference completed in 146.17s
2026-01-19 23:12:58,485 [INFO] __main__: [DEBUG] Got batch 13, extracting texts...
2026-01-19 23:12:58,486 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:14:56,768 [INFO] __main__: [DEBUG] Inference completed in 118.28s
2026-01-19 23:15:00,359 [INFO] __main__: [DEBUG] Got batch 14, extracting texts...
2026-01-19 23:15:00,360 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:16:36,923 [INFO] __main__: [DEBUG] Inference completed in 96.56s
2026-01-19 23:16:39,580 [INFO] __main__: [DEBUG] Got batch 15, extracting texts...
2026-01-19 23:16:39,580 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:17:41,245 [INFO] __main__: [DEBUG] Inference completed in 61.66s
2026-01-19 23:17:42,737 [INFO] __main__: [DEBUG] Got batch 16, extracting texts...
2026-01-19 23:17:42,737 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:19:16,439 [INFO] __main__: [DEBUG] Inference completed in 93.70s
2026-01-19 23:19:19,009 [INFO] __main__: [DEBUG] Got batch 17, extracting texts...
2026-01-19 23:19:19,009 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:20:37,701 [INFO] __main__: [DEBUG] Inference completed in 78.69s
2026-01-19 23:20:39,598 [INFO] __main__: [DEBUG] Got batch 18, extracting texts...
2026-01-19 23:20:39,599 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:22:06,960 [INFO] __main__: [DEBUG] Inference completed in 87.36s
2026-01-19 23:22:09,505 [INFO] __main__: [DEBUG] Got batch 19, extracting texts...
2026-01-19 23:22:09,505 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:23:29,008 [INFO] __main__: [DEBUG] Inference completed in 79.50s
2026-01-19 23:23:30,946 [INFO] __main__: [DEBUG] Got batch 20, extracting texts...
2026-01-19 23:23:30,946 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:25:27,130 [INFO] __main__: [DEBUG] Inference completed in 116.18s
2026-01-19 23:25:27,131 [INFO] __main__:    Processed 20 batches...
2026-01-19 23:25:27,131 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 20 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-19 23:25:46,441 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.41 GB reserved
2026-01-19 23:25:46,442 [INFO] __main__: [DEBUG] Got batch 21, extracting texts...
2026-01-19 23:25:46,442 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:27:37,848 [INFO] __main__: [DEBUG] Inference completed in 111.40s
2026-01-19 23:27:41,203 [INFO] __main__: [DEBUG] Got batch 22, extracting texts...
2026-01-19 23:27:41,203 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:29:08,387 [INFO] __main__: [DEBUG] Inference completed in 87.18s
2026-01-19 23:29:10,629 [INFO] __main__: [DEBUG] Got batch 23, extracting texts...
2026-01-19 23:29:10,629 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:30:16,613 [INFO] __main__: [DEBUG] Inference completed in 65.98s
2026-01-19 23:30:18,391 [INFO] __main__: [DEBUG] Got batch 24, extracting texts...
2026-01-19 23:30:18,391 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:32:55,639 [INFO] __main__: [DEBUG] Inference completed in 157.25s
2026-01-19 23:33:01,066 [INFO] __main__: [DEBUG] Got batch 25, extracting texts...
2026-01-19 23:33:01,067 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:34:10,265 [INFO] __main__: [DEBUG] Inference completed in 69.20s
2026-01-19 23:34:12,174 [INFO] __main__: [DEBUG] Got batch 26, extracting texts...
2026-01-19 23:34:12,174 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:35:39,212 [INFO] __main__: [DEBUG] Inference completed in 87.04s
2026-01-19 23:35:41,506 [INFO] __main__: [DEBUG] Got batch 27, extracting texts...
2026-01-19 23:35:41,507 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:37:17,247 [INFO] __main__: [DEBUG] Inference completed in 95.74s
2026-01-19 23:37:19,909 [INFO] __main__: [DEBUG] Got batch 28, extracting texts...
2026-01-19 23:37:19,910 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:38:37,602 [INFO] __main__: [DEBUG] Inference completed in 77.69s
2026-01-19 23:38:40,060 [INFO] __main__: [DEBUG] Got batch 29, extracting texts...
2026-01-19 23:38:40,060 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:40:54,678 [INFO] __main__: [DEBUG] Inference completed in 134.62s
2026-01-19 23:40:59,073 [INFO] __main__: [DEBUG] Got batch 30, extracting texts...
2026-01-19 23:40:59,073 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:42:11,514 [INFO] __main__: [DEBUG] Inference completed in 72.44s
2026-01-19 23:42:11,514 [INFO] __main__:    Processed 30 batches...
2026-01-19 23:42:11,514 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 30 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-19 23:42:28,556 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.54 GB reserved
2026-01-19 23:42:28,557 [INFO] __main__: [DEBUG] Got batch 31, extracting texts...
2026-01-19 23:42:28,557 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:43:47,288 [INFO] __main__: [DEBUG] Inference completed in 78.73s
2026-01-19 23:43:49,397 [INFO] __main__: [DEBUG] Got batch 32, extracting texts...
2026-01-19 23:43:49,398 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:45:33,434 [INFO] __main__: [DEBUG] Inference completed in 104.04s
2026-01-19 23:45:36,296 [INFO] __main__: [DEBUG] Got batch 33, extracting texts...
2026-01-19 23:45:36,297 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:46:50,846 [INFO] __main__: [DEBUG] Inference completed in 74.55s
2026-01-19 23:46:53,050 [INFO] __main__: [DEBUG] Got batch 34, extracting texts...
2026-01-19 23:46:53,051 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:48:07,601 [INFO] __main__: [DEBUG] Inference completed in 74.55s
2026-01-19 23:48:09,299 [INFO] __main__: [DEBUG] Got batch 35, extracting texts...
2026-01-19 23:48:09,299 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:49:38,289 [INFO] __main__: [DEBUG] Inference completed in 88.99s
2026-01-19 23:49:40,859 [INFO] __main__: [DEBUG] Got batch 36, extracting texts...
2026-01-19 23:49:40,859 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:51:22,548 [INFO] __main__: [DEBUG] Inference completed in 101.69s
2026-01-19 23:51:24,896 [INFO] __main__: [DEBUG] Got batch 37, extracting texts...
2026-01-19 23:51:24,896 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:53:59,862 [INFO] __main__: [DEBUG] Inference completed in 154.97s
2026-01-19 23:54:05,007 [INFO] __main__: [DEBUG] Got batch 38, extracting texts...
2026-01-19 23:54:05,007 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:55:18,165 [INFO] __main__: [DEBUG] Inference completed in 73.16s
2026-01-19 23:55:20,253 [INFO] __main__: [DEBUG] Got batch 39, extracting texts...
2026-01-19 23:55:20,253 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:56:47,271 [INFO] __main__: [DEBUG] Inference completed in 87.02s
2026-01-19 23:56:49,509 [INFO] __main__: [DEBUG] Got batch 40, extracting texts...
2026-01-19 23:56:49,509 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:58:45,331 [INFO] __main__: [DEBUG] Inference completed in 115.82s
2026-01-19 23:58:45,331 [INFO] __main__:    Processed 40 batches...
2026-01-19 23:58:45,331 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 40 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-19 23:59:04,662 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.40 GB reserved
2026-01-19 23:59:04,663 [INFO] __main__: [DEBUG] Got batch 41, extracting texts...
2026-01-19 23:59:04,663 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:01:31,821 [INFO] __main__: [DEBUG] Inference completed in 147.16s
2026-01-20 00:01:36,245 [INFO] __main__: [DEBUG] Got batch 42, extracting texts...
2026-01-20 00:01:36,246 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:03:31,878 [INFO] __main__: [DEBUG] Inference completed in 115.63s
2026-01-20 00:03:35,606 [INFO] __main__: [DEBUG] Got batch 43, extracting texts...
2026-01-20 00:03:35,606 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:04:58,957 [INFO] __main__: [DEBUG] Inference completed in 83.35s
2026-01-20 00:05:00,999 [INFO] __main__: [DEBUG] Got batch 44, extracting texts...
2026-01-20 00:05:00,999 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:06:22,751 [INFO] __main__: [DEBUG] Inference completed in 81.75s
2026-01-20 00:06:24,859 [INFO] __main__: [DEBUG] Got batch 45, extracting texts...
2026-01-20 00:06:24,859 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:07:24,533 [INFO] __main__: [DEBUG] Inference completed in 59.67s
2026-01-20 00:07:25,947 [INFO] __main__: [DEBUG] Got batch 46, extracting texts...
2026-01-20 00:07:25,948 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:08:33,175 [INFO] __main__: [DEBUG] Inference completed in 67.23s
2026-01-20 00:08:34,975 [INFO] __main__: [DEBUG] Got batch 47, extracting texts...
2026-01-20 00:08:34,975 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:10:12,263 [INFO] __main__: [DEBUG] Inference completed in 97.29s
2026-01-20 00:10:14,971 [INFO] __main__: [DEBUG] Got batch 48, extracting texts...
2026-01-20 00:10:14,972 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:11:31,238 [INFO] __main__: [DEBUG] Inference completed in 76.27s
2026-01-20 00:11:33,376 [INFO] __main__: [DEBUG] Got batch 49, extracting texts...
2026-01-20 00:11:33,376 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:12:33,941 [INFO] __main__: [DEBUG] Inference completed in 60.56s
2026-01-20 00:12:35,639 [INFO] __main__: [DEBUG] Got batch 50, extracting texts...
2026-01-20 00:12:35,639 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:13:59,848 [INFO] __main__: [DEBUG] Inference completed in 84.21s
2026-01-20 00:13:59,848 [INFO] __main__:    Processed 50 batches...
2026-01-20 00:13:59,848 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 50 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 00:14:17,823 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.45 GB reserved
2026-01-20 00:14:17,824 [INFO] __main__: [DEBUG] Got batch 51, extracting texts...
2026-01-20 00:14:17,824 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:18:22,138 [INFO] __main__: [DEBUG] Inference completed in 244.31s
2026-01-20 00:18:30,809 [INFO] __main__: [DEBUG] Got batch 52, extracting texts...
2026-01-20 00:18:30,809 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:19:30,923 [INFO] __main__: [DEBUG] Inference completed in 60.11s
2026-01-20 00:19:32,565 [INFO] __main__: [DEBUG] Got batch 53, extracting texts...
2026-01-20 00:19:32,566 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:21:14,799 [INFO] __main__: [DEBUG] Inference completed in 102.23s
2026-01-20 00:21:18,319 [INFO] __main__: [DEBUG] Got batch 54, extracting texts...
2026-01-20 00:21:18,319 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:23:09,870 [INFO] __main__: [DEBUG] Inference completed in 111.55s
2026-01-20 00:23:13,471 [INFO] __main__: [DEBUG] Got batch 55, extracting texts...
2026-01-20 00:23:13,471 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:24:05,888 [INFO] __main__: [DEBUG] Inference completed in 52.42s
2026-01-20 00:24:07,479 [INFO] __main__: [DEBUG] Got batch 56, extracting texts...
2026-01-20 00:24:07,479 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:25:18,981 [INFO] __main__: [DEBUG] Inference completed in 71.50s
2026-01-20 00:25:21,455 [INFO] __main__: [DEBUG] Got batch 57, extracting texts...
2026-01-20 00:25:21,455 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:27:02,888 [INFO] __main__: [DEBUG] Inference completed in 101.43s
2026-01-20 00:27:06,198 [INFO] __main__: [DEBUG] Got batch 58, extracting texts...
2026-01-20 00:27:06,199 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:28:29,533 [INFO] __main__: [DEBUG] Inference completed in 83.33s
2026-01-20 00:28:32,041 [INFO] __main__: [DEBUG] Got batch 59, extracting texts...
2026-01-20 00:28:32,042 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:30:25,239 [INFO] __main__: [DEBUG] Inference completed in 113.20s
2026-01-20 00:30:29,029 [INFO] __main__: [DEBUG] Got batch 60, extracting texts...
2026-01-20 00:30:29,030 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:31:21,191 [INFO] __main__: [DEBUG] Inference completed in 52.16s
2026-01-20 00:31:21,191 [INFO] __main__:    Processed 60 batches...
2026-01-20 00:31:21,191 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 60 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 00:31:38,091 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.39 GB reserved
2026-01-20 00:31:38,092 [INFO] __main__: [DEBUG] Got batch 61, extracting texts...
2026-01-20 00:31:38,092 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:33:13,185 [INFO] __main__: [DEBUG] Inference completed in 95.09s
2026-01-20 00:33:16,394 [INFO] __main__: [DEBUG] Got batch 62, extracting texts...
2026-01-20 00:33:16,395 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:35:13,259 [INFO] __main__: [DEBUG] Inference completed in 116.86s
2026-01-20 00:35:17,102 [INFO] __main__: [DEBUG] Got batch 63, extracting texts...
2026-01-20 00:35:17,102 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:37:14,913 [INFO] __main__: [DEBUG] Inference completed in 117.81s
2026-01-20 00:37:18,928 [INFO] __main__: [DEBUG] Got batch 64, extracting texts...
2026-01-20 00:37:18,930 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:38:56,537 [INFO] __main__: [DEBUG] Inference completed in 97.61s
2026-01-20 00:38:59,606 [INFO] __main__: [DEBUG] Got batch 65, extracting texts...
2026-01-20 00:38:59,607 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:40:37,997 [INFO] __main__: [DEBUG] Inference completed in 98.39s
2026-01-20 00:40:41,302 [INFO] __main__: [DEBUG] Got batch 66, extracting texts...
2026-01-20 00:40:41,302 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:42:16,752 [INFO] __main__: [DEBUG] Inference completed in 95.45s
2026-01-20 00:42:19,723 [INFO] __main__: [DEBUG] Got batch 67, extracting texts...
2026-01-20 00:42:19,723 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:43:52,540 [INFO] __main__: [DEBUG] Inference completed in 92.82s
2026-01-20 00:43:55,552 [INFO] __main__: [DEBUG] Got batch 68, extracting texts...
2026-01-20 00:43:55,552 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:44:50,563 [INFO] __main__: [DEBUG] Inference completed in 55.01s
2026-01-20 00:44:52,566 [INFO] __main__: [DEBUG] Got batch 69, extracting texts...
2026-01-20 00:44:52,567 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:46:13,051 [INFO] __main__: [DEBUG] Inference completed in 80.48s
2026-01-20 00:46:15,461 [INFO] __main__: [DEBUG] Got batch 70, extracting texts...
2026-01-20 00:46:15,461 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:47:33,937 [INFO] __main__: [DEBUG] Inference completed in 78.48s
2026-01-20 00:47:33,938 [INFO] __main__:    Processed 70 batches...
2026-01-20 00:47:33,938 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 70 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 00:47:52,295 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.47 GB reserved
2026-01-20 00:47:52,296 [INFO] __main__: [DEBUG] Got batch 71, extracting texts...
2026-01-20 00:47:52,297 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:49:07,906 [INFO] __main__: [DEBUG] Inference completed in 75.61s
2026-01-20 00:49:10,275 [INFO] __main__: [DEBUG] Got batch 72, extracting texts...
2026-01-20 00:49:10,276 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:51:31,505 [INFO] __main__: [DEBUG] Inference completed in 141.23s
2026-01-20 00:51:36,482 [INFO] __main__: [DEBUG] Got batch 73, extracting texts...
2026-01-20 00:51:36,483 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:53:43,028 [INFO] __main__: [DEBUG] Inference completed in 126.55s
2026-01-20 00:53:47,121 [INFO] __main__: [DEBUG] Got batch 74, extracting texts...
2026-01-20 00:53:47,121 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:54:58,034 [INFO] __main__: [DEBUG] Inference completed in 70.91s
2026-01-20 00:55:00,311 [INFO] __main__: [DEBUG] Got batch 75, extracting texts...
2026-01-20 00:55:00,311 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:56:37,580 [INFO] __main__: [DEBUG] Inference completed in 97.27s
2026-01-20 00:56:40,777 [INFO] __main__: [DEBUG] Got batch 76, extracting texts...
2026-01-20 00:56:40,778 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:57:44,234 [INFO] __main__: [DEBUG] Inference completed in 63.46s
2026-01-20 00:57:46,286 [INFO] __main__: [DEBUG] Got batch 77, extracting texts...
2026-01-20 00:57:46,287 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:59:24,501 [INFO] __main__: [DEBUG] Inference completed in 98.21s
2026-01-20 00:59:27,688 [INFO] __main__: [DEBUG] Got batch 78, extracting texts...
2026-01-20 00:59:27,688 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:01:15,681 [INFO] __main__: [DEBUG] Inference completed in 107.99s
2026-01-20 01:01:19,349 [INFO] __main__: [DEBUG] Got batch 79, extracting texts...
2026-01-20 01:01:19,349 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:02:41,685 [INFO] __main__: [DEBUG] Inference completed in 82.34s
2026-01-20 01:02:44,220 [INFO] __main__: [DEBUG] Got batch 80, extracting texts...
2026-01-20 01:02:44,221 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:04:12,993 [INFO] __main__: [DEBUG] Inference completed in 88.77s
2026-01-20 01:04:12,994 [INFO] __main__:    Processed 80 batches...
2026-01-20 01:04:12,994 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 80 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 01:04:31,405 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.49 GB reserved
2026-01-20 01:04:31,406 [INFO] __main__: [DEBUG] Got batch 81, extracting texts...
2026-01-20 01:04:31,406 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:05:46,240 [INFO] __main__: [DEBUG] Inference completed in 74.83s
2026-01-20 01:05:48,644 [INFO] __main__: [DEBUG] Got batch 82, extracting texts...
2026-01-20 01:05:48,645 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:07:39,197 [INFO] __main__: [DEBUG] Inference completed in 110.55s
2026-01-20 01:07:42,965 [INFO] __main__: [DEBUG] Got batch 83, extracting texts...
2026-01-20 01:07:42,965 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:09:08,883 [INFO] __main__: [DEBUG] Inference completed in 85.92s
2026-01-20 01:09:11,563 [INFO] __main__: [DEBUG] Got batch 84, extracting texts...
2026-01-20 01:09:11,564 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:10:07,446 [INFO] __main__: [DEBUG] Inference completed in 55.88s
2026-01-20 01:10:08,931 [INFO] __main__: [DEBUG] Got batch 85, extracting texts...
2026-01-20 01:10:08,932 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:12:06,275 [INFO] __main__: [DEBUG] Inference completed in 117.34s
2026-01-20 01:12:10,279 [INFO] __main__: [DEBUG] Got batch 86, extracting texts...
2026-01-20 01:12:10,279 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:13:55,562 [INFO] __main__: [DEBUG] Inference completed in 105.28s
2026-01-20 01:13:58,827 [INFO] __main__: [DEBUG] Got batch 87, extracting texts...
2026-01-20 01:13:58,828 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:15:04,210 [INFO] __main__: [DEBUG] Inference completed in 65.38s
2026-01-20 01:15:06,176 [INFO] __main__: [DEBUG] Got batch 88, extracting texts...
2026-01-20 01:15:06,177 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:16:30,825 [INFO] __main__: [DEBUG] Inference completed in 84.65s
2026-01-20 01:16:33,474 [INFO] __main__: [DEBUG] Got batch 89, extracting texts...
2026-01-20 01:16:33,475 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:18:11,017 [INFO] __main__: [DEBUG] Inference completed in 97.54s
2026-01-20 01:18:14,237 [INFO] __main__: [DEBUG] Got batch 90, extracting texts...
2026-01-20 01:18:14,238 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:19:48,017 [INFO] __main__: [DEBUG] Inference completed in 93.78s
2026-01-20 01:19:48,018 [INFO] __main__:    Processed 90 batches...
2026-01-20 01:19:48,018 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 90 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 01:20:06,515 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.51 GB reserved
2026-01-20 01:20:06,516 [INFO] __main__: [DEBUG] Got batch 91, extracting texts...
2026-01-20 01:20:06,516 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:21:14,251 [INFO] __main__: [DEBUG] Inference completed in 67.74s
2026-01-20 01:21:16,300 [INFO] __main__: [DEBUG] Got batch 92, extracting texts...
2026-01-20 01:21:16,302 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:22:25,972 [INFO] __main__: [DEBUG] Inference completed in 69.67s
2026-01-20 01:22:28,282 [INFO] __main__: [DEBUG] Got batch 93, extracting texts...
2026-01-20 01:22:28,283 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:24:24,949 [INFO] __main__: [DEBUG] Inference completed in 116.67s
2026-01-20 01:24:28,759 [INFO] __main__: [DEBUG] Got batch 94, extracting texts...
2026-01-20 01:24:28,759 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:26:05,120 [INFO] __main__: [DEBUG] Inference completed in 96.36s
2026-01-20 01:26:08,376 [INFO] __main__: [DEBUG] Got batch 95, extracting texts...
2026-01-20 01:26:08,376 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:27:34,544 [INFO] __main__: [DEBUG] Inference completed in 86.17s
2026-01-20 01:27:37,278 [INFO] __main__: [DEBUG] Got batch 96, extracting texts...
2026-01-20 01:27:37,279 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:29:05,817 [INFO] __main__: [DEBUG] Inference completed in 88.54s
2026-01-20 01:29:08,603 [INFO] __main__: [DEBUG] Got batch 97, extracting texts...
2026-01-20 01:29:08,603 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:30:37,721 [INFO] __main__: [DEBUG] Inference completed in 89.12s
2026-01-20 01:30:40,175 [INFO] __main__: [DEBUG] Got batch 98, extracting texts...
2026-01-20 01:30:40,176 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:31:56,015 [INFO] __main__: [DEBUG] Inference completed in 75.84s
2026-01-20 01:31:58,136 [INFO] __main__: [DEBUG] Got batch 99, extracting texts...
2026-01-20 01:31:58,136 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:34:40,350 [INFO] __main__: [DEBUG] Inference completed in 162.21s
2026-01-20 01:34:45,457 [INFO] __main__: [DEBUG] Got batch 100, extracting texts...
2026-01-20 01:34:45,457 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:35:49,787 [INFO] __main__: [DEBUG] Inference completed in 64.33s
2026-01-20 01:35:49,788 [INFO] __main__:    Processed 100 batches...
2026-01-20 01:35:49,788 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 100 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 01:36:07,026 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.57 GB reserved
2026-01-20 01:36:07,028 [INFO] __main__: [DEBUG] Got batch 101, extracting texts...
2026-01-20 01:36:07,028 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:36:53,700 [INFO] __main__: [DEBUG] Inference completed in 46.67s
2026-01-20 01:36:54,964 [INFO] __main__: [DEBUG] Got batch 102, extracting texts...
2026-01-20 01:36:54,965 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:38:01,434 [INFO] __main__: [DEBUG] Inference completed in 66.47s
2026-01-20 01:38:03,707 [INFO] __main__: [DEBUG] Got batch 103, extracting texts...
2026-01-20 01:38:03,707 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:39:57,040 [INFO] __main__: [DEBUG] Inference completed in 113.33s
2026-01-20 01:40:00,620 [INFO] __main__: [DEBUG] Got batch 104, extracting texts...
2026-01-20 01:40:00,620 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:41:31,790 [INFO] __main__: [DEBUG] Inference completed in 91.17s
2026-01-20 01:41:34,889 [INFO] __main__: [DEBUG] Got batch 105, extracting texts...
2026-01-20 01:41:34,889 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:42:51,792 [INFO] __main__: [DEBUG] Inference completed in 76.90s
2026-01-20 01:42:54,129 [INFO] __main__: [DEBUG] Got batch 106, extracting texts...
2026-01-20 01:42:54,129 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:44:16,106 [INFO] __main__: [DEBUG] Inference completed in 81.98s
2026-01-20 01:44:18,713 [INFO] __main__: [DEBUG] Got batch 107, extracting texts...
2026-01-20 01:44:18,713 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:45:39,778 [INFO] __main__: [DEBUG] Inference completed in 81.06s
2026-01-20 01:45:42,253 [INFO] __main__: [DEBUG] Got batch 108, extracting texts...
2026-01-20 01:45:42,254 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:47:23,806 [INFO] __main__: [DEBUG] Inference completed in 101.55s
2026-01-20 01:47:27,091 [INFO] __main__: [DEBUG] Got batch 109, extracting texts...
2026-01-20 01:47:27,091 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:49:29,869 [INFO] __main__: [DEBUG] Inference completed in 122.78s
2026-01-20 01:49:34,112 [INFO] __main__: [DEBUG] Got batch 110, extracting texts...
2026-01-20 01:49:34,112 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:51:48,106 [INFO] __main__: [DEBUG] Inference completed in 133.99s
2026-01-20 01:51:48,107 [INFO] __main__:    Processed 110 batches...
2026-01-20 01:51:48,107 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 110 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 01:52:08,826 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.43 GB reserved
2026-01-20 01:52:08,827 [INFO] __main__: [DEBUG] Got batch 111, extracting texts...
2026-01-20 01:52:08,827 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:53:55,837 [INFO] __main__: [DEBUG] Inference completed in 107.01s
2026-01-20 01:53:59,290 [INFO] __main__: [DEBUG] Got batch 112, extracting texts...
2026-01-20 01:53:59,291 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:56:43,182 [INFO] __main__: [DEBUG] Inference completed in 163.89s
2026-01-20 01:56:48,540 [INFO] __main__: [DEBUG] Got batch 113, extracting texts...
2026-01-20 01:56:48,541 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:58:29,099 [INFO] __main__: [DEBUG] Inference completed in 100.56s
2026-01-20 01:58:32,317 [INFO] __main__: [DEBUG] Got batch 114, extracting texts...
2026-01-20 01:58:32,317 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:00:15,713 [INFO] __main__: [DEBUG] Inference completed in 103.40s
2026-01-20 02:00:19,236 [INFO] __main__: [DEBUG] Got batch 115, extracting texts...
2026-01-20 02:00:19,236 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:01:31,813 [INFO] __main__: [DEBUG] Inference completed in 72.58s
2026-01-20 02:01:34,007 [INFO] __main__: [DEBUG] Got batch 116, extracting texts...
2026-01-20 02:01:34,008 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:02:47,615 [INFO] __main__: [DEBUG] Inference completed in 73.61s
2026-01-20 02:02:49,942 [INFO] __main__: [DEBUG] Got batch 117, extracting texts...
2026-01-20 02:02:49,942 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:04:48,125 [INFO] __main__: [DEBUG] Inference completed in 118.18s
2026-01-20 02:04:52,405 [INFO] __main__: [DEBUG] Got batch 118, extracting texts...
2026-01-20 02:04:52,405 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:08:54,227 [INFO] __main__: [DEBUG] Inference completed in 241.82s
2026-01-20 02:09:02,190 [INFO] __main__: [DEBUG] Got batch 119, extracting texts...
2026-01-20 02:09:02,191 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:10:10,360 [INFO] __main__: [DEBUG] Inference completed in 68.17s
2026-01-20 02:10:12,089 [INFO] __main__: [DEBUG] Got batch 120, extracting texts...
2026-01-20 02:10:12,089 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:11:54,150 [INFO] __main__: [DEBUG] Inference completed in 102.06s
2026-01-20 02:11:54,151 [INFO] __main__:    Processed 120 batches...
2026-01-20 02:11:54,151 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 120 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 02:12:13,231 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.39 GB reserved
2026-01-20 02:12:13,232 [INFO] __main__: [DEBUG] Got batch 121, extracting texts...
2026-01-20 02:12:13,232 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:13:19,373 [INFO] __main__: [DEBUG] Inference completed in 66.14s
2026-01-20 02:13:21,389 [INFO] __main__: [DEBUG] Got batch 122, extracting texts...
2026-01-20 02:13:21,390 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:16:13,582 [INFO] __main__: [DEBUG] Inference completed in 172.19s
2026-01-20 02:16:19,510 [INFO] __main__: [DEBUG] Got batch 123, extracting texts...
2026-01-20 02:16:19,511 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:18:43,790 [INFO] __main__: [DEBUG] Inference completed in 144.28s
2026-01-20 02:18:48,647 [INFO] __main__: [DEBUG] Got batch 124, extracting texts...
2026-01-20 02:18:48,647 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:20:17,503 [INFO] __main__: [DEBUG] Inference completed in 88.86s
2026-01-20 02:20:20,423 [INFO] __main__: [DEBUG] Got batch 125, extracting texts...
2026-01-20 02:20:20,424 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:21:41,170 [INFO] __main__: [DEBUG] Inference completed in 80.75s
2026-01-20 02:21:43,478 [INFO] __main__: [DEBUG] Got batch 126, extracting texts...
2026-01-20 02:21:43,479 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:23:21,924 [INFO] __main__: [DEBUG] Inference completed in 98.45s
2026-01-20 02:23:25,151 [INFO] __main__: [DEBUG] Got batch 127, extracting texts...
2026-01-20 02:23:25,151 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:24:36,103 [INFO] __main__: [DEBUG] Inference completed in 70.95s
2026-01-20 02:24:38,071 [INFO] __main__: [DEBUG] Got batch 128, extracting texts...
2026-01-20 02:24:38,072 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:25:50,095 [INFO] __main__: [DEBUG] Inference completed in 72.02s
2026-01-20 02:25:52,274 [INFO] __main__: [DEBUG] Got batch 129, extracting texts...
2026-01-20 02:25:52,274 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:27:06,044 [INFO] __main__: [DEBUG] Inference completed in 73.77s
2026-01-20 02:27:08,338 [INFO] __main__: [DEBUG] Got batch 130, extracting texts...
2026-01-20 02:27:08,338 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:28:09,870 [INFO] __main__: [DEBUG] Inference completed in 61.53s
2026-01-20 02:28:09,871 [INFO] __main__:    Processed 130 batches...
2026-01-20 02:28:09,871 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 130 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 02:28:27,277 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.42 GB reserved
2026-01-20 02:28:27,278 [INFO] __main__: [DEBUG] Got batch 131, extracting texts...
2026-01-20 02:28:27,278 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:29:29,096 [INFO] __main__: [DEBUG] Inference completed in 61.82s
2026-01-20 02:29:30,996 [INFO] __main__: [DEBUG] Got batch 132, extracting texts...
2026-01-20 02:29:30,996 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:30:33,432 [INFO] __main__: [DEBUG] Inference completed in 62.44s
2026-01-20 02:30:35,474 [INFO] __main__: [DEBUG] Got batch 133, extracting texts...
2026-01-20 02:30:35,474 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:31:59,159 [INFO] __main__: [DEBUG] Inference completed in 83.68s
2026-01-20 02:32:01,668 [INFO] __main__: [DEBUG] Got batch 134, extracting texts...
2026-01-20 02:32:01,669 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:33:44,486 [INFO] __main__: [DEBUG] Inference completed in 102.82s
2026-01-20 02:33:47,900 [INFO] __main__: [DEBUG] Got batch 135, extracting texts...
2026-01-20 02:33:47,900 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:36:02,198 [INFO] __main__: [DEBUG] Inference completed in 134.30s
2026-01-20 02:36:06,444 [INFO] __main__: [DEBUG] Got batch 136, extracting texts...
2026-01-20 02:36:06,444 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:37:08,106 [INFO] __main__: [DEBUG] Inference completed in 61.66s
2026-01-20 02:37:09,907 [INFO] __main__: [DEBUG] Got batch 137, extracting texts...
2026-01-20 02:37:09,907 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:38:46,673 [INFO] __main__: [DEBUG] Inference completed in 96.77s
2026-01-20 02:38:49,633 [INFO] __main__: [DEBUG] Got batch 138, extracting texts...
2026-01-20 02:38:49,633 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:39:51,185 [INFO] __main__: [DEBUG] Inference completed in 61.55s
2026-01-20 02:39:52,962 [INFO] __main__: [DEBUG] Got batch 139, extracting texts...
2026-01-20 02:39:52,963 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:41:29,432 [INFO] __main__: [DEBUG] Inference completed in 96.47s
2026-01-20 02:41:32,526 [INFO] __main__: [DEBUG] Got batch 140, extracting texts...
2026-01-20 02:41:32,526 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:43:06,374 [INFO] __main__: [DEBUG] Inference completed in 93.85s
2026-01-20 02:43:06,375 [INFO] __main__:    Processed 140 batches...
2026-01-20 02:43:06,375 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 140 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 02:43:25,232 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.51 GB reserved
2026-01-20 02:43:25,233 [INFO] __main__: [DEBUG] Got batch 141, extracting texts...
2026-01-20 02:43:25,233 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:44:42,995 [INFO] __main__: [DEBUG] Inference completed in 77.76s
2026-01-20 02:44:45,448 [INFO] __main__: [DEBUG] Got batch 142, extracting texts...
2026-01-20 02:44:45,449 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:45:40,607 [INFO] __main__: [DEBUG] Inference completed in 55.16s
2026-01-20 02:45:42,074 [INFO] __main__: [DEBUG] Got batch 143, extracting texts...
2026-01-20 02:45:42,074 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:46:51,557 [INFO] __main__: [DEBUG] Inference completed in 69.48s
2026-01-20 02:46:53,717 [INFO] __main__: [DEBUG] Got batch 144, extracting texts...
2026-01-20 02:46:53,718 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:48:09,479 [INFO] __main__: [DEBUG] Inference completed in 75.76s
2026-01-20 02:48:11,861 [INFO] __main__: [DEBUG] Got batch 145, extracting texts...
2026-01-20 02:48:11,862 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:49:51,527 [INFO] __main__: [DEBUG] Inference completed in 99.67s
2026-01-20 02:49:54,816 [INFO] __main__: [DEBUG] Got batch 146, extracting texts...
2026-01-20 02:49:54,817 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:52:01,645 [INFO] __main__: [DEBUG] Inference completed in 126.83s
2026-01-20 02:52:05,702 [INFO] __main__: [DEBUG] Got batch 147, extracting texts...
2026-01-20 02:52:05,702 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:53:12,157 [INFO] __main__: [DEBUG] Inference completed in 66.45s
2026-01-20 02:53:14,149 [INFO] __main__: [DEBUG] Got batch 148, extracting texts...
2026-01-20 02:53:14,150 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:54:40,461 [INFO] __main__: [DEBUG] Inference completed in 86.31s
2026-01-20 02:54:43,102 [INFO] __main__: [DEBUG] Got batch 149, extracting texts...
2026-01-20 02:54:43,102 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:56:07,124 [INFO] __main__: [DEBUG] Inference completed in 84.02s
2026-01-20 02:56:09,668 [INFO] __main__: [DEBUG] Got batch 150, extracting texts...
2026-01-20 02:56:09,668 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:57:59,916 [INFO] __main__: [DEBUG] Inference completed in 110.25s
2026-01-20 02:57:59,917 [INFO] __main__:    Processed 150 batches...
2026-01-20 02:57:59,917 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 150 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 02:58:19,186 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.40 GB reserved
2026-01-20 02:58:19,187 [INFO] __main__: [DEBUG] Got batch 151, extracting texts...
2026-01-20 02:58:19,187 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:59:47,438 [INFO] __main__: [DEBUG] Inference completed in 88.25s
2026-01-20 02:59:50,338 [INFO] __main__: [DEBUG] Got batch 152, extracting texts...
2026-01-20 02:59:50,339 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:01:15,261 [INFO] __main__: [DEBUG] Inference completed in 84.92s
2026-01-20 03:01:17,881 [INFO] __main__: [DEBUG] Got batch 153, extracting texts...
2026-01-20 03:01:17,882 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:03:50,215 [INFO] __main__: [DEBUG] Inference completed in 152.33s
2026-01-20 03:03:55,420 [INFO] __main__: [DEBUG] Got batch 154, extracting texts...
2026-01-20 03:03:55,421 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:05:26,860 [INFO] __main__: [DEBUG] Inference completed in 91.44s
2026-01-20 03:05:29,700 [INFO] __main__: [DEBUG] Got batch 155, extracting texts...
2026-01-20 03:05:29,701 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:06:51,788 [INFO] __main__: [DEBUG] Inference completed in 82.09s
2026-01-20 03:06:54,319 [INFO] __main__: [DEBUG] Got batch 156, extracting texts...
2026-01-20 03:06:54,320 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:08:40,664 [INFO] __main__: [DEBUG] Inference completed in 106.34s
2026-01-20 03:08:44,144 [INFO] __main__: [DEBUG] Got batch 157, extracting texts...
2026-01-20 03:08:44,144 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:10:06,054 [INFO] __main__: [DEBUG] Inference completed in 81.91s
2026-01-20 03:10:08,438 [INFO] __main__: [DEBUG] Got batch 158, extracting texts...
2026-01-20 03:10:08,438 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:11:17,730 [INFO] __main__: [DEBUG] Inference completed in 69.29s
2026-01-20 03:11:19,871 [INFO] __main__: [DEBUG] Got batch 159, extracting texts...
2026-01-20 03:11:19,872 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:14:36,378 [INFO] __main__: [DEBUG] Inference completed in 196.51s
2026-01-20 03:14:42,526 [INFO] __main__: [DEBUG] Got batch 160, extracting texts...
2026-01-20 03:14:42,528 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:16:24,524 [INFO] __main__: [DEBUG] Inference completed in 102.00s
2026-01-20 03:16:24,524 [INFO] __main__:    Processed 160 batches...
2026-01-20 03:16:24,524 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 160 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 03:16:44,098 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.53 GB reserved
2026-01-20 03:16:44,099 [INFO] __main__: [DEBUG] Got batch 161, extracting texts...
2026-01-20 03:16:44,099 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:17:52,028 [INFO] __main__: [DEBUG] Inference completed in 67.93s
2026-01-20 03:17:54,095 [INFO] __main__: [DEBUG] Got batch 162, extracting texts...
2026-01-20 03:17:54,095 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:19:35,876 [INFO] __main__: [DEBUG] Inference completed in 101.78s
2026-01-20 03:19:39,271 [INFO] __main__: [DEBUG] Got batch 163, extracting texts...
2026-01-20 03:19:39,272 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:20:45,257 [INFO] __main__: [DEBUG] Inference completed in 65.99s
2026-01-20 03:20:47,228 [INFO] __main__: [DEBUG] Got batch 164, extracting texts...
2026-01-20 03:20:47,229 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:23:12,381 [INFO] __main__: [DEBUG] Inference completed in 145.15s
2026-01-20 03:23:17,054 [INFO] __main__: [DEBUG] Got batch 165, extracting texts...
2026-01-20 03:23:17,054 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:24:06,326 [INFO] __main__: [DEBUG] Inference completed in 49.27s
2026-01-20 03:24:07,791 [INFO] __main__: [DEBUG] Got batch 166, extracting texts...
2026-01-20 03:24:07,791 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:25:38,728 [INFO] __main__: [DEBUG] Inference completed in 90.94s
2026-01-20 03:25:41,461 [INFO] __main__: [DEBUG] Got batch 167, extracting texts...
2026-01-20 03:25:41,462 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:26:52,297 [INFO] __main__: [DEBUG] Inference completed in 70.83s
2026-01-20 03:26:54,509 [INFO] __main__: [DEBUG] Got batch 168, extracting texts...
2026-01-20 03:26:54,509 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:30:22,027 [INFO] __main__: [DEBUG] Inference completed in 207.52s
2026-01-20 03:30:28,687 [INFO] __main__: [DEBUG] Got batch 169, extracting texts...
2026-01-20 03:30:28,688 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:31:37,846 [INFO] __main__: [DEBUG] Inference completed in 69.16s
2026-01-20 03:31:39,913 [INFO] __main__: [DEBUG] Got batch 170, extracting texts...
2026-01-20 03:31:39,913 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:32:49,903 [INFO] __main__: [DEBUG] Inference completed in 69.99s
2026-01-20 03:32:49,904 [INFO] __main__:    Processed 170 batches...
2026-01-20 03:32:49,904 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 170 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 03:33:08,088 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.65 GB reserved
2026-01-20 03:33:08,089 [INFO] __main__: [DEBUG] Got batch 171, extracting texts...
2026-01-20 03:33:08,089 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:34:46,021 [INFO] __main__: [DEBUG] Inference completed in 97.93s
2026-01-20 03:34:49,356 [INFO] __main__: [DEBUG] Got batch 172, extracting texts...
2026-01-20 03:34:49,356 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:36:11,784 [INFO] __main__: [DEBUG] Inference completed in 82.43s
2026-01-20 03:36:14,298 [INFO] __main__: [DEBUG] Got batch 173, extracting texts...
2026-01-20 03:36:14,298 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:37:34,562 [INFO] __main__: [DEBUG] Inference completed in 80.26s
2026-01-20 03:37:36,924 [INFO] __main__: [DEBUG] Got batch 174, extracting texts...
2026-01-20 03:37:36,924 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:38:50,489 [INFO] __main__: [DEBUG] Inference completed in 73.56s
2026-01-20 03:38:52,832 [INFO] __main__: [DEBUG] Got batch 175, extracting texts...
2026-01-20 03:38:52,832 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:39:50,469 [INFO] __main__: [DEBUG] Inference completed in 57.64s
2026-01-20 03:39:52,235 [INFO] __main__: [DEBUG] Got batch 176, extracting texts...
2026-01-20 03:39:52,235 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:41:10,674 [INFO] __main__: [DEBUG] Inference completed in 78.44s
2026-01-20 03:41:13,249 [INFO] __main__: [DEBUG] Got batch 177, extracting texts...
2026-01-20 03:41:13,250 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:42:30,482 [INFO] __main__: [DEBUG] Inference completed in 77.23s
2026-01-20 03:42:32,825 [INFO] __main__: [DEBUG] Got batch 178, extracting texts...
2026-01-20 03:42:32,826 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:43:53,113 [INFO] __main__: [DEBUG] Inference completed in 80.29s
2026-01-20 03:43:55,540 [INFO] __main__: [DEBUG] Got batch 179, extracting texts...
2026-01-20 03:43:55,540 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:46:23,113 [INFO] __main__: [DEBUG] Inference completed in 147.57s
2026-01-20 03:46:28,163 [INFO] __main__: [DEBUG] Got batch 180, extracting texts...
2026-01-20 03:46:28,164 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:48:39,067 [INFO] __main__: [DEBUG] Inference completed in 130.90s
2026-01-20 03:48:39,067 [INFO] __main__:    Processed 180 batches...
2026-01-20 03:48:39,067 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 180 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 03:48:59,693 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.63 GB reserved
2026-01-20 03:48:59,694 [INFO] __main__: [DEBUG] Got batch 181, extracting texts...
2026-01-20 03:48:59,694 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:53:00,859 [INFO] __main__: [DEBUG] Inference completed in 241.17s
2026-01-20 03:53:08,824 [INFO] __main__: [DEBUG] Got batch 182, extracting texts...
2026-01-20 03:53:08,825 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:54:15,809 [INFO] __main__: [DEBUG] Inference completed in 66.98s
2026-01-20 03:54:17,383 [INFO] __main__: [DEBUG] Got batch 183, extracting texts...
2026-01-20 03:54:17,384 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:55:30,004 [INFO] __main__: [DEBUG] Inference completed in 72.62s
2026-01-20 03:55:32,283 [INFO] __main__: [DEBUG] Got batch 184, extracting texts...
2026-01-20 03:55:32,284 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:56:43,795 [INFO] __main__: [DEBUG] Inference completed in 71.51s
2026-01-20 03:56:46,045 [INFO] __main__: [DEBUG] Got batch 185, extracting texts...
2026-01-20 03:56:46,046 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:57:52,982 [INFO] __main__: [DEBUG] Inference completed in 66.94s
2026-01-20 03:57:55,208 [INFO] __main__: [DEBUG] Got batch 186, extracting texts...
2026-01-20 03:57:55,208 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:59:08,454 [INFO] __main__: [DEBUG] Inference completed in 73.25s
2026-01-20 03:59:10,549 [INFO] __main__: [DEBUG] Got batch 187, extracting texts...
2026-01-20 03:59:10,550 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:00:27,466 [INFO] __main__: [DEBUG] Inference completed in 76.92s
2026-01-20 04:00:29,888 [INFO] __main__: [DEBUG] Got batch 188, extracting texts...
2026-01-20 04:00:29,888 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:01:54,158 [INFO] __main__: [DEBUG] Inference completed in 84.27s
2026-01-20 04:01:56,770 [INFO] __main__: [DEBUG] Got batch 189, extracting texts...
2026-01-20 04:01:56,770 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:03:17,572 [INFO] __main__: [DEBUG] Inference completed in 80.80s
2026-01-20 04:03:20,037 [INFO] __main__: [DEBUG] Got batch 190, extracting texts...
2026-01-20 04:03:20,038 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:04:46,073 [INFO] __main__: [DEBUG] Inference completed in 86.04s
2026-01-20 04:04:46,074 [INFO] __main__:    Processed 190 batches...
2026-01-20 04:04:46,074 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 190 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 04:05:06,296 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.49 GB reserved
2026-01-20 04:05:06,317 [INFO] __main__: [DEBUG] Got batch 191, extracting texts...
2026-01-20 04:05:06,317 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:06:31,958 [INFO] __main__: [DEBUG] Inference completed in 85.64s
2026-01-20 04:06:34,640 [INFO] __main__: [DEBUG] Got batch 192, extracting texts...
2026-01-20 04:06:34,640 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:07:58,713 [INFO] __main__: [DEBUG] Inference completed in 84.07s
2026-01-20 04:08:00,923 [INFO] __main__: [DEBUG] Got batch 193, extracting texts...
2026-01-20 04:08:00,924 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:09:15,454 [INFO] __main__: [DEBUG] Inference completed in 74.53s
2026-01-20 04:09:17,828 [INFO] __main__: [DEBUG] Got batch 194, extracting texts...
2026-01-20 04:09:17,828 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:10:21,163 [INFO] __main__: [DEBUG] Inference completed in 63.33s
2026-01-20 04:10:23,177 [INFO] __main__: [DEBUG] Got batch 195, extracting texts...
2026-01-20 04:10:23,178 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:12:17,711 [INFO] __main__: [DEBUG] Inference completed in 114.53s
2026-01-20 04:12:21,723 [INFO] __main__: [DEBUG] Got batch 196, extracting texts...
2026-01-20 04:12:21,723 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:13:57,996 [INFO] __main__: [DEBUG] Inference completed in 96.27s
2026-01-20 04:14:01,293 [INFO] __main__: [DEBUG] Got batch 197, extracting texts...
2026-01-20 04:14:01,293 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:15:31,152 [INFO] __main__: [DEBUG] Inference completed in 89.86s
2026-01-20 04:15:34,097 [INFO] __main__: [DEBUG] Got batch 198, extracting texts...
2026-01-20 04:15:34,097 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:16:54,570 [INFO] __main__: [DEBUG] Inference completed in 80.47s
2026-01-20 04:16:57,028 [INFO] __main__: [DEBUG] Got batch 199, extracting texts...
2026-01-20 04:16:57,028 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:18:24,198 [INFO] __main__: [DEBUG] Inference completed in 87.17s
2026-01-20 04:18:27,032 [INFO] __main__: [DEBUG] Got batch 200, extracting texts...
2026-01-20 04:18:27,032 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:20:18,515 [INFO] __main__: [DEBUG] Inference completed in 111.48s
2026-01-20 04:20:18,516 [INFO] __main__:    Processed 200 batches...
2026-01-20 04:20:18,516 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 200 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 04:20:38,490 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.40 GB reserved
2026-01-20 04:20:38,491 [INFO] __main__: [DEBUG] Got batch 201, extracting texts...
2026-01-20 04:20:38,491 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:22:35,830 [INFO] __main__: [DEBUG] Inference completed in 117.34s
2026-01-20 04:22:39,799 [INFO] __main__: [DEBUG] Got batch 202, extracting texts...
2026-01-20 04:22:39,800 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:24:44,485 [INFO] __main__: [DEBUG] Inference completed in 124.69s
2026-01-20 04:24:48,653 [INFO] __main__: [DEBUG] Got batch 203, extracting texts...
2026-01-20 04:24:48,653 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:26:20,440 [INFO] __main__: [DEBUG] Inference completed in 91.79s
2026-01-20 04:26:23,511 [INFO] __main__: [DEBUG] Got batch 204, extracting texts...
2026-01-20 04:26:23,511 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:27:46,478 [INFO] __main__: [DEBUG] Inference completed in 82.97s
2026-01-20 04:27:48,803 [INFO] __main__: [DEBUG] Got batch 205, extracting texts...
2026-01-20 04:27:48,803 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:29:40,511 [INFO] __main__: [DEBUG] Inference completed in 111.71s
2026-01-20 04:29:44,019 [INFO] __main__: [DEBUG] Got batch 206, extracting texts...
2026-01-20 04:29:44,020 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:32:04,854 [INFO] __main__: [DEBUG] Inference completed in 140.83s
2026-01-20 04:32:09,733 [INFO] __main__: [DEBUG] Got batch 207, extracting texts...
2026-01-20 04:32:09,734 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:34:00,421 [INFO] __main__: [DEBUG] Inference completed in 110.69s
2026-01-20 04:34:04,044 [INFO] __main__: [DEBUG] Got batch 208, extracting texts...
2026-01-20 04:34:04,044 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:36:21,169 [INFO] __main__: [DEBUG] Inference completed in 137.12s
2026-01-20 04:36:25,479 [INFO] __main__: [DEBUG] Got batch 209, extracting texts...
2026-01-20 04:36:25,480 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:37:29,821 [INFO] __main__: [DEBUG] Inference completed in 64.34s
2026-01-20 04:37:31,782 [INFO] __main__: [DEBUG] Got batch 210, extracting texts...
2026-01-20 04:37:31,782 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:38:46,283 [INFO] __main__: [DEBUG] Inference completed in 74.50s
2026-01-20 04:38:46,283 [INFO] __main__:    Processed 210 batches...
2026-01-20 04:38:46,284 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 210 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 04:39:04,629 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.55 GB reserved
2026-01-20 04:39:04,630 [INFO] __main__: [DEBUG] Got batch 211, extracting texts...
2026-01-20 04:39:04,630 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:40:38,261 [INFO] __main__: [DEBUG] Inference completed in 93.63s
2026-01-20 04:40:41,113 [INFO] __main__: [DEBUG] Got batch 212, extracting texts...
2026-01-20 04:40:41,114 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:41:56,941 [INFO] __main__: [DEBUG] Inference completed in 75.83s
2026-01-20 04:41:59,441 [INFO] __main__: [DEBUG] Got batch 213, extracting texts...
2026-01-20 04:41:59,441 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:43:41,520 [INFO] __main__: [DEBUG] Inference completed in 102.08s
2026-01-20 04:43:44,977 [INFO] __main__: [DEBUG] Got batch 214, extracting texts...
2026-01-20 04:43:44,977 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:44:48,749 [INFO] __main__: [DEBUG] Inference completed in 63.77s
2026-01-20 04:44:50,628 [INFO] __main__: [DEBUG] Got batch 215, extracting texts...
2026-01-20 04:44:50,629 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:46:19,523 [INFO] __main__: [DEBUG] Inference completed in 88.89s
2026-01-20 04:46:22,188 [INFO] __main__: [DEBUG] Got batch 216, extracting texts...
2026-01-20 04:46:22,189 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:47:50,592 [INFO] __main__: [DEBUG] Inference completed in 88.40s
2026-01-20 04:47:53,349 [INFO] __main__: [DEBUG] Got batch 217, extracting texts...
2026-01-20 04:47:53,349 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:50:20,303 [INFO] __main__: [DEBUG] Inference completed in 146.95s
2026-01-20 04:50:24,995 [INFO] __main__: [DEBUG] Got batch 218, extracting texts...
2026-01-20 04:50:24,995 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:52:14,498 [INFO] __main__: [DEBUG] Inference completed in 109.50s
2026-01-20 04:52:18,110 [INFO] __main__: [DEBUG] Got batch 219, extracting texts...
2026-01-20 04:52:18,110 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:53:44,947 [INFO] __main__: [DEBUG] Inference completed in 86.84s
2026-01-20 04:53:47,718 [INFO] __main__: [DEBUG] Got batch 220, extracting texts...
2026-01-20 04:53:47,718 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:55:43,417 [INFO] __main__: [DEBUG] Inference completed in 115.70s
2026-01-20 04:55:43,417 [INFO] __main__:    Processed 220 batches...
2026-01-20 04:55:43,417 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 220 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 04:56:03,359 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.41 GB reserved
2026-01-20 04:56:03,359 [INFO] __main__: [DEBUG] Got batch 221, extracting texts...
2026-01-20 04:56:03,360 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:58:04,219 [INFO] __main__: [DEBUG] Inference completed in 120.86s
2026-01-20 04:58:08,011 [INFO] __main__: [DEBUG] Got batch 222, extracting texts...
2026-01-20 04:58:08,011 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:59:18,712 [INFO] __main__: [DEBUG] Inference completed in 70.70s
2026-01-20 04:59:20,926 [INFO] __main__: [DEBUG] Got batch 223, extracting texts...
2026-01-20 04:59:20,927 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:01:48,682 [INFO] __main__: [DEBUG] Inference completed in 147.75s
2026-01-20 05:01:53,214 [INFO] __main__: [DEBUG] Got batch 224, extracting texts...
2026-01-20 05:01:53,214 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:03:13,855 [INFO] __main__: [DEBUG] Inference completed in 80.64s
2026-01-20 05:03:16,454 [INFO] __main__: [DEBUG] Got batch 225, extracting texts...
2026-01-20 05:03:16,455 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:05:28,103 [INFO] __main__: [DEBUG] Inference completed in 131.65s
2026-01-20 05:05:32,294 [INFO] __main__: [DEBUG] Got batch 226, extracting texts...
2026-01-20 05:05:32,295 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:07:44,362 [INFO] __main__: [DEBUG] Inference completed in 132.07s
2026-01-20 05:07:49,050 [INFO] __main__: [DEBUG] Got batch 227, extracting texts...
2026-01-20 05:07:49,050 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:09:09,262 [INFO] __main__: [DEBUG] Inference completed in 80.21s
2026-01-20 05:09:11,735 [INFO] __main__: [DEBUG] Got batch 228, extracting texts...
2026-01-20 05:09:11,736 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:10:31,272 [INFO] __main__: [DEBUG] Inference completed in 79.54s
2026-01-20 05:10:33,694 [INFO] __main__: [DEBUG] Got batch 229, extracting texts...
2026-01-20 05:10:33,694 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:11:38,848 [INFO] __main__: [DEBUG] Inference completed in 65.15s
2026-01-20 05:11:40,947 [INFO] __main__: [DEBUG] Got batch 230, extracting texts...
2026-01-20 05:11:40,947 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:13:25,883 [INFO] __main__: [DEBUG] Inference completed in 104.94s
2026-01-20 05:13:25,883 [INFO] __main__:    Processed 230 batches...
2026-01-20 05:13:25,883 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 230 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 05:13:46,158 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.49 GB reserved
2026-01-20 05:13:46,175 [INFO] __main__: [DEBUG] Got batch 231, extracting texts...
2026-01-20 05:13:46,175 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:14:53,756 [INFO] __main__: [DEBUG] Inference completed in 67.58s
2026-01-20 05:14:55,940 [INFO] __main__: [DEBUG] Got batch 232, extracting texts...
2026-01-20 05:14:55,941 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:15:59,213 [INFO] __main__: [DEBUG] Inference completed in 63.27s
2026-01-20 05:16:01,052 [INFO] __main__: [DEBUG] Got batch 233, extracting texts...
2026-01-20 05:16:01,052 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:17:37,724 [INFO] __main__: [DEBUG] Inference completed in 96.67s
2026-01-20 05:17:40,748 [INFO] __main__: [DEBUG] Got batch 234, extracting texts...
2026-01-20 05:17:40,748 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:19:33,429 [INFO] __main__: [DEBUG] Inference completed in 112.68s
2026-01-20 05:19:37,080 [INFO] __main__: [DEBUG] Got batch 235, extracting texts...
2026-01-20 05:19:37,081 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:20:43,681 [INFO] __main__: [DEBUG] Inference completed in 66.60s
2026-01-20 05:20:45,860 [INFO] __main__: [DEBUG] Got batch 236, extracting texts...
2026-01-20 05:20:45,860 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:22:21,429 [INFO] __main__: [DEBUG] Inference completed in 95.57s
2026-01-20 05:22:24,638 [INFO] __main__: [DEBUG] Got batch 237, extracting texts...
2026-01-20 05:22:24,639 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:23:31,899 [INFO] __main__: [DEBUG] Inference completed in 67.26s
2026-01-20 05:23:33,609 [INFO] __main__: [DEBUG] Got batch 238, extracting texts...
2026-01-20 05:23:33,610 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:25:02,597 [INFO] __main__: [DEBUG] Inference completed in 88.99s
2026-01-20 05:25:05,332 [INFO] __main__: [DEBUG] Got batch 239, extracting texts...
2026-01-20 05:25:05,333 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:26:32,154 [INFO] __main__: [DEBUG] Inference completed in 86.82s
2026-01-20 05:26:34,825 [INFO] __main__: [DEBUG] Got batch 240, extracting texts...
2026-01-20 05:26:34,826 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:27:58,805 [INFO] __main__: [DEBUG] Inference completed in 83.98s
2026-01-20 05:27:58,805 [INFO] __main__:    Processed 240 batches...
2026-01-20 05:27:58,806 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 240 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 05:28:17,694 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.49 GB reserved
2026-01-20 05:28:17,695 [INFO] __main__: [DEBUG] Got batch 241, extracting texts...
2026-01-20 05:28:17,695 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:29:13,325 [INFO] __main__: [DEBUG] Inference completed in 55.63s
2026-01-20 05:29:14,914 [INFO] __main__: [DEBUG] Got batch 242, extracting texts...
2026-01-20 05:29:14,915 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:30:26,693 [INFO] __main__: [DEBUG] Inference completed in 71.78s
2026-01-20 05:30:29,082 [INFO] __main__: [DEBUG] Got batch 243, extracting texts...
2026-01-20 05:30:29,082 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:31:21,374 [INFO] __main__: [DEBUG] Inference completed in 52.29s
2026-01-20 05:31:22,815 [INFO] __main__: [DEBUG] Got batch 244, extracting texts...
2026-01-20 05:31:22,816 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:32:56,561 [INFO] __main__: [DEBUG] Inference completed in 93.75s
2026-01-20 05:32:59,541 [INFO] __main__: [DEBUG] Got batch 245, extracting texts...
2026-01-20 05:32:59,541 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:34:43,261 [INFO] __main__: [DEBUG] Inference completed in 103.72s
2026-01-20 05:34:46,615 [INFO] __main__: [DEBUG] Got batch 246, extracting texts...
2026-01-20 05:34:46,615 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:36:43,873 [INFO] __main__: [DEBUG] Inference completed in 117.26s
2026-01-20 05:36:47,793 [INFO] __main__: [DEBUG] Got batch 247, extracting texts...
2026-01-20 05:36:47,794 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:38:06,365 [INFO] __main__: [DEBUG] Inference completed in 78.57s
2026-01-20 05:38:08,875 [INFO] __main__: [DEBUG] Got batch 248, extracting texts...
2026-01-20 05:38:08,876 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:41:06,471 [INFO] __main__: [DEBUG] Inference completed in 177.60s
2026-01-20 05:41:12,367 [INFO] __main__: [DEBUG] Got batch 249, extracting texts...
2026-01-20 05:41:12,367 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:42:52,251 [INFO] __main__: [DEBUG] Inference completed in 99.88s
2026-01-20 05:42:55,448 [INFO] __main__: [DEBUG] Got batch 250, extracting texts...
2026-01-20 05:42:55,450 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:44:58,473 [INFO] __main__: [DEBUG] Inference completed in 123.02s
2026-01-20 05:44:58,474 [INFO] __main__:    Processed 250 batches...
2026-01-20 05:44:58,474 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 250 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 05:45:18,766 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.68 GB reserved
2026-01-20 05:45:18,767 [INFO] __main__: [DEBUG] Got batch 251, extracting texts...
2026-01-20 05:45:18,767 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:47:07,051 [INFO] __main__: [DEBUG] Inference completed in 108.28s
2026-01-20 05:47:10,670 [INFO] __main__: [DEBUG] Got batch 252, extracting texts...
2026-01-20 05:47:10,670 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:51:06,271 [INFO] __main__: [DEBUG] Inference completed in 235.60s
2026-01-20 05:51:13,886 [INFO] __main__: [DEBUG] Got batch 253, extracting texts...
2026-01-20 05:51:13,887 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:52:57,978 [INFO] __main__: [DEBUG] Inference completed in 104.09s
2026-01-20 05:53:01,016 [INFO] __main__: [DEBUG] Got batch 254, extracting texts...
2026-01-20 05:53:01,017 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:54:08,180 [INFO] __main__: [DEBUG] Inference completed in 67.16s
2026-01-20 05:54:10,342 [INFO] __main__: [DEBUG] Got batch 255, extracting texts...
2026-01-20 05:54:10,343 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:56:01,036 [INFO] __main__: [DEBUG] Inference completed in 110.69s
2026-01-20 05:56:04,632 [INFO] __main__: [DEBUG] Got batch 256, extracting texts...
2026-01-20 05:56:04,633 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:57:24,454 [INFO] __main__: [DEBUG] Inference completed in 79.82s
2026-01-20 05:57:26,935 [INFO] __main__: [DEBUG] Got batch 257, extracting texts...
2026-01-20 05:57:26,936 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:59:55,845 [INFO] __main__: [DEBUG] Inference completed in 148.91s
2026-01-20 06:00:01,067 [INFO] __main__: [DEBUG] Got batch 258, extracting texts...
2026-01-20 06:00:01,067 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:01:06,935 [INFO] __main__: [DEBUG] Inference completed in 65.87s
2026-01-20 06:01:08,851 [INFO] __main__: [DEBUG] Got batch 259, extracting texts...
2026-01-20 06:01:08,852 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:03:11,945 [INFO] __main__: [DEBUG] Inference completed in 123.09s
2026-01-20 06:03:15,779 [INFO] __main__: [DEBUG] Got batch 260, extracting texts...
2026-01-20 06:03:15,779 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:04:36,827 [INFO] __main__: [DEBUG] Inference completed in 81.05s
2026-01-20 06:04:36,828 [INFO] __main__:    Processed 260 batches...
2026-01-20 06:04:36,828 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 260 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 06:04:55,088 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.48 GB reserved
2026-01-20 06:04:55,089 [INFO] __main__: [DEBUG] Got batch 261, extracting texts...
2026-01-20 06:04:55,089 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:06:10,092 [INFO] __main__: [DEBUG] Inference completed in 75.00s
2026-01-20 06:06:12,484 [INFO] __main__: [DEBUG] Got batch 262, extracting texts...
2026-01-20 06:06:12,485 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:07:20,031 [INFO] __main__: [DEBUG] Inference completed in 67.55s
2026-01-20 06:07:22,176 [INFO] __main__: [DEBUG] Got batch 263, extracting texts...
2026-01-20 06:07:22,177 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:08:58,478 [INFO] __main__: [DEBUG] Inference completed in 96.30s
2026-01-20 06:09:01,666 [INFO] __main__: [DEBUG] Got batch 264, extracting texts...
2026-01-20 06:09:01,666 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:09:53,739 [INFO] __main__: [DEBUG] Inference completed in 52.07s
2026-01-20 06:09:55,239 [INFO] __main__: [DEBUG] Got batch 265, extracting texts...
2026-01-20 06:09:55,239 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:11:37,450 [INFO] __main__: [DEBUG] Inference completed in 102.21s
2026-01-20 06:11:40,633 [INFO] __main__: [DEBUG] Got batch 266, extracting texts...
2026-01-20 06:11:40,634 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:13:49,098 [INFO] __main__: [DEBUG] Inference completed in 128.46s
2026-01-20 06:13:53,005 [INFO] __main__: [DEBUG] Got batch 267, extracting texts...
2026-01-20 06:13:53,006 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:15:12,879 [INFO] __main__: [DEBUG] Inference completed in 79.87s
2026-01-20 06:15:15,353 [INFO] __main__: [DEBUG] Got batch 268, extracting texts...
2026-01-20 06:15:15,353 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:17:24,733 [INFO] __main__: [DEBUG] Inference completed in 129.38s
2026-01-20 06:17:29,112 [INFO] __main__: [DEBUG] Got batch 269, extracting texts...
2026-01-20 06:17:29,112 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:18:20,737 [INFO] __main__: [DEBUG] Inference completed in 51.62s
2026-01-20 06:18:22,328 [INFO] __main__: [DEBUG] Got batch 270, extracting texts...
2026-01-20 06:18:22,328 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:19:50,207 [INFO] __main__: [DEBUG] Inference completed in 87.88s
2026-01-20 06:19:50,207 [INFO] __main__:    Processed 270 batches...
2026-01-20 06:19:50,207 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 270 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 06:20:09,037 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.50 GB reserved
2026-01-20 06:20:09,038 [INFO] __main__: [DEBUG] Got batch 271, extracting texts...
2026-01-20 06:20:09,038 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:21:59,272 [INFO] __main__: [DEBUG] Inference completed in 110.23s
2026-01-20 06:22:03,017 [INFO] __main__: [DEBUG] Got batch 272, extracting texts...
2026-01-20 06:22:03,018 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:23:46,539 [INFO] __main__: [DEBUG] Inference completed in 103.52s
2026-01-20 06:23:50,003 [INFO] __main__: [DEBUG] Got batch 273, extracting texts...
2026-01-20 06:23:50,003 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:24:51,045 [INFO] __main__: [DEBUG] Inference completed in 61.04s
2026-01-20 06:24:52,757 [INFO] __main__: [DEBUG] Got batch 274, extracting texts...
2026-01-20 06:24:52,757 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:26:27,092 [INFO] __main__: [DEBUG] Inference completed in 94.33s
2026-01-20 06:26:30,124 [INFO] __main__: [DEBUG] Got batch 275, extracting texts...
2026-01-20 06:26:30,124 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:27:24,960 [INFO] __main__: [DEBUG] Inference completed in 54.84s
2026-01-20 06:27:26,632 [INFO] __main__: [DEBUG] Got batch 276, extracting texts...
2026-01-20 06:27:26,633 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:28:20,361 [INFO] __main__: [DEBUG] Inference completed in 53.73s
2026-01-20 06:28:21,835 [INFO] __main__: [DEBUG] Got batch 277, extracting texts...
2026-01-20 06:28:21,835 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:29:58,837 [INFO] __main__: [DEBUG] Inference completed in 97.00s
2026-01-20 06:30:01,887 [INFO] __main__: [DEBUG] Got batch 278, extracting texts...
2026-01-20 06:30:01,888 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:31:10,660 [INFO] __main__: [DEBUG] Inference completed in 68.77s
2026-01-20 06:31:12,780 [INFO] __main__: [DEBUG] Got batch 279, extracting texts...
2026-01-20 06:31:12,780 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:32:38,630 [INFO] __main__: [DEBUG] Inference completed in 85.85s
2026-01-20 06:32:41,265 [INFO] __main__: [DEBUG] Got batch 280, extracting texts...
2026-01-20 06:32:41,266 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:33:35,886 [INFO] __main__: [DEBUG] Inference completed in 54.62s
2026-01-20 06:33:35,886 [INFO] __main__:    Processed 280 batches...
2026-01-20 06:33:35,886 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 280 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 06:33:53,250 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.40 GB reserved
2026-01-20 06:33:53,251 [INFO] __main__: [DEBUG] Got batch 281, extracting texts...
2026-01-20 06:33:53,251 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:35:31,058 [INFO] __main__: [DEBUG] Inference completed in 97.81s
2026-01-20 06:35:34,324 [INFO] __main__: [DEBUG] Got batch 282, extracting texts...
2026-01-20 06:35:34,325 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:36:52,487 [INFO] __main__: [DEBUG] Inference completed in 78.16s
2026-01-20 06:36:54,415 [INFO] __main__: [DEBUG] Got batch 283, extracting texts...
2026-01-20 06:36:54,415 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:38:27,016 [INFO] __main__: [DEBUG] Inference completed in 92.60s
2026-01-20 06:38:29,885 [INFO] __main__: [DEBUG] Got batch 284, extracting texts...
2026-01-20 06:38:29,886 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:39:55,202 [INFO] __main__: [DEBUG] Inference completed in 85.32s
2026-01-20 06:39:57,793 [INFO] __main__: [DEBUG] Got batch 285, extracting texts...
2026-01-20 06:39:57,794 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:42:21,182 [INFO] __main__: [DEBUG] Inference completed in 143.39s
2026-01-20 06:42:25,553 [INFO] __main__: [DEBUG] Got batch 286, extracting texts...
2026-01-20 06:42:25,554 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:43:41,288 [INFO] __main__: [DEBUG] Inference completed in 75.73s
2026-01-20 06:43:43,758 [INFO] __main__: [DEBUG] Got batch 287, extracting texts...
2026-01-20 06:43:43,758 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:45:06,478 [INFO] __main__: [DEBUG] Inference completed in 82.72s
2026-01-20 06:45:09,004 [INFO] __main__: [DEBUG] Got batch 288, extracting texts...
2026-01-20 06:45:09,004 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:46:39,412 [INFO] __main__: [DEBUG] Inference completed in 90.41s
2026-01-20 06:46:42,523 [INFO] __main__: [DEBUG] Got batch 289, extracting texts...
2026-01-20 06:46:42,524 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:48:33,394 [INFO] __main__: [DEBUG] Inference completed in 110.87s
2026-01-20 06:48:37,073 [INFO] __main__: [DEBUG] Got batch 290, extracting texts...
2026-01-20 06:48:37,073 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:50:07,569 [INFO] __main__: [DEBUG] Inference completed in 90.50s
2026-01-20 06:50:07,569 [INFO] __main__:    Processed 290 batches...
2026-01-20 06:50:07,569 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 290 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 06:50:26,221 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.50 GB reserved
2026-01-20 06:50:26,222 [INFO] __main__: [DEBUG] Got batch 291, extracting texts...
2026-01-20 06:50:26,222 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:52:12,627 [INFO] __main__: [DEBUG] Inference completed in 106.40s
2026-01-20 06:52:16,014 [INFO] __main__: [DEBUG] Got batch 292, extracting texts...
2026-01-20 06:52:16,015 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:55:03,146 [INFO] __main__: [DEBUG] Inference completed in 167.13s
2026-01-20 06:55:08,606 [INFO] __main__: [DEBUG] Got batch 293, extracting texts...
2026-01-20 06:55:08,606 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:56:39,057 [INFO] __main__: [DEBUG] Inference completed in 90.45s
2026-01-20 06:56:42,087 [INFO] __main__: [DEBUG] Got batch 294, extracting texts...
2026-01-20 06:56:42,087 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:58:44,694 [INFO] __main__: [DEBUG] Inference completed in 122.61s
2026-01-20 06:58:48,455 [INFO] __main__: [DEBUG] Got batch 295, extracting texts...
2026-01-20 06:58:48,456 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:00:28,170 [INFO] __main__: [DEBUG] Inference completed in 99.71s
2026-01-20 07:00:31,497 [INFO] __main__: [DEBUG] Got batch 296, extracting texts...
2026-01-20 07:00:31,498 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:02:06,448 [INFO] __main__: [DEBUG] Inference completed in 94.95s
2026-01-20 07:02:09,721 [INFO] __main__: [DEBUG] Got batch 297, extracting texts...
2026-01-20 07:02:09,721 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:03:30,918 [INFO] __main__: [DEBUG] Inference completed in 81.20s
2026-01-20 07:03:33,386 [INFO] __main__: [DEBUG] Got batch 298, extracting texts...
2026-01-20 07:03:33,387 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:04:37,442 [INFO] __main__: [DEBUG] Inference completed in 64.05s
2026-01-20 07:04:39,496 [INFO] __main__: [DEBUG] Got batch 299, extracting texts...
2026-01-20 07:04:39,497 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:05:57,645 [INFO] __main__: [DEBUG] Inference completed in 78.15s
2026-01-20 07:06:00,180 [INFO] __main__: [DEBUG] Got batch 300, extracting texts...
2026-01-20 07:06:00,181 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:07:05,966 [INFO] __main__: [DEBUG] Inference completed in 65.79s
2026-01-20 07:07:05,967 [INFO] __main__:    Processed 300 batches...
2026-01-20 07:07:05,967 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 300 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 07:07:24,145 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.43 GB reserved
2026-01-20 07:07:24,184 [INFO] __main__: [DEBUG] Got batch 301, extracting texts...
2026-01-20 07:07:24,185 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:09:41,369 [INFO] __main__: [DEBUG] Inference completed in 137.18s
2026-01-20 07:09:45,607 [INFO] __main__: [DEBUG] Got batch 302, extracting texts...
2026-01-20 07:09:45,608 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:10:52,377 [INFO] __main__: [DEBUG] Inference completed in 66.77s
2026-01-20 07:10:54,588 [INFO] __main__: [DEBUG] Got batch 303, extracting texts...
2026-01-20 07:10:54,588 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:13:05,889 [INFO] __main__: [DEBUG] Inference completed in 131.30s
2026-01-20 07:13:09,993 [INFO] __main__: [DEBUG] Got batch 304, extracting texts...
2026-01-20 07:13:09,993 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:15:02,248 [INFO] __main__: [DEBUG] Inference completed in 112.26s
2026-01-20 07:15:05,967 [INFO] __main__: [DEBUG] Got batch 305, extracting texts...
2026-01-20 07:15:05,968 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:16:40,900 [INFO] __main__: [DEBUG] Inference completed in 94.93s
2026-01-20 07:16:43,953 [INFO] __main__: [DEBUG] Got batch 306, extracting texts...
2026-01-20 07:16:43,953 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:18:04,007 [INFO] __main__: [DEBUG] Inference completed in 80.05s
2026-01-20 07:18:06,496 [INFO] __main__: [DEBUG] Got batch 307, extracting texts...
2026-01-20 07:18:06,497 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:20:15,762 [INFO] __main__: [DEBUG] Inference completed in 129.27s
2026-01-20 07:20:19,794 [INFO] __main__: [DEBUG] Got batch 308, extracting texts...
2026-01-20 07:20:19,794 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:21:51,153 [INFO] __main__: [DEBUG] Inference completed in 91.36s
2026-01-20 07:21:54,041 [INFO] __main__: [DEBUG] Got batch 309, extracting texts...
2026-01-20 07:21:54,042 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:22:48,799 [INFO] __main__: [DEBUG] Inference completed in 54.76s
2026-01-20 07:22:50,552 [INFO] __main__: [DEBUG] Got batch 310, extracting texts...
2026-01-20 07:22:50,553 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:24:06,881 [INFO] __main__: [DEBUG] Inference completed in 76.33s
2026-01-20 07:24:06,882 [INFO] __main__:    Processed 310 batches...
2026-01-20 07:24:06,882 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 310 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 07:24:25,607 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.43 GB reserved
2026-01-20 07:24:25,608 [INFO] __main__: [DEBUG] Got batch 311, extracting texts...
2026-01-20 07:24:25,608 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:26:17,194 [INFO] __main__: [DEBUG] Inference completed in 111.59s
2026-01-20 07:26:20,788 [INFO] __main__: [DEBUG] Got batch 312, extracting texts...
2026-01-20 07:26:20,789 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:28:03,516 [INFO] __main__: [DEBUG] Inference completed in 102.73s
2026-01-20 07:28:06,906 [INFO] __main__: [DEBUG] Got batch 313, extracting texts...
2026-01-20 07:28:06,906 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:29:35,708 [INFO] __main__: [DEBUG] Inference completed in 88.80s
2026-01-20 07:29:38,617 [INFO] __main__: [DEBUG] Got batch 314, extracting texts...
2026-01-20 07:29:38,618 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:31:08,142 [INFO] __main__: [DEBUG] Inference completed in 89.52s
2026-01-20 07:31:11,099 [INFO] __main__: [DEBUG] Got batch 315, extracting texts...
2026-01-20 07:31:11,100 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:32:45,209 [INFO] __main__: [DEBUG] Inference completed in 94.11s
2026-01-20 07:32:48,303 [INFO] __main__: [DEBUG] Got batch 316, extracting texts...
2026-01-20 07:32:48,304 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:34:43,020 [INFO] __main__: [DEBUG] Inference completed in 114.72s
2026-01-20 07:34:46,888 [INFO] __main__: [DEBUG] Got batch 317, extracting texts...
2026-01-20 07:34:46,888 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:36:15,438 [INFO] __main__: [DEBUG] Inference completed in 88.55s
2026-01-20 07:36:18,269 [INFO] __main__: [DEBUG] Got batch 318, extracting texts...
2026-01-20 07:36:18,270 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:37:32,853 [INFO] __main__: [DEBUG] Inference completed in 74.58s
2026-01-20 07:37:35,294 [INFO] __main__: [DEBUG] Got batch 319, extracting texts...
2026-01-20 07:37:35,294 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:39:17,229 [INFO] __main__: [DEBUG] Inference completed in 101.93s
2026-01-20 07:39:20,518 [INFO] __main__: [DEBUG] Got batch 320, extracting texts...
2026-01-20 07:39:20,518 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:40:51,224 [INFO] __main__: [DEBUG] Inference completed in 90.71s
2026-01-20 07:40:51,224 [INFO] __main__:    Processed 320 batches...
2026-01-20 07:40:51,224 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 320 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 07:41:10,322 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.51 GB reserved
2026-01-20 07:41:10,322 [INFO] __main__: [DEBUG] Got batch 321, extracting texts...
2026-01-20 07:41:10,322 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:42:15,429 [INFO] __main__: [DEBUG] Inference completed in 65.11s
2026-01-20 07:42:17,235 [INFO] __main__: [DEBUG] Got batch 322, extracting texts...
2026-01-20 07:42:17,235 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:44:14,615 [INFO] __main__: [DEBUG] Inference completed in 117.38s
2026-01-20 07:44:18,241 [INFO] __main__: [DEBUG] Got batch 323, extracting texts...
2026-01-20 07:44:18,241 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:45:44,636 [INFO] __main__: [DEBUG] Inference completed in 86.39s
2026-01-20 07:45:47,390 [INFO] __main__: [DEBUG] Got batch 324, extracting texts...
2026-01-20 07:45:47,390 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:47:49,948 [INFO] __main__: [DEBUG] Inference completed in 122.56s
2026-01-20 07:47:54,001 [INFO] __main__: [DEBUG] Got batch 325, extracting texts...
2026-01-20 07:47:54,001 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:49:26,984 [INFO] __main__: [DEBUG] Inference completed in 92.98s
2026-01-20 07:49:29,902 [INFO] __main__: [DEBUG] Got batch 326, extracting texts...
2026-01-20 07:49:29,902 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:50:45,416 [INFO] __main__: [DEBUG] Inference completed in 75.51s
2026-01-20 07:50:47,936 [INFO] __main__: [DEBUG] Got batch 327, extracting texts...
2026-01-20 07:50:47,937 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:52:12,213 [INFO] __main__: [DEBUG] Inference completed in 84.28s
2026-01-20 07:52:14,892 [INFO] __main__: [DEBUG] Got batch 328, extracting texts...
2026-01-20 07:52:14,893 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:53:40,103 [INFO] __main__: [DEBUG] Inference completed in 85.21s
2026-01-20 07:53:42,797 [INFO] __main__: [DEBUG] Got batch 329, extracting texts...
2026-01-20 07:53:42,798 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:56:16,675 [INFO] __main__: [DEBUG] Inference completed in 153.88s
2026-01-20 07:56:21,567 [INFO] __main__: [DEBUG] Got batch 330, extracting texts...
2026-01-20 07:56:21,568 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:57:23,195 [INFO] __main__: [DEBUG] Inference completed in 61.63s
2026-01-20 07:57:23,195 [INFO] __main__:    Processed 330 batches...
2026-01-20 07:57:23,195 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 330 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 07:57:41,053 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.42 GB reserved
2026-01-20 07:57:41,054 [INFO] __main__: [DEBUG] Got batch 331, extracting texts...
2026-01-20 07:57:41,054 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:58:52,255 [INFO] __main__: [DEBUG] Inference completed in 71.20s
2026-01-20 07:58:54,547 [INFO] __main__: [DEBUG] Got batch 332, extracting texts...
2026-01-20 07:58:54,548 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:00:38,168 [INFO] __main__: [DEBUG] Inference completed in 103.62s
2026-01-20 08:00:41,433 [INFO] __main__: [DEBUG] Got batch 333, extracting texts...
2026-01-20 08:00:41,433 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:02:06,957 [INFO] __main__: [DEBUG] Inference completed in 85.52s
2026-01-20 08:02:09,641 [INFO] __main__: [DEBUG] Got batch 334, extracting texts...
2026-01-20 08:02:09,641 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:03:16,012 [INFO] __main__: [DEBUG] Inference completed in 66.37s
2026-01-20 08:03:18,127 [INFO] __main__: [DEBUG] Got batch 335, extracting texts...
2026-01-20 08:03:18,127 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:05:18,782 [INFO] __main__: [DEBUG] Inference completed in 120.65s
2026-01-20 08:05:22,772 [INFO] __main__: [DEBUG] Got batch 336, extracting texts...
2026-01-20 08:05:22,773 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:06:32,600 [INFO] __main__: [DEBUG] Inference completed in 69.83s
2026-01-20 08:06:34,359 [INFO] __main__: [DEBUG] Got batch 337, extracting texts...
2026-01-20 08:06:34,359 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:08:36,388 [INFO] __main__: [DEBUG] Inference completed in 122.03s
2026-01-20 08:08:40,394 [INFO] __main__: [DEBUG] Got batch 338, extracting texts...
2026-01-20 08:08:40,395 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:10:23,783 [INFO] __main__: [DEBUG] Inference completed in 103.39s
2026-01-20 08:10:27,142 [INFO] __main__: [DEBUG] Got batch 339, extracting texts...
2026-01-20 08:10:27,143 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:11:19,038 [INFO] __main__: [DEBUG] Inference completed in 51.90s
2026-01-20 08:11:20,357 [INFO] __main__: [DEBUG] Got batch 340, extracting texts...
2026-01-20 08:11:20,357 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:13:33,807 [INFO] __main__: [DEBUG] Inference completed in 133.45s
2026-01-20 08:13:33,808 [INFO] __main__:    Processed 340 batches...
2026-01-20 08:13:33,808 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 340 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 08:13:55,335 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.43 GB reserved
2026-01-20 08:13:55,336 [INFO] __main__: [DEBUG] Got batch 341, extracting texts...
2026-01-20 08:13:55,336 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:15:03,010 [INFO] __main__: [DEBUG] Inference completed in 67.67s
2026-01-20 08:15:04,659 [INFO] __main__: [DEBUG] Got batch 342, extracting texts...
2026-01-20 08:15:04,659 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:16:37,890 [INFO] __main__: [DEBUG] Inference completed in 93.23s
2026-01-20 08:16:40,728 [INFO] __main__: [DEBUG] Got batch 343, extracting texts...
2026-01-20 08:16:40,729 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:18:28,808 [INFO] __main__: [DEBUG] Inference completed in 108.08s
2026-01-20 08:18:32,459 [INFO] __main__: [DEBUG] Got batch 344, extracting texts...
2026-01-20 08:18:32,460 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:19:26,089 [INFO] __main__: [DEBUG] Inference completed in 53.63s
2026-01-20 08:19:27,653 [INFO] __main__: [DEBUG] Got batch 345, extracting texts...
2026-01-20 08:19:27,654 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:20:42,215 [INFO] __main__: [DEBUG] Inference completed in 74.56s
2026-01-20 08:20:44,294 [INFO] __main__: [DEBUG] Got batch 346, extracting texts...
2026-01-20 08:20:44,294 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:22:08,577 [INFO] __main__: [DEBUG] Inference completed in 84.28s
2026-01-20 08:22:11,229 [INFO] __main__: [DEBUG] Got batch 347, extracting texts...
2026-01-20 08:22:11,229 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:24:14,199 [INFO] __main__: [DEBUG] Inference completed in 122.97s
2026-01-20 08:24:17,991 [INFO] __main__: [DEBUG] Got batch 348, extracting texts...
2026-01-20 08:24:17,992 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:25:20,410 [INFO] __main__: [DEBUG] Inference completed in 62.42s
2026-01-20 08:25:22,565 [INFO] __main__: [DEBUG] Got batch 349, extracting texts...
2026-01-20 08:25:22,566 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:27:20,697 [INFO] __main__: [DEBUG] Inference completed in 118.13s
2026-01-20 08:27:24,442 [INFO] __main__: [DEBUG] Got batch 350, extracting texts...
2026-01-20 08:27:24,443 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:30:59,251 [INFO] __main__: [DEBUG] Inference completed in 214.81s
2026-01-20 08:30:59,251 [INFO] __main__:    Processed 350 batches...
2026-01-20 08:30:59,251 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 350 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 08:31:24,541 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.88 GB reserved
2026-01-20 08:31:24,542 [INFO] __main__: [DEBUG] Got batch 351, extracting texts...
2026-01-20 08:31:24,542 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:33:18,769 [INFO] __main__: [DEBUG] Inference completed in 114.23s
2026-01-20 08:33:22,308 [INFO] __main__: [DEBUG] Got batch 352, extracting texts...
2026-01-20 08:33:22,308 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:34:26,447 [INFO] __main__: [DEBUG] Inference completed in 64.14s
2026-01-20 08:34:28,159 [INFO] __main__: [DEBUG] Got batch 353, extracting texts...
2026-01-20 08:34:28,160 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:35:32,404 [INFO] __main__: [DEBUG] Inference completed in 64.24s
2026-01-20 08:35:34,489 [INFO] __main__: [DEBUG] Got batch 354, extracting texts...
2026-01-20 08:35:34,489 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:39:18,919 [INFO] __main__: [DEBUG] Inference completed in 224.43s
2026-01-20 08:39:26,121 [INFO] __main__: [DEBUG] Got batch 355, extracting texts...
2026-01-20 08:39:26,123 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:42:23,429 [INFO] __main__: [DEBUG] Inference completed in 177.31s
2026-01-20 08:42:29,252 [INFO] __main__: [DEBUG] Got batch 356, extracting texts...
2026-01-20 08:42:29,253 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:44:30,344 [INFO] __main__: [DEBUG] Inference completed in 121.09s
2026-01-20 08:44:34,249 [INFO] __main__: [DEBUG] Got batch 357, extracting texts...
2026-01-20 08:44:34,249 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:46:03,799 [INFO] __main__: [DEBUG] Inference completed in 89.55s
2026-01-20 08:46:06,572 [INFO] __main__: [DEBUG] Got batch 358, extracting texts...
2026-01-20 08:46:06,572 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:47:27,407 [INFO] __main__: [DEBUG] Inference completed in 80.84s
2026-01-20 08:47:30,010 [INFO] __main__: [DEBUG] Got batch 359, extracting texts...
2026-01-20 08:47:30,011 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:49:02,515 [INFO] __main__: [DEBUG] Inference completed in 92.50s
2026-01-20 08:49:05,638 [INFO] __main__: [DEBUG] Got batch 360, extracting texts...
2026-01-20 08:49:05,638 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:49:50,621 [INFO] __main__: [DEBUG] Inference completed in 44.98s
2026-01-20 08:49:50,622 [INFO] __main__:    Processed 360 batches...
2026-01-20 08:49:50,622 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 360 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 08:50:07,483 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.44 GB reserved
2026-01-20 08:50:07,484 [INFO] __main__: [DEBUG] Got batch 361, extracting texts...
2026-01-20 08:50:07,484 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:51:56,841 [INFO] __main__: [DEBUG] Inference completed in 109.36s
2026-01-20 08:52:00,311 [INFO] __main__: [DEBUG] Got batch 362, extracting texts...
2026-01-20 08:52:00,312 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:54:16,472 [INFO] __main__: [DEBUG] Inference completed in 136.16s
2026-01-20 08:54:21,195 [INFO] __main__: [DEBUG] Got batch 363, extracting texts...
2026-01-20 08:54:21,195 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:56:07,311 [INFO] __main__: [DEBUG] Inference completed in 106.12s
2026-01-20 08:56:10,617 [INFO] __main__: [DEBUG] Got batch 364, extracting texts...
2026-01-20 08:56:10,618 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:58:46,699 [INFO] __main__: [DEBUG] Inference completed in 156.08s
2026-01-20 08:58:51,975 [INFO] __main__: [DEBUG] Got batch 365, extracting texts...
2026-01-20 08:58:51,976 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:00:06,958 [INFO] __main__: [DEBUG] Inference completed in 74.98s
2026-01-20 09:00:09,327 [INFO] __main__: [DEBUG] Got batch 366, extracting texts...
2026-01-20 09:00:09,328 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:01:52,714 [INFO] __main__: [DEBUG] Inference completed in 103.39s
2026-01-20 09:01:55,983 [INFO] __main__: [DEBUG] Got batch 367, extracting texts...
2026-01-20 09:01:55,983 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:03:08,996 [INFO] __main__: [DEBUG] Inference completed in 73.01s
2026-01-20 09:03:11,166 [INFO] __main__: [DEBUG] Got batch 368, extracting texts...
2026-01-20 09:03:11,166 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:04:35,575 [INFO] __main__: [DEBUG] Inference completed in 84.41s
2026-01-20 09:04:38,325 [INFO] __main__: [DEBUG] Got batch 369, extracting texts...
2026-01-20 09:04:38,325 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:07:02,706 [INFO] __main__: [DEBUG] Inference completed in 144.38s
2026-01-20 09:07:07,813 [INFO] __main__: [DEBUG] Got batch 370, extracting texts...
2026-01-20 09:07:07,814 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:09:37,531 [INFO] __main__: [DEBUG] Inference completed in 149.72s
2026-01-20 09:09:37,531 [INFO] __main__:    Processed 370 batches...
2026-01-20 09:09:37,531 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 370 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 09:09:59,249 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.46 GB reserved
2026-01-20 09:09:59,250 [INFO] __main__: [DEBUG] Got batch 371, extracting texts...
2026-01-20 09:09:59,250 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:12:20,653 [INFO] __main__: [DEBUG] Inference completed in 141.40s
2026-01-20 09:12:25,761 [INFO] __main__: [DEBUG] Got batch 372, extracting texts...
2026-01-20 09:12:25,761 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:13:46,414 [INFO] __main__: [DEBUG] Inference completed in 80.65s
2026-01-20 09:13:48,949 [INFO] __main__: [DEBUG] Got batch 373, extracting texts...
2026-01-20 09:13:48,949 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:15:01,079 [INFO] __main__: [DEBUG] Inference completed in 72.13s
2026-01-20 09:15:03,513 [INFO] __main__: [DEBUG] Got batch 374, extracting texts...
2026-01-20 09:15:03,513 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:16:06,130 [INFO] __main__: [DEBUG] Inference completed in 62.62s
2026-01-20 09:16:08,130 [INFO] __main__: [DEBUG] Got batch 375, extracting texts...
2026-01-20 09:16:08,131 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:17:11,582 [INFO] __main__: [DEBUG] Inference completed in 63.45s
2026-01-20 09:17:13,670 [INFO] __main__: [DEBUG] Got batch 376, extracting texts...
2026-01-20 09:17:13,670 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:18:36,106 [INFO] __main__: [DEBUG] Inference completed in 82.44s
2026-01-20 09:18:38,591 [INFO] __main__: [DEBUG] Got batch 377, extracting texts...
2026-01-20 09:18:38,591 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:20:43,636 [INFO] __main__: [DEBUG] Inference completed in 125.04s
2026-01-20 09:20:47,717 [INFO] __main__: [DEBUG] Got batch 378, extracting texts...
2026-01-20 09:20:47,717 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:21:57,404 [INFO] __main__: [DEBUG] Inference completed in 69.69s
2026-01-20 09:21:59,225 [INFO] __main__: [DEBUG] Got batch 379, extracting texts...
2026-01-20 09:21:59,225 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:24:08,819 [INFO] __main__: [DEBUG] Inference completed in 129.59s
2026-01-20 09:24:12,999 [INFO] __main__: [DEBUG] Got batch 380, extracting texts...
2026-01-20 09:24:13,000 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:25:37,976 [INFO] __main__: [DEBUG] Inference completed in 84.98s
2026-01-20 09:25:37,977 [INFO] __main__:    Processed 380 batches...
2026-01-20 09:25:37,978 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 380 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 09:25:56,843 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.49 GB reserved
2026-01-20 09:25:56,847 [INFO] __main__: [DEBUG] Got batch 381, extracting texts...
2026-01-20 09:25:56,847 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:26:55,215 [INFO] __main__: [DEBUG] Inference completed in 58.37s
2026-01-20 09:26:57,001 [INFO] __main__: [DEBUG] Got batch 382, extracting texts...
2026-01-20 09:26:57,002 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:28:23,893 [INFO] __main__: [DEBUG] Inference completed in 86.89s
2026-01-20 09:28:26,583 [INFO] __main__: [DEBUG] Got batch 383, extracting texts...
2026-01-20 09:28:26,583 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:29:50,673 [INFO] __main__: [DEBUG] Inference completed in 84.09s
2026-01-20 09:29:53,232 [INFO] __main__: [DEBUG] Got batch 384, extracting texts...
2026-01-20 09:29:53,233 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:31:02,060 [INFO] __main__: [DEBUG] Inference completed in 68.83s
2026-01-20 09:31:04,322 [INFO] __main__: [DEBUG] Got batch 385, extracting texts...
2026-01-20 09:31:04,322 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:32:33,618 [INFO] __main__: [DEBUG] Inference completed in 89.30s
2026-01-20 09:32:36,535 [INFO] __main__: [DEBUG] Got batch 386, extracting texts...
2026-01-20 09:32:36,536 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:34:12,534 [INFO] __main__: [DEBUG] Inference completed in 96.00s
2026-01-20 09:34:15,665 [INFO] __main__: [DEBUG] Got batch 387, extracting texts...
2026-01-20 09:34:15,665 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:36:50,563 [INFO] __main__: [DEBUG] Inference completed in 154.90s
2026-01-20 09:36:55,895 [INFO] __main__: [DEBUG] Got batch 388, extracting texts...
2026-01-20 09:36:55,895 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:37:50,436 [INFO] __main__: [DEBUG] Inference completed in 54.54s
2026-01-20 09:37:51,946 [INFO] __main__: [DEBUG] Got batch 389, extracting texts...
2026-01-20 09:37:51,947 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:39:36,795 [INFO] __main__: [DEBUG] Inference completed in 104.85s
2026-01-20 09:39:40,160 [INFO] __main__: [DEBUG] Got batch 390, extracting texts...
2026-01-20 09:39:40,162 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:42:11,747 [INFO] __main__: [DEBUG] Inference completed in 151.58s
2026-01-20 09:42:11,747 [INFO] __main__:    Processed 390 batches...
2026-01-20 09:42:11,747 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 390 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 09:42:33,605 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.69 GB reserved
2026-01-20 09:42:33,606 [INFO] __main__: [DEBUG] Got batch 391, extracting texts...
2026-01-20 09:42:33,606 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:43:40,761 [INFO] __main__: [DEBUG] Inference completed in 67.15s
2026-01-20 09:43:42,985 [INFO] __main__: [DEBUG] Got batch 392, extracting texts...
2026-01-20 09:43:42,986 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:45:11,302 [INFO] __main__: [DEBUG] Inference completed in 88.32s
2026-01-20 09:45:14,139 [INFO] __main__: [DEBUG] Got batch 393, extracting texts...
2026-01-20 09:45:14,139 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:47:29,887 [INFO] __main__: [DEBUG] Inference completed in 135.75s
2026-01-20 09:47:33,977 [INFO] __main__: [DEBUG] Got batch 394, extracting texts...
2026-01-20 09:47:33,978 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:48:46,504 [INFO] __main__: [DEBUG] Inference completed in 72.53s
2026-01-20 09:48:48,923 [INFO] __main__: [DEBUG] Got batch 395, extracting texts...
2026-01-20 09:48:48,923 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:49:43,938 [INFO] __main__: [DEBUG] Inference completed in 55.01s
2026-01-20 09:49:45,412 [INFO] __main__: [DEBUG] Got batch 396, extracting texts...
2026-01-20 09:49:45,412 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:51:28,940 [INFO] __main__: [DEBUG] Inference completed in 103.53s
2026-01-20 09:51:32,326 [INFO] __main__: [DEBUG] Got batch 397, extracting texts...
2026-01-20 09:51:32,327 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:52:48,089 [INFO] __main__: [DEBUG] Inference completed in 75.76s
2026-01-20 09:52:50,166 [INFO] __main__: [DEBUG] Got batch 398, extracting texts...
2026-01-20 09:52:50,166 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:54:22,287 [INFO] __main__: [DEBUG] Inference completed in 92.12s
2026-01-20 09:54:25,260 [INFO] __main__: [DEBUG] Got batch 399, extracting texts...
2026-01-20 09:54:25,260 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:55:53,358 [INFO] __main__: [DEBUG] Inference completed in 88.10s
2026-01-20 09:55:56,390 [INFO] __main__: [DEBUG] Got batch 400, extracting texts...
2026-01-20 09:55:56,391 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:57:09,345 [INFO] __main__: [DEBUG] Inference completed in 72.95s
2026-01-20 09:57:09,346 [INFO] __main__:    Processed 400 batches...
2026-01-20 09:57:09,346 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 400 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 09:57:27,972 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.48 GB reserved
2026-01-20 09:57:27,974 [INFO] __main__: [DEBUG] Got batch 401, extracting texts...
2026-01-20 09:57:27,975 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:58:28,785 [INFO] __main__: [DEBUG] Inference completed in 60.81s
2026-01-20 09:58:30,566 [INFO] __main__: [DEBUG] Got batch 402, extracting texts...
2026-01-20 09:58:30,567 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 09:59:52,613 [INFO] __main__: [DEBUG] Inference completed in 82.05s
2026-01-20 09:59:55,145 [INFO] __main__: [DEBUG] Got batch 403, extracting texts...
2026-01-20 09:59:55,146 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:02:24,833 [INFO] __main__: [DEBUG] Inference completed in 149.69s
2026-01-20 10:02:29,294 [INFO] __main__: [DEBUG] Got batch 404, extracting texts...
2026-01-20 10:02:29,295 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:03:23,931 [INFO] __main__: [DEBUG] Inference completed in 54.64s
2026-01-20 10:03:25,571 [INFO] __main__: [DEBUG] Got batch 405, extracting texts...
2026-01-20 10:03:25,571 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:05:14,951 [INFO] __main__: [DEBUG] Inference completed in 109.38s
2026-01-20 10:05:18,270 [INFO] __main__: [DEBUG] Got batch 406, extracting texts...
2026-01-20 10:05:18,271 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:06:19,596 [INFO] __main__: [DEBUG] Inference completed in 61.33s
2026-01-20 10:06:21,363 [INFO] __main__: [DEBUG] Got batch 407, extracting texts...
2026-01-20 10:06:21,364 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:07:37,071 [INFO] __main__: [DEBUG] Inference completed in 75.71s
2026-01-20 10:07:39,572 [INFO] __main__: [DEBUG] Got batch 408, extracting texts...
2026-01-20 10:07:39,573 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:08:39,299 [INFO] __main__: [DEBUG] Inference completed in 59.73s
2026-01-20 10:08:40,934 [INFO] __main__: [DEBUG] Got batch 409, extracting texts...
2026-01-20 10:08:40,935 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:11:10,116 [INFO] __main__: [DEBUG] Inference completed in 149.18s
2026-01-20 10:11:15,375 [INFO] __main__: [DEBUG] Got batch 410, extracting texts...
2026-01-20 10:11:15,376 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:13:27,998 [INFO] __main__: [DEBUG] Inference completed in 132.62s
2026-01-20 10:13:27,998 [INFO] __main__:    Processed 410 batches...
2026-01-20 10:13:27,999 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 410 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 10:13:49,253 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.63 GB reserved
2026-01-20 10:13:49,254 [INFO] __main__: [DEBUG] Got batch 411, extracting texts...
2026-01-20 10:13:49,254 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:15:32,969 [INFO] __main__: [DEBUG] Inference completed in 103.71s
2026-01-20 10:15:36,358 [INFO] __main__: [DEBUG] Got batch 412, extracting texts...
2026-01-20 10:15:36,358 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:16:43,638 [INFO] __main__: [DEBUG] Inference completed in 67.28s
2026-01-20 10:16:45,504 [INFO] __main__: [DEBUG] Got batch 413, extracting texts...
2026-01-20 10:16:45,505 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:18:11,956 [INFO] __main__: [DEBUG] Inference completed in 86.45s
2026-01-20 10:18:14,705 [INFO] __main__: [DEBUG] Got batch 414, extracting texts...
2026-01-20 10:18:14,706 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:20:04,537 [INFO] __main__: [DEBUG] Inference completed in 109.83s
2026-01-20 10:20:07,947 [INFO] __main__: [DEBUG] Got batch 415, extracting texts...
2026-01-20 10:20:07,948 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:21:25,104 [INFO] __main__: [DEBUG] Inference completed in 77.16s
2026-01-20 10:21:27,384 [INFO] __main__: [DEBUG] Got batch 416, extracting texts...
2026-01-20 10:21:27,384 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:22:48,983 [INFO] __main__: [DEBUG] Inference completed in 81.60s
2026-01-20 10:22:51,463 [INFO] __main__: [DEBUG] Got batch 417, extracting texts...
2026-01-20 10:22:51,463 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:24:08,010 [INFO] __main__: [DEBUG] Inference completed in 76.55s
2026-01-20 10:24:10,539 [INFO] __main__: [DEBUG] Got batch 418, extracting texts...
2026-01-20 10:24:10,539 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:25:22,197 [INFO] __main__: [DEBUG] Inference completed in 71.66s
2026-01-20 10:25:24,557 [INFO] __main__: [DEBUG] Got batch 419, extracting texts...
2026-01-20 10:25:24,558 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:26:27,541 [INFO] __main__: [DEBUG] Inference completed in 62.98s
2026-01-20 10:26:29,517 [INFO] __main__: [DEBUG] Got batch 420, extracting texts...
2026-01-20 10:26:29,517 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:27:56,917 [INFO] __main__: [DEBUG] Inference completed in 87.40s
2026-01-20 10:27:56,918 [INFO] __main__:    Processed 420 batches...
2026-01-20 10:27:56,918 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 420 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 10:28:16,051 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.50 GB reserved
2026-01-20 10:28:16,088 [INFO] __main__: [DEBUG] Got batch 421, extracting texts...
2026-01-20 10:28:16,088 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:29:45,800 [INFO] __main__: [DEBUG] Inference completed in 89.71s
2026-01-20 10:29:48,558 [INFO] __main__: [DEBUG] Got batch 422, extracting texts...
2026-01-20 10:29:48,558 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:31:08,755 [INFO] __main__: [DEBUG] Inference completed in 80.20s
2026-01-20 10:31:11,281 [INFO] __main__: [DEBUG] Got batch 423, extracting texts...
2026-01-20 10:31:11,281 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:33:28,426 [INFO] __main__: [DEBUG] Inference completed in 137.14s
2026-01-20 10:33:32,605 [INFO] __main__: [DEBUG] Got batch 424, extracting texts...
2026-01-20 10:33:32,606 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:35:12,436 [INFO] __main__: [DEBUG] Inference completed in 99.83s
2026-01-20 10:35:15,637 [INFO] __main__: [DEBUG] Got batch 425, extracting texts...
2026-01-20 10:35:15,637 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:36:29,821 [INFO] __main__: [DEBUG] Inference completed in 74.18s
2026-01-20 10:36:32,212 [INFO] __main__: [DEBUG] Got batch 426, extracting texts...
2026-01-20 10:36:32,212 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:38:30,231 [INFO] __main__: [DEBUG] Inference completed in 118.02s
2026-01-20 10:38:34,357 [INFO] __main__: [DEBUG] Got batch 427, extracting texts...
2026-01-20 10:38:34,358 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:40:18,682 [INFO] __main__: [DEBUG] Inference completed in 104.32s
2026-01-20 10:40:22,157 [INFO] __main__: [DEBUG] Got batch 428, extracting texts...
2026-01-20 10:40:22,157 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:43:02,112 [INFO] __main__: [DEBUG] Inference completed in 159.95s
2026-01-20 10:43:07,778 [INFO] __main__: [DEBUG] Got batch 429, extracting texts...
2026-01-20 10:43:07,778 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:44:40,776 [INFO] __main__: [DEBUG] Inference completed in 93.00s
2026-01-20 10:44:43,947 [INFO] __main__: [DEBUG] Got batch 430, extracting texts...
2026-01-20 10:44:43,948 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 10:48:01,404 [INFO] __main__: [DEBUG] Inference completed in 197.46s
2026-01-20 10:48:01,405 [INFO] __main__:    Processed 430 batches...
2026-01-20 10:48:01,405 [INFO] __main__: ğŸ’¾ Dumping heaps at batch 430 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225346
2026-01-20 10:48:25,677 [INFO] __main__: ğŸ’¾ CUDA memory: 19.33 GB allocated, 19.83 GB reserved
2026-01-20 10:48:25,716 [INFO] __main__: [DEBUG] Got batch 431, extracting texts...
2026-01-20 10:48:25,716 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
slurmstepd: error: *** JOB 1524134 ON hopper CANCELLED AT 2026-01-20T10:50:19 DUE TO TIME LIMIT ***
