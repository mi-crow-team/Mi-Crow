Audited 188 packages in 3.60s
2026-01-19 22:53:11,466 [INFO] __main__: ðŸš€ Starting SAE Inference with Text Tracking
2026-01-19 22:53:11,467 [INFO] __main__: ðŸ“± Using device: cuda
2026-01-19 22:53:11,467 [INFO] __main__: ðŸ”§ Model: speakleash/Bielik-1.5B-v3.0-Instruct
2026-01-19 22:53:11,467 [INFO] __main__: ðŸ“Š Dataset: clarin-pl/polemo2-official
2026-01-19 22:53:11,467 [INFO] __main__: ðŸ“Š Data limit: None
2026-01-19 22:53:11,467 [INFO] __main__: ðŸ“ Store directory: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store
2026-01-19 22:53:11,468 [INFO] __main__: ðŸŽ¯ Layers (2): llamaforcausallm_model_layers_15, llamaforcausallm_model_layers_20
2026-01-19 22:53:11,468 [INFO] __main__: ðŸ“¦ SAE paths (2):
2026-01-19 22:53:11,468 [INFO] __main__:    0: experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_15_20260118_150004/model.pt
2026-01-19 22:53:11,468 [INFO] __main__:    1: experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_20_20260118_150004/model.pt
2026-01-19 22:53:11,468 [INFO] __main__: ðŸ”¢ Top-K texts per neuron: 15
2026-01-19 22:53:11,468 [INFO] __main__: ðŸ“Š Tracking: both positive (top) and negative (bottom) activations
2026-01-19 22:53:11,468 [INFO] __main__: ðŸ“¦ Batch size: 32
2026-01-19 22:53:11,867 [INFO] __main__: ðŸ”Œ CUDA available: NVIDIA H100 PCIe
2026-01-19 22:53:11,867 [INFO] __main__: ðŸ’¾ CUDA memory: 85.03 GB
2026-01-19 22:53:11,868 [INFO] __main__: ðŸ“¥ Loading language model...
2026-01-19 22:53:25,504 [INFO] __main__: âœ… Model loaded: speakleash_Bielik-1.5B-v3.0-Instruct
2026-01-19 22:53:25,505 [INFO] __main__: ðŸ“¥ Loading SAEs...
2026-01-19 22:53:25,505 [INFO] __main__:    Loading SAE 1/2 from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_15_20260118_150004/model.pt...
2026-01-19 22:53:26,660 [INFO] mi_crow.mechanistic.sae.modules.l1_sae: 
Loaded L1SAE from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_15_20260118_150004/model.pt
n_latents=6144, n_inputs=1536
2026-01-19 22:53:26,660 [INFO] __main__:    âœ… Loaded L1Sae: 1536 -> 6144
2026-01-19 22:53:26,691 [INFO] __main__:    âœ… Attached L1Sae to llamaforcausallm_model_layers_15
2026-01-19 22:53:26,691 [INFO] __main__:    Loading SAE 2/2 from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_20_20260118_150004/model.pt...
2026-01-19 22:53:27,323 [INFO] mi_crow.mechanistic.sae.modules.l1_sae: 
Loaded L1SAE from experiments/slurm_sae_pipeline/store/runs/sae_llamaforcausallm_model_layers_20_20260118_150004/model.pt
n_latents=6144, n_inputs=1536
2026-01-19 22:53:27,324 [INFO] __main__:    âœ… Loaded L1Sae: 1536 -> 6144
2026-01-19 22:53:27,324 [INFO] __main__:    âœ… Attached L1Sae to llamaforcausallm_model_layers_20
2026-01-19 22:53:27,324 [INFO] __main__: ðŸ”§ Attaching ModelInputDetector...
2026-01-19 22:53:27,324 [INFO] __main__: âœ… ModelInputDetector attached (hook ID: model_input_detector)
2026-01-19 22:53:27,324 [INFO] __main__: ðŸ“¥ Loading dataset...
2026-01-19 22:53:27,324 [INFO] __main__: ðŸ“¥ Using HuggingFace datasets server API for polemo2-official...
Saving the dataset (0/1 shards):   0%|          | 0/71827 [00:00<?, ? examples/s]Saving the dataset (0/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71827/71827 [00:00<00:00, 675842.60 examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71827/71827 [00:00<00:00, 675842.60 examples/s]Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71827/71827 [00:00<00:00, 654164.36 examples/s]
2026-01-19 22:53:30,066 [INFO] __main__: âœ… Loaded 71827 text samples from dataset
2026-01-19 22:53:30,066 [INFO] __main__: ðŸ“Š Using all 71827 available rows (no data limit)
2026-01-19 22:53:30,067 [INFO] __main__: ðŸš€ Running inference to collect texts...
2026-01-19 22:53:30,067 [INFO] __main__:    Tracking both positive (top) and negative (bottom) activations
2026-01-19 22:53:30,067 [INFO] __main__:    Processing 71827 samples in batches of 32
2026-01-19 22:53:30,067 [INFO] __main__: [DEBUG] Starting batch loop...
2026-01-19 22:53:30,069 [INFO] __main__: [DEBUG] Got batch 1, extracting texts...
2026-01-19 22:53:30,069 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:55:05,291 [INFO] __main__: [DEBUG] Inference completed in 95.22s
2026-01-19 22:55:07,758 [INFO] __main__: [DEBUG] Got batch 2, extracting texts...
2026-01-19 22:55:07,758 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:55:53,624 [INFO] __main__: [DEBUG] Inference completed in 45.87s
2026-01-19 22:55:54,886 [INFO] __main__: [DEBUG] Got batch 3, extracting texts...
2026-01-19 22:55:54,887 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:56:47,061 [INFO] __main__: [DEBUG] Inference completed in 52.17s
2026-01-19 22:56:48,507 [INFO] __main__: [DEBUG] Got batch 4, extracting texts...
2026-01-19 22:56:48,507 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:58:13,447 [INFO] __main__: [DEBUG] Inference completed in 84.94s
2026-01-19 22:58:15,554 [INFO] __main__: [DEBUG] Got batch 5, extracting texts...
2026-01-19 22:58:15,555 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 22:59:17,006 [INFO] __main__: [DEBUG] Inference completed in 61.45s
2026-01-19 22:59:18,825 [INFO] __main__: [DEBUG] Got batch 6, extracting texts...
2026-01-19 22:59:18,825 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:00:32,362 [INFO] __main__: [DEBUG] Inference completed in 73.54s
2026-01-19 23:00:34,166 [INFO] __main__: [DEBUG] Got batch 7, extracting texts...
2026-01-19 23:00:34,167 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:01:30,873 [INFO] __main__: [DEBUG] Inference completed in 56.71s
2026-01-19 23:01:32,419 [INFO] __main__: [DEBUG] Got batch 8, extracting texts...
2026-01-19 23:01:32,420 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:02:22,448 [INFO] __main__: [DEBUG] Inference completed in 50.03s
2026-01-19 23:02:23,918 [INFO] __main__: [DEBUG] Got batch 9, extracting texts...
2026-01-19 23:02:23,919 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:03:40,960 [INFO] __main__: [DEBUG] Inference completed in 77.04s
2026-01-19 23:03:43,047 [INFO] __main__: [DEBUG] Got batch 10, extracting texts...
2026-01-19 23:03:43,048 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:04:27,934 [INFO] __main__: [DEBUG] Inference completed in 44.89s
2026-01-19 23:04:27,936 [INFO] __main__:    Processed 10 batches...
2026-01-19 23:04:27,936 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 10 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-19 23:04:41,296 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.77 GB reserved
2026-01-19 23:04:41,296 [INFO] __main__: [DEBUG] Got batch 11, extracting texts...
2026-01-19 23:04:41,296 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:06:03,289 [INFO] __main__: [DEBUG] Inference completed in 81.99s
2026-01-19 23:06:05,743 [INFO] __main__: [DEBUG] Got batch 12, extracting texts...
2026-01-19 23:06:05,743 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:07:54,784 [INFO] __main__: [DEBUG] Inference completed in 109.04s
2026-01-19 23:07:57,948 [INFO] __main__: [DEBUG] Got batch 13, extracting texts...
2026-01-19 23:07:57,949 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:09:25,621 [INFO] __main__: [DEBUG] Inference completed in 87.67s
2026-01-19 23:09:28,057 [INFO] __main__: [DEBUG] Got batch 14, extracting texts...
2026-01-19 23:09:28,057 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:10:40,791 [INFO] __main__: [DEBUG] Inference completed in 72.73s
2026-01-19 23:10:42,802 [INFO] __main__: [DEBUG] Got batch 15, extracting texts...
2026-01-19 23:10:42,802 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:11:28,807 [INFO] __main__: [DEBUG] Inference completed in 46.00s
2026-01-19 23:11:29,954 [INFO] __main__: [DEBUG] Got batch 16, extracting texts...
2026-01-19 23:11:29,955 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:12:39,553 [INFO] __main__: [DEBUG] Inference completed in 69.60s
2026-01-19 23:12:41,681 [INFO] __main__: [DEBUG] Got batch 17, extracting texts...
2026-01-19 23:12:41,681 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:13:40,304 [INFO] __main__: [DEBUG] Inference completed in 58.62s
2026-01-19 23:13:41,786 [INFO] __main__: [DEBUG] Got batch 18, extracting texts...
2026-01-19 23:13:41,786 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:14:46,445 [INFO] __main__: [DEBUG] Inference completed in 64.66s
2026-01-19 23:14:48,164 [INFO] __main__: [DEBUG] Got batch 19, extracting texts...
2026-01-19 23:14:48,165 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:15:47,634 [INFO] __main__: [DEBUG] Inference completed in 59.47s
2026-01-19 23:15:49,039 [INFO] __main__: [DEBUG] Got batch 20, extracting texts...
2026-01-19 23:15:49,040 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:17:15,996 [INFO] __main__: [DEBUG] Inference completed in 86.96s
2026-01-19 23:17:15,996 [INFO] __main__:    Processed 20 batches...
2026-01-19 23:17:15,996 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 20 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-19 23:17:31,468 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.93 GB reserved
2026-01-19 23:17:31,490 [INFO] __main__: [DEBUG] Got batch 21, extracting texts...
2026-01-19 23:17:31,490 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:18:54,846 [INFO] __main__: [DEBUG] Inference completed in 83.36s
2026-01-19 23:18:57,124 [INFO] __main__: [DEBUG] Got batch 22, extracting texts...
2026-01-19 23:18:57,124 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:20:02,822 [INFO] __main__: [DEBUG] Inference completed in 65.70s
2026-01-19 23:20:04,676 [INFO] __main__: [DEBUG] Got batch 23, extracting texts...
2026-01-19 23:20:04,677 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:20:53,888 [INFO] __main__: [DEBUG] Inference completed in 49.21s
2026-01-19 23:20:55,272 [INFO] __main__: [DEBUG] Got batch 24, extracting texts...
2026-01-19 23:20:55,272 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:22:53,145 [INFO] __main__: [DEBUG] Inference completed in 117.87s
2026-01-19 23:22:56,600 [INFO] __main__: [DEBUG] Got batch 25, extracting texts...
2026-01-19 23:22:56,600 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:23:48,307 [INFO] __main__: [DEBUG] Inference completed in 51.71s
2026-01-19 23:23:49,653 [INFO] __main__: [DEBUG] Got batch 26, extracting texts...
2026-01-19 23:23:49,654 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:24:54,742 [INFO] __main__: [DEBUG] Inference completed in 65.09s
2026-01-19 23:24:56,679 [INFO] __main__: [DEBUG] Got batch 27, extracting texts...
2026-01-19 23:24:56,679 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:26:07,703 [INFO] __main__: [DEBUG] Inference completed in 71.02s
2026-01-19 23:26:09,341 [INFO] __main__: [DEBUG] Got batch 28, extracting texts...
2026-01-19 23:26:09,341 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:27:07,096 [INFO] __main__: [DEBUG] Inference completed in 57.75s
2026-01-19 23:27:08,416 [INFO] __main__: [DEBUG] Got batch 29, extracting texts...
2026-01-19 23:27:08,416 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:28:48,744 [INFO] __main__: [DEBUG] Inference completed in 100.33s
2026-01-19 23:28:51,983 [INFO] __main__: [DEBUG] Got batch 30, extracting texts...
2026-01-19 23:28:51,984 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:29:46,223 [INFO] __main__: [DEBUG] Inference completed in 54.24s
2026-01-19 23:29:46,223 [INFO] __main__:    Processed 30 batches...
2026-01-19 23:29:46,223 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 30 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-19 23:30:00,054 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.82 GB reserved
2026-01-19 23:30:00,055 [INFO] __main__: [DEBUG] Got batch 31, extracting texts...
2026-01-19 23:30:00,055 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:30:58,644 [INFO] __main__: [DEBUG] Inference completed in 58.59s
2026-01-19 23:31:00,293 [INFO] __main__: [DEBUG] Got batch 32, extracting texts...
2026-01-19 23:31:00,293 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:32:17,936 [INFO] __main__: [DEBUG] Inference completed in 77.64s
2026-01-19 23:32:20,068 [INFO] __main__: [DEBUG] Got batch 33, extracting texts...
2026-01-19 23:32:20,068 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:33:15,637 [INFO] __main__: [DEBUG] Inference completed in 55.57s
2026-01-19 23:33:17,275 [INFO] __main__: [DEBUG] Got batch 34, extracting texts...
2026-01-19 23:33:17,275 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:34:12,831 [INFO] __main__: [DEBUG] Inference completed in 55.56s
2026-01-19 23:34:14,473 [INFO] __main__: [DEBUG] Got batch 35, extracting texts...
2026-01-19 23:34:14,474 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:35:20,634 [INFO] __main__: [DEBUG] Inference completed in 66.16s
2026-01-19 23:35:22,413 [INFO] __main__: [DEBUG] Got batch 36, extracting texts...
2026-01-19 23:35:22,414 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:36:38,994 [INFO] __main__: [DEBUG] Inference completed in 76.58s
2026-01-19 23:36:41,473 [INFO] __main__: [DEBUG] Got batch 37, extracting texts...
2026-01-19 23:36:41,474 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:38:37,496 [INFO] __main__: [DEBUG] Inference completed in 116.02s
2026-01-19 23:38:41,244 [INFO] __main__: [DEBUG] Got batch 38, extracting texts...
2026-01-19 23:38:41,245 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:39:35,748 [INFO] __main__: [DEBUG] Inference completed in 54.50s
2026-01-19 23:39:36,977 [INFO] __main__: [DEBUG] Got batch 39, extracting texts...
2026-01-19 23:39:36,978 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:40:41,888 [INFO] __main__: [DEBUG] Inference completed in 64.91s
2026-01-19 23:40:43,578 [INFO] __main__: [DEBUG] Got batch 40, extracting texts...
2026-01-19 23:40:43,578 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:42:10,031 [INFO] __main__: [DEBUG] Inference completed in 86.45s
2026-01-19 23:42:10,031 [INFO] __main__:    Processed 40 batches...
2026-01-19 23:42:10,031 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 40 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-19 23:42:26,060 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.92 GB reserved
2026-01-19 23:42:26,061 [INFO] __main__: [DEBUG] Got batch 41, extracting texts...
2026-01-19 23:42:26,061 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:44:17,138 [INFO] __main__: [DEBUG] Inference completed in 111.08s
2026-01-19 23:44:20,537 [INFO] __main__: [DEBUG] Got batch 42, extracting texts...
2026-01-19 23:44:20,538 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:45:46,756 [INFO] __main__: [DEBUG] Inference completed in 86.22s
2026-01-19 23:45:49,421 [INFO] __main__: [DEBUG] Got batch 43, extracting texts...
2026-01-19 23:45:49,423 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:46:51,683 [INFO] __main__: [DEBUG] Inference completed in 62.26s
2026-01-19 23:46:53,437 [INFO] __main__: [DEBUG] Got batch 44, extracting texts...
2026-01-19 23:46:53,438 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:47:54,149 [INFO] __main__: [DEBUG] Inference completed in 60.71s
2026-01-19 23:47:55,960 [INFO] __main__: [DEBUG] Got batch 45, extracting texts...
2026-01-19 23:47:55,960 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:48:40,269 [INFO] __main__: [DEBUG] Inference completed in 44.31s
2026-01-19 23:48:41,497 [INFO] __main__: [DEBUG] Got batch 46, extracting texts...
2026-01-19 23:48:41,497 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:49:31,402 [INFO] __main__: [DEBUG] Inference completed in 49.90s
2026-01-19 23:49:32,830 [INFO] __main__: [DEBUG] Got batch 47, extracting texts...
2026-01-19 23:49:32,830 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:50:45,438 [INFO] __main__: [DEBUG] Inference completed in 72.61s
2026-01-19 23:50:47,417 [INFO] __main__: [DEBUG] Got batch 48, extracting texts...
2026-01-19 23:50:47,417 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:51:44,650 [INFO] __main__: [DEBUG] Inference completed in 57.23s
2026-01-19 23:51:46,281 [INFO] __main__: [DEBUG] Got batch 49, extracting texts...
2026-01-19 23:51:46,282 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:52:31,369 [INFO] __main__: [DEBUG] Inference completed in 45.09s
2026-01-19 23:52:32,747 [INFO] __main__: [DEBUG] Got batch 50, extracting texts...
2026-01-19 23:52:32,748 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:53:35,753 [INFO] __main__: [DEBUG] Inference completed in 63.01s
2026-01-19 23:53:35,754 [INFO] __main__:    Processed 50 batches...
2026-01-19 23:53:35,754 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 50 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-19 23:53:50,549 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.85 GB reserved
2026-01-19 23:53:50,553 [INFO] __main__: [DEBUG] Got batch 51, extracting texts...
2026-01-19 23:53:50,553 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:56:52,800 [INFO] __main__: [DEBUG] Inference completed in 182.25s
2026-01-19 23:56:58,825 [INFO] __main__: [DEBUG] Got batch 52, extracting texts...
2026-01-19 23:56:58,825 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:57:43,645 [INFO] __main__: [DEBUG] Inference completed in 44.82s
2026-01-19 23:57:44,911 [INFO] __main__: [DEBUG] Got batch 53, extracting texts...
2026-01-19 23:57:44,911 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-19 23:59:00,587 [INFO] __main__: [DEBUG] Inference completed in 75.68s
2026-01-19 23:59:02,730 [INFO] __main__: [DEBUG] Got batch 54, extracting texts...
2026-01-19 23:59:02,730 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:00:25,677 [INFO] __main__: [DEBUG] Inference completed in 82.95s
2026-01-20 00:00:27,874 [INFO] __main__: [DEBUG] Got batch 55, extracting texts...
2026-01-20 00:00:27,874 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:01:06,913 [INFO] __main__: [DEBUG] Inference completed in 39.04s
2026-01-20 00:01:08,018 [INFO] __main__: [DEBUG] Got batch 56, extracting texts...
2026-01-20 00:01:08,018 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:02:00,970 [INFO] __main__: [DEBUG] Inference completed in 52.95s
2026-01-20 00:02:02,598 [INFO] __main__: [DEBUG] Got batch 57, extracting texts...
2026-01-20 00:02:02,598 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:03:18,422 [INFO] __main__: [DEBUG] Inference completed in 75.82s
2026-01-20 00:03:20,736 [INFO] __main__: [DEBUG] Got batch 58, extracting texts...
2026-01-20 00:03:20,736 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:04:23,760 [INFO] __main__: [DEBUG] Inference completed in 63.02s
2026-01-20 00:04:25,509 [INFO] __main__: [DEBUG] Got batch 59, extracting texts...
2026-01-20 00:04:25,510 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:05:50,578 [INFO] __main__: [DEBUG] Inference completed in 85.07s
2026-01-20 00:05:53,154 [INFO] __main__: [DEBUG] Got batch 60, extracting texts...
2026-01-20 00:05:53,155 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:06:32,128 [INFO] __main__: [DEBUG] Inference completed in 38.97s
2026-01-20 00:06:32,129 [INFO] __main__:    Processed 60 batches...
2026-01-20 00:06:32,129 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 60 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 00:06:46,119 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.92 GB reserved
2026-01-20 00:06:46,119 [INFO] __main__: [DEBUG] Got batch 61, extracting texts...
2026-01-20 00:06:46,119 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:07:57,231 [INFO] __main__: [DEBUG] Inference completed in 71.11s
2026-01-20 00:07:59,538 [INFO] __main__: [DEBUG] Got batch 62, extracting texts...
2026-01-20 00:07:59,538 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:09:27,117 [INFO] __main__: [DEBUG] Inference completed in 87.58s
2026-01-20 00:09:29,401 [INFO] __main__: [DEBUG] Got batch 63, extracting texts...
2026-01-20 00:09:29,402 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:10:57,346 [INFO] __main__: [DEBUG] Inference completed in 87.94s
2026-01-20 00:10:59,798 [INFO] __main__: [DEBUG] Got batch 64, extracting texts...
2026-01-20 00:10:59,799 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:12:12,368 [INFO] __main__: [DEBUG] Inference completed in 72.57s
2026-01-20 00:12:14,714 [INFO] __main__: [DEBUG] Got batch 65, extracting texts...
2026-01-20 00:12:14,715 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:13:28,032 [INFO] __main__: [DEBUG] Inference completed in 73.32s
2026-01-20 00:13:29,841 [INFO] __main__: [DEBUG] Got batch 66, extracting texts...
2026-01-20 00:13:29,842 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:14:41,144 [INFO] __main__: [DEBUG] Inference completed in 71.30s
2026-01-20 00:14:43,640 [INFO] __main__: [DEBUG] Got batch 67, extracting texts...
2026-01-20 00:14:43,640 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:15:52,461 [INFO] __main__: [DEBUG] Inference completed in 68.82s
2026-01-20 00:15:54,569 [INFO] __main__: [DEBUG] Got batch 68, extracting texts...
2026-01-20 00:15:54,569 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:16:36,531 [INFO] __main__: [DEBUG] Inference completed in 41.96s
2026-01-20 00:16:37,585 [INFO] __main__: [DEBUG] Got batch 69, extracting texts...
2026-01-20 00:16:37,586 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:17:37,904 [INFO] __main__: [DEBUG] Inference completed in 60.32s
2026-01-20 00:17:39,675 [INFO] __main__: [DEBUG] Got batch 70, extracting texts...
2026-01-20 00:17:39,676 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:18:38,396 [INFO] __main__: [DEBUG] Inference completed in 58.72s
2026-01-20 00:18:38,396 [INFO] __main__:    Processed 70 batches...
2026-01-20 00:18:38,396 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 70 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 00:18:52,971 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.84 GB reserved
2026-01-20 00:18:52,972 [INFO] __main__: [DEBUG] Got batch 71, extracting texts...
2026-01-20 00:18:52,972 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:19:49,866 [INFO] __main__: [DEBUG] Inference completed in 56.89s
2026-01-20 00:19:51,469 [INFO] __main__: [DEBUG] Got batch 72, extracting texts...
2026-01-20 00:19:51,469 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:21:38,254 [INFO] __main__: [DEBUG] Inference completed in 106.79s
2026-01-20 00:21:41,596 [INFO] __main__: [DEBUG] Got batch 73, extracting texts...
2026-01-20 00:21:41,596 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:23:18,990 [INFO] __main__: [DEBUG] Inference completed in 97.39s
2026-01-20 00:23:21,560 [INFO] __main__: [DEBUG] Got batch 74, extracting texts...
2026-01-20 00:23:21,560 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:24:14,826 [INFO] __main__: [DEBUG] Inference completed in 53.27s
2026-01-20 00:24:16,716 [INFO] __main__: [DEBUG] Got batch 75, extracting texts...
2026-01-20 00:24:16,717 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:25:29,949 [INFO] __main__: [DEBUG] Inference completed in 73.23s
2026-01-20 00:25:32,046 [INFO] __main__: [DEBUG] Got batch 76, extracting texts...
2026-01-20 00:25:32,047 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:26:19,889 [INFO] __main__: [DEBUG] Inference completed in 47.84s
2026-01-20 00:26:21,404 [INFO] __main__: [DEBUG] Got batch 77, extracting texts...
2026-01-20 00:26:21,405 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:27:34,938 [INFO] __main__: [DEBUG] Inference completed in 73.53s
2026-01-20 00:27:37,171 [INFO] __main__: [DEBUG] Got batch 78, extracting texts...
2026-01-20 00:27:37,171 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:28:57,329 [INFO] __main__: [DEBUG] Inference completed in 80.16s
2026-01-20 00:28:59,531 [INFO] __main__: [DEBUG] Got batch 79, extracting texts...
2026-01-20 00:28:59,532 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:30:01,007 [INFO] __main__: [DEBUG] Inference completed in 61.48s
2026-01-20 00:30:02,687 [INFO] __main__: [DEBUG] Got batch 80, extracting texts...
2026-01-20 00:30:02,687 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:31:08,894 [INFO] __main__: [DEBUG] Inference completed in 66.21s
2026-01-20 00:31:08,894 [INFO] __main__:    Processed 80 batches...
2026-01-20 00:31:08,894 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 80 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 00:31:23,959 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.86 GB reserved
2026-01-20 00:31:23,959 [INFO] __main__: [DEBUG] Got batch 81, extracting texts...
2026-01-20 00:31:23,959 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:32:19,561 [INFO] __main__: [DEBUG] Inference completed in 55.60s
2026-01-20 00:32:21,196 [INFO] __main__: [DEBUG] Got batch 82, extracting texts...
2026-01-20 00:32:21,197 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:33:43,380 [INFO] __main__: [DEBUG] Inference completed in 82.18s
2026-01-20 00:33:45,678 [INFO] __main__: [DEBUG] Got batch 83, extracting texts...
2026-01-20 00:33:45,678 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:34:49,807 [INFO] __main__: [DEBUG] Inference completed in 64.13s
2026-01-20 00:34:51,589 [INFO] __main__: [DEBUG] Got batch 84, extracting texts...
2026-01-20 00:34:51,589 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:35:33,178 [INFO] __main__: [DEBUG] Inference completed in 41.59s
2026-01-20 00:35:34,375 [INFO] __main__: [DEBUG] Got batch 85, extracting texts...
2026-01-20 00:35:34,375 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:37:03,148 [INFO] __main__: [DEBUG] Inference completed in 88.77s
2026-01-20 00:37:05,720 [INFO] __main__: [DEBUG] Got batch 86, extracting texts...
2026-01-20 00:37:05,720 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:38:25,575 [INFO] __main__: [DEBUG] Inference completed in 79.85s
2026-01-20 00:38:28,256 [INFO] __main__: [DEBUG] Got batch 87, extracting texts...
2026-01-20 00:38:28,257 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:39:16,715 [INFO] __main__: [DEBUG] Inference completed in 48.46s
2026-01-20 00:39:17,879 [INFO] __main__: [DEBUG] Got batch 88, extracting texts...
2026-01-20 00:39:17,879 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:40:20,511 [INFO] __main__: [DEBUG] Inference completed in 62.63s
2026-01-20 00:40:22,341 [INFO] __main__: [DEBUG] Got batch 89, extracting texts...
2026-01-20 00:40:22,342 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:41:34,311 [INFO] __main__: [DEBUG] Inference completed in 71.97s
2026-01-20 00:41:36,220 [INFO] __main__: [DEBUG] Got batch 90, extracting texts...
2026-01-20 00:41:36,220 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:42:46,079 [INFO] __main__: [DEBUG] Inference completed in 69.86s
2026-01-20 00:42:46,079 [INFO] __main__:    Processed 90 batches...
2026-01-20 00:42:46,079 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 90 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 00:43:01,265 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.87 GB reserved
2026-01-20 00:43:01,265 [INFO] __main__: [DEBUG] Got batch 91, extracting texts...
2026-01-20 00:43:01,265 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:43:51,430 [INFO] __main__: [DEBUG] Inference completed in 50.16s
2026-01-20 00:43:52,931 [INFO] __main__: [DEBUG] Got batch 92, extracting texts...
2026-01-20 00:43:52,931 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:44:44,468 [INFO] __main__: [DEBUG] Inference completed in 51.54s
2026-01-20 00:44:45,949 [INFO] __main__: [DEBUG] Got batch 93, extracting texts...
2026-01-20 00:44:45,949 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:46:11,786 [INFO] __main__: [DEBUG] Inference completed in 85.84s
2026-01-20 00:46:14,547 [INFO] __main__: [DEBUG] Got batch 94, extracting texts...
2026-01-20 00:46:14,548 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:47:27,079 [INFO] __main__: [DEBUG] Inference completed in 72.53s
2026-01-20 00:47:29,658 [INFO] __main__: [DEBUG] Got batch 95, extracting texts...
2026-01-20 00:47:29,658 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:48:33,775 [INFO] __main__: [DEBUG] Inference completed in 64.12s
2026-01-20 00:48:35,408 [INFO] __main__: [DEBUG] Got batch 96, extracting texts...
2026-01-20 00:48:35,408 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:49:41,021 [INFO] __main__: [DEBUG] Inference completed in 65.61s
2026-01-20 00:49:42,611 [INFO] __main__: [DEBUG] Got batch 97, extracting texts...
2026-01-20 00:49:42,612 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:50:49,016 [INFO] __main__: [DEBUG] Inference completed in 66.40s
2026-01-20 00:50:50,897 [INFO] __main__: [DEBUG] Got batch 98, extracting texts...
2026-01-20 00:50:50,897 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:51:47,084 [INFO] __main__: [DEBUG] Inference completed in 56.19s
2026-01-20 00:51:48,678 [INFO] __main__: [DEBUG] Got batch 99, extracting texts...
2026-01-20 00:51:48,678 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:53:49,064 [INFO] __main__: [DEBUG] Inference completed in 120.39s
2026-01-20 00:53:53,019 [INFO] __main__: [DEBUG] Got batch 100, extracting texts...
2026-01-20 00:53:53,020 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:54:41,539 [INFO] __main__: [DEBUG] Inference completed in 48.52s
2026-01-20 00:54:41,539 [INFO] __main__:    Processed 100 batches...
2026-01-20 00:54:41,539 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 100 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 00:54:55,974 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.84 GB reserved
2026-01-20 00:54:55,995 [INFO] __main__: [DEBUG] Got batch 101, extracting texts...
2026-01-20 00:54:55,995 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:55:31,914 [INFO] __main__: [DEBUG] Inference completed in 35.92s
2026-01-20 00:55:32,844 [INFO] __main__: [DEBUG] Got batch 102, extracting texts...
2026-01-20 00:55:32,844 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:56:21,761 [INFO] __main__: [DEBUG] Inference completed in 48.92s
2026-01-20 00:56:23,501 [INFO] __main__: [DEBUG] Got batch 103, extracting texts...
2026-01-20 00:56:23,502 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:57:49,171 [INFO] __main__: [DEBUG] Inference completed in 85.67s
2026-01-20 00:57:52,013 [INFO] __main__: [DEBUG] Got batch 104, extracting texts...
2026-01-20 00:57:52,013 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:59:00,195 [INFO] __main__: [DEBUG] Inference completed in 68.18s
2026-01-20 00:59:01,895 [INFO] __main__: [DEBUG] Got batch 105, extracting texts...
2026-01-20 00:59:01,896 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 00:59:59,924 [INFO] __main__: [DEBUG] Inference completed in 58.03s
2026-01-20 01:00:01,618 [INFO] __main__: [DEBUG] Got batch 106, extracting texts...
2026-01-20 01:00:01,618 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:01:03,194 [INFO] __main__: [DEBUG] Inference completed in 61.58s
2026-01-20 01:01:05,067 [INFO] __main__: [DEBUG] Got batch 107, extracting texts...
2026-01-20 01:01:05,067 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:02:05,511 [INFO] __main__: [DEBUG] Inference completed in 60.44s
2026-01-20 01:02:07,102 [INFO] __main__: [DEBUG] Got batch 108, extracting texts...
2026-01-20 01:02:07,103 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:03:23,113 [INFO] __main__: [DEBUG] Inference completed in 76.01s
2026-01-20 01:03:25,333 [INFO] __main__: [DEBUG] Got batch 109, extracting texts...
2026-01-20 01:03:25,334 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:04:57,078 [INFO] __main__: [DEBUG] Inference completed in 91.74s
2026-01-20 01:04:59,916 [INFO] __main__: [DEBUG] Got batch 110, extracting texts...
2026-01-20 01:04:59,916 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:06:39,668 [INFO] __main__: [DEBUG] Inference completed in 99.75s
2026-01-20 01:06:39,669 [INFO] __main__:    Processed 110 batches...
2026-01-20 01:06:39,669 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 110 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 01:06:56,125 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.97 GB reserved
2026-01-20 01:06:56,130 [INFO] __main__: [DEBUG] Got batch 111, extracting texts...
2026-01-20 01:06:56,131 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:08:15,571 [INFO] __main__: [DEBUG] Inference completed in 79.44s
2026-01-20 01:08:18,024 [INFO] __main__: [DEBUG] Got batch 112, extracting texts...
2026-01-20 01:08:18,025 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:10:20,209 [INFO] __main__: [DEBUG] Inference completed in 122.18s
2026-01-20 01:10:24,123 [INFO] __main__: [DEBUG] Got batch 113, extracting texts...
2026-01-20 01:10:24,124 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:11:38,960 [INFO] __main__: [DEBUG] Inference completed in 74.84s
2026-01-20 01:11:41,086 [INFO] __main__: [DEBUG] Got batch 114, extracting texts...
2026-01-20 01:11:41,086 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:12:57,982 [INFO] __main__: [DEBUG] Inference completed in 76.90s
2026-01-20 01:13:00,260 [INFO] __main__: [DEBUG] Got batch 115, extracting texts...
2026-01-20 01:13:00,260 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:13:54,408 [INFO] __main__: [DEBUG] Inference completed in 54.15s
2026-01-20 01:13:55,834 [INFO] __main__: [DEBUG] Got batch 116, extracting texts...
2026-01-20 01:13:55,835 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:14:50,724 [INFO] __main__: [DEBUG] Inference completed in 54.89s
2026-01-20 01:14:52,414 [INFO] __main__: [DEBUG] Got batch 117, extracting texts...
2026-01-20 01:14:52,414 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:16:20,200 [INFO] __main__: [DEBUG] Inference completed in 87.79s
2026-01-20 01:16:22,666 [INFO] __main__: [DEBUG] Got batch 118, extracting texts...
2026-01-20 01:16:22,666 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:19:24,814 [INFO] __main__: [DEBUG] Inference completed in 182.15s
2026-01-20 01:19:30,850 [INFO] __main__: [DEBUG] Got batch 119, extracting texts...
2026-01-20 01:19:30,850 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:20:21,785 [INFO] __main__: [DEBUG] Inference completed in 50.93s
2026-01-20 01:20:23,207 [INFO] __main__: [DEBUG] Got batch 120, extracting texts...
2026-01-20 01:20:23,207 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:21:39,749 [INFO] __main__: [DEBUG] Inference completed in 76.54s
2026-01-20 01:21:39,750 [INFO] __main__:    Processed 120 batches...
2026-01-20 01:21:39,750 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 120 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 01:21:55,519 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.89 GB reserved
2026-01-20 01:21:55,519 [INFO] __main__: [DEBUG] Got batch 121, extracting texts...
2026-01-20 01:21:55,520 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:22:44,990 [INFO] __main__: [DEBUG] Inference completed in 49.47s
2026-01-20 01:22:46,396 [INFO] __main__: [DEBUG] Got batch 122, extracting texts...
2026-01-20 01:22:46,397 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:24:55,694 [INFO] __main__: [DEBUG] Inference completed in 129.30s
2026-01-20 01:25:00,036 [INFO] __main__: [DEBUG] Got batch 123, extracting texts...
2026-01-20 01:25:00,036 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:26:46,278 [INFO] __main__: [DEBUG] Inference completed in 106.24s
2026-01-20 01:26:48,212 [INFO] __main__: [DEBUG] Got batch 124, extracting texts...
2026-01-20 01:26:48,213 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:27:54,255 [INFO] __main__: [DEBUG] Inference completed in 66.04s
2026-01-20 01:27:55,842 [INFO] __main__: [DEBUG] Got batch 125, extracting texts...
2026-01-20 01:27:55,843 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:28:55,792 [INFO] __main__: [DEBUG] Inference completed in 59.95s
2026-01-20 01:28:57,568 [INFO] __main__: [DEBUG] Got batch 126, extracting texts...
2026-01-20 01:28:57,569 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:30:11,408 [INFO] __main__: [DEBUG] Inference completed in 73.84s
2026-01-20 01:30:13,389 [INFO] __main__: [DEBUG] Got batch 127, extracting texts...
2026-01-20 01:30:13,389 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:31:06,469 [INFO] __main__: [DEBUG] Inference completed in 53.08s
2026-01-20 01:31:07,906 [INFO] __main__: [DEBUG] Got batch 128, extracting texts...
2026-01-20 01:31:07,906 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:32:01,533 [INFO] __main__: [DEBUG] Inference completed in 53.63s
2026-01-20 01:32:03,191 [INFO] __main__: [DEBUG] Got batch 129, extracting texts...
2026-01-20 01:32:03,191 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:32:58,192 [INFO] __main__: [DEBUG] Inference completed in 55.00s
2026-01-20 01:32:59,765 [INFO] __main__: [DEBUG] Got batch 130, extracting texts...
2026-01-20 01:32:59,765 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:33:46,122 [INFO] __main__: [DEBUG] Inference completed in 46.36s
2026-01-20 01:33:46,123 [INFO] __main__:    Processed 130 batches...
2026-01-20 01:33:46,123 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 130 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 01:34:00,506 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.81 GB reserved
2026-01-20 01:34:00,506 [INFO] __main__: [DEBUG] Got batch 131, extracting texts...
2026-01-20 01:34:00,506 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:34:46,536 [INFO] __main__: [DEBUG] Inference completed in 46.03s
2026-01-20 01:34:47,814 [INFO] __main__: [DEBUG] Got batch 132, extracting texts...
2026-01-20 01:34:47,815 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:35:34,369 [INFO] __main__: [DEBUG] Inference completed in 46.55s
2026-01-20 01:35:35,782 [INFO] __main__: [DEBUG] Got batch 133, extracting texts...
2026-01-20 01:35:35,782 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:36:38,077 [INFO] __main__: [DEBUG] Inference completed in 62.29s
2026-01-20 01:36:39,994 [INFO] __main__: [DEBUG] Got batch 134, extracting texts...
2026-01-20 01:36:39,994 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:37:56,398 [INFO] __main__: [DEBUG] Inference completed in 76.40s
2026-01-20 01:37:58,449 [INFO] __main__: [DEBUG] Got batch 135, extracting texts...
2026-01-20 01:37:58,450 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:39:37,920 [INFO] __main__: [DEBUG] Inference completed in 99.47s
2026-01-20 01:39:40,929 [INFO] __main__: [DEBUG] Got batch 136, extracting texts...
2026-01-20 01:39:40,929 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:40:26,752 [INFO] __main__: [DEBUG] Inference completed in 45.82s
2026-01-20 01:40:28,112 [INFO] __main__: [DEBUG] Got batch 137, extracting texts...
2026-01-20 01:40:28,112 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:41:39,796 [INFO] __main__: [DEBUG] Inference completed in 71.68s
2026-01-20 01:41:42,116 [INFO] __main__: [DEBUG] Got batch 138, extracting texts...
2026-01-20 01:41:42,116 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:42:27,874 [INFO] __main__: [DEBUG] Inference completed in 45.76s
2026-01-20 01:42:28,970 [INFO] __main__: [DEBUG] Got batch 139, extracting texts...
2026-01-20 01:42:28,970 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:43:40,755 [INFO] __main__: [DEBUG] Inference completed in 71.79s
2026-01-20 01:43:42,859 [INFO] __main__: [DEBUG] Got batch 140, extracting texts...
2026-01-20 01:43:42,861 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:44:52,609 [INFO] __main__: [DEBUG] Inference completed in 69.75s
2026-01-20 01:44:52,609 [INFO] __main__:    Processed 140 batches...
2026-01-20 01:44:52,609 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 140 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 01:45:08,401 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.92 GB reserved
2026-01-20 01:45:08,401 [INFO] __main__: [DEBUG] Got batch 141, extracting texts...
2026-01-20 01:45:08,401 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:46:06,007 [INFO] __main__: [DEBUG] Inference completed in 57.61s
2026-01-20 01:46:07,692 [INFO] __main__: [DEBUG] Got batch 142, extracting texts...
2026-01-20 01:46:07,692 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:46:49,146 [INFO] __main__: [DEBUG] Inference completed in 41.45s
2026-01-20 01:46:50,169 [INFO] __main__: [DEBUG] Got batch 143, extracting texts...
2026-01-20 01:46:50,169 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:47:41,433 [INFO] __main__: [DEBUG] Inference completed in 51.26s
2026-01-20 01:47:43,004 [INFO] __main__: [DEBUG] Got batch 144, extracting texts...
2026-01-20 01:47:43,004 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:48:39,084 [INFO] __main__: [DEBUG] Inference completed in 56.08s
2026-01-20 01:48:40,867 [INFO] __main__: [DEBUG] Got batch 145, extracting texts...
2026-01-20 01:48:40,868 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:49:54,707 [INFO] __main__: [DEBUG] Inference completed in 73.84s
2026-01-20 01:49:56,765 [INFO] __main__: [DEBUG] Got batch 146, extracting texts...
2026-01-20 01:49:56,765 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:51:30,196 [INFO] __main__: [DEBUG] Inference completed in 93.43s
2026-01-20 01:51:33,041 [INFO] __main__: [DEBUG] Got batch 147, extracting texts...
2026-01-20 01:51:33,042 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:52:22,433 [INFO] __main__: [DEBUG] Inference completed in 49.39s
2026-01-20 01:52:23,863 [INFO] __main__: [DEBUG] Got batch 148, extracting texts...
2026-01-20 01:52:23,863 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:53:27,596 [INFO] __main__: [DEBUG] Inference completed in 63.73s
2026-01-20 01:53:29,597 [INFO] __main__: [DEBUG] Got batch 149, extracting texts...
2026-01-20 01:53:29,597 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:54:33,023 [INFO] __main__: [DEBUG] Inference completed in 63.43s
2026-01-20 01:54:34,698 [INFO] __main__: [DEBUG] Got batch 150, extracting texts...
2026-01-20 01:54:34,699 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:55:56,126 [INFO] __main__: [DEBUG] Inference completed in 81.43s
2026-01-20 01:55:56,126 [INFO] __main__:    Processed 150 batches...
2026-01-20 01:55:56,126 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 150 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 01:56:12,532 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.91 GB reserved
2026-01-20 01:56:12,533 [INFO] __main__: [DEBUG] Got batch 151, extracting texts...
2026-01-20 01:56:12,533 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:57:17,236 [INFO] __main__: [DEBUG] Inference completed in 64.70s
2026-01-20 01:57:18,787 [INFO] __main__: [DEBUG] Got batch 152, extracting texts...
2026-01-20 01:57:18,788 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 01:58:21,781 [INFO] __main__: [DEBUG] Inference completed in 62.99s
2026-01-20 01:58:23,523 [INFO] __main__: [DEBUG] Got batch 153, extracting texts...
2026-01-20 01:58:23,524 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:00:16,572 [INFO] __main__: [DEBUG] Inference completed in 113.05s
2026-01-20 02:00:20,018 [INFO] __main__: [DEBUG] Got batch 154, extracting texts...
2026-01-20 02:00:20,019 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:01:27,620 [INFO] __main__: [DEBUG] Inference completed in 67.60s
2026-01-20 02:01:29,657 [INFO] __main__: [DEBUG] Got batch 155, extracting texts...
2026-01-20 02:01:29,658 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:02:30,162 [INFO] __main__: [DEBUG] Inference completed in 60.50s
2026-01-20 02:02:31,868 [INFO] __main__: [DEBUG] Got batch 156, extracting texts...
2026-01-20 02:02:31,869 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:03:50,458 [INFO] __main__: [DEBUG] Inference completed in 78.59s
2026-01-20 02:03:52,466 [INFO] __main__: [DEBUG] Got batch 157, extracting texts...
2026-01-20 02:03:52,466 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:04:52,835 [INFO] __main__: [DEBUG] Inference completed in 60.37s
2026-01-20 02:04:54,525 [INFO] __main__: [DEBUG] Got batch 158, extracting texts...
2026-01-20 02:04:54,526 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:05:45,725 [INFO] __main__: [DEBUG] Inference completed in 51.20s
2026-01-20 02:05:47,203 [INFO] __main__: [DEBUG] Got batch 159, extracting texts...
2026-01-20 02:05:47,204 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:08:14,178 [INFO] __main__: [DEBUG] Inference completed in 146.97s
2026-01-20 02:08:19,252 [INFO] __main__: [DEBUG] Got batch 160, extracting texts...
2026-01-20 02:08:19,253 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:09:34,403 [INFO] __main__: [DEBUG] Inference completed in 75.15s
2026-01-20 02:09:34,404 [INFO] __main__:    Processed 160 batches...
2026-01-20 02:09:34,404 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 160 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 02:09:49,459 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.89 GB reserved
2026-01-20 02:09:49,460 [INFO] __main__: [DEBUG] Got batch 161, extracting texts...
2026-01-20 02:09:49,460 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:10:40,913 [INFO] __main__: [DEBUG] Inference completed in 51.45s
2026-01-20 02:10:42,487 [INFO] __main__: [DEBUG] Got batch 162, extracting texts...
2026-01-20 02:10:42,487 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:11:59,516 [INFO] __main__: [DEBUG] Inference completed in 77.03s
2026-01-20 02:12:01,895 [INFO] __main__: [DEBUG] Got batch 163, extracting texts...
2026-01-20 02:12:01,895 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:12:51,072 [INFO] __main__: [DEBUG] Inference completed in 49.18s
2026-01-20 02:12:52,256 [INFO] __main__: [DEBUG] Got batch 164, extracting texts...
2026-01-20 02:12:52,257 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:14:40,425 [INFO] __main__: [DEBUG] Inference completed in 108.17s
2026-01-20 02:14:42,653 [INFO] __main__: [DEBUG] Got batch 165, extracting texts...
2026-01-20 02:14:42,654 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:15:19,121 [INFO] __main__: [DEBUG] Inference completed in 36.47s
2026-01-20 02:15:19,890 [INFO] __main__: [DEBUG] Got batch 166, extracting texts...
2026-01-20 02:15:19,890 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:16:26,917 [INFO] __main__: [DEBUG] Inference completed in 67.03s
2026-01-20 02:16:28,390 [INFO] __main__: [DEBUG] Got batch 167, extracting texts...
2026-01-20 02:16:28,390 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:17:21,069 [INFO] __main__: [DEBUG] Inference completed in 52.68s
2026-01-20 02:17:22,501 [INFO] __main__: [DEBUG] Got batch 168, extracting texts...
2026-01-20 02:17:22,501 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:19:57,747 [INFO] __main__: [DEBUG] Inference completed in 155.25s
2026-01-20 02:20:03,041 [INFO] __main__: [DEBUG] Got batch 169, extracting texts...
2026-01-20 02:20:03,041 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:20:54,845 [INFO] __main__: [DEBUG] Inference completed in 51.80s
2026-01-20 02:20:56,143 [INFO] __main__: [DEBUG] Got batch 170, extracting texts...
2026-01-20 02:20:56,143 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:21:48,114 [INFO] __main__: [DEBUG] Inference completed in 51.97s
2026-01-20 02:21:48,115 [INFO] __main__:    Processed 170 batches...
2026-01-20 02:21:48,115 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 170 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 02:22:03,397 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.90 GB reserved
2026-01-20 02:22:03,413 [INFO] __main__: [DEBUG] Got batch 171, extracting texts...
2026-01-20 02:22:03,413 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:23:16,684 [INFO] __main__: [DEBUG] Inference completed in 73.27s
2026-01-20 02:23:18,699 [INFO] __main__: [DEBUG] Got batch 172, extracting texts...
2026-01-20 02:23:18,699 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:24:19,851 [INFO] __main__: [DEBUG] Inference completed in 61.15s
2026-01-20 02:24:21,724 [INFO] __main__: [DEBUG] Got batch 173, extracting texts...
2026-01-20 02:24:21,725 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:25:21,611 [INFO] __main__: [DEBUG] Inference completed in 59.89s
2026-01-20 02:25:23,378 [INFO] __main__: [DEBUG] Got batch 174, extracting texts...
2026-01-20 02:25:23,379 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:26:18,344 [INFO] __main__: [DEBUG] Inference completed in 54.97s
2026-01-20 02:26:19,767 [INFO] __main__: [DEBUG] Got batch 175, extracting texts...
2026-01-20 02:26:19,768 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:27:04,339 [INFO] __main__: [DEBUG] Inference completed in 44.57s
2026-01-20 02:27:05,502 [INFO] __main__: [DEBUG] Got batch 176, extracting texts...
2026-01-20 02:27:05,503 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:28:03,667 [INFO] __main__: [DEBUG] Inference completed in 58.16s
2026-01-20 02:28:05,138 [INFO] __main__: [DEBUG] Got batch 177, extracting texts...
2026-01-20 02:28:05,138 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:29:02,411 [INFO] __main__: [DEBUG] Inference completed in 57.27s
2026-01-20 02:29:04,098 [INFO] __main__: [DEBUG] Got batch 178, extracting texts...
2026-01-20 02:29:04,098 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:30:04,089 [INFO] __main__: [DEBUG] Inference completed in 59.99s
2026-01-20 02:30:05,561 [INFO] __main__: [DEBUG] Got batch 179, extracting texts...
2026-01-20 02:30:05,561 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:31:55,821 [INFO] __main__: [DEBUG] Inference completed in 110.26s
2026-01-20 02:31:59,544 [INFO] __main__: [DEBUG] Got batch 180, extracting texts...
2026-01-20 02:31:59,545 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:33:38,379 [INFO] __main__: [DEBUG] Inference completed in 98.83s
2026-01-20 02:33:38,380 [INFO] __main__:    Processed 180 batches...
2026-01-20 02:33:38,380 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 180 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 02:33:53,777 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.96 GB reserved
2026-01-20 02:33:53,778 [INFO] __main__: [DEBUG] Got batch 181, extracting texts...
2026-01-20 02:33:53,778 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:36:55,351 [INFO] __main__: [DEBUG] Inference completed in 181.57s
2026-01-20 02:37:01,861 [INFO] __main__: [DEBUG] Got batch 182, extracting texts...
2026-01-20 02:37:01,862 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:37:52,009 [INFO] __main__: [DEBUG] Inference completed in 50.15s
2026-01-20 02:37:53,182 [INFO] __main__: [DEBUG] Got batch 183, extracting texts...
2026-01-20 02:37:53,182 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:38:47,279 [INFO] __main__: [DEBUG] Inference completed in 54.10s
2026-01-20 02:38:48,887 [INFO] __main__: [DEBUG] Got batch 184, extracting texts...
2026-01-20 02:38:48,887 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:39:42,457 [INFO] __main__: [DEBUG] Inference completed in 53.57s
2026-01-20 02:39:44,075 [INFO] __main__: [DEBUG] Got batch 185, extracting texts...
2026-01-20 02:39:44,075 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:40:33,955 [INFO] __main__: [DEBUG] Inference completed in 49.88s
2026-01-20 02:40:35,362 [INFO] __main__: [DEBUG] Got batch 186, extracting texts...
2026-01-20 02:40:35,362 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:41:30,166 [INFO] __main__: [DEBUG] Inference completed in 54.80s
2026-01-20 02:41:31,937 [INFO] __main__: [DEBUG] Got batch 187, extracting texts...
2026-01-20 02:41:31,938 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:42:29,418 [INFO] __main__: [DEBUG] Inference completed in 57.48s
2026-01-20 02:42:31,173 [INFO] __main__: [DEBUG] Got batch 188, extracting texts...
2026-01-20 02:42:31,174 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:43:33,641 [INFO] __main__: [DEBUG] Inference completed in 62.47s
2026-01-20 02:43:35,221 [INFO] __main__: [DEBUG] Got batch 189, extracting texts...
2026-01-20 02:43:35,221 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:44:35,585 [INFO] __main__: [DEBUG] Inference completed in 60.36s
2026-01-20 02:44:37,252 [INFO] __main__: [DEBUG] Got batch 190, extracting texts...
2026-01-20 02:44:37,252 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:45:41,488 [INFO] __main__: [DEBUG] Inference completed in 64.24s
2026-01-20 02:45:41,488 [INFO] __main__:    Processed 190 batches...
2026-01-20 02:45:41,489 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 190 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 02:45:57,144 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.85 GB reserved
2026-01-20 02:45:57,145 [INFO] __main__: [DEBUG] Got batch 191, extracting texts...
2026-01-20 02:45:57,145 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:47:00,650 [INFO] __main__: [DEBUG] Inference completed in 63.50s
2026-01-20 02:47:02,180 [INFO] __main__: [DEBUG] Got batch 192, extracting texts...
2026-01-20 02:47:02,181 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:48:05,207 [INFO] __main__: [DEBUG] Inference completed in 63.03s
2026-01-20 02:48:06,973 [INFO] __main__: [DEBUG] Got batch 193, extracting texts...
2026-01-20 02:48:06,974 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:49:02,834 [INFO] __main__: [DEBUG] Inference completed in 55.86s
2026-01-20 02:49:04,421 [INFO] __main__: [DEBUG] Got batch 194, extracting texts...
2026-01-20 02:49:04,421 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:49:51,638 [INFO] __main__: [DEBUG] Inference completed in 47.22s
2026-01-20 02:49:52,985 [INFO] __main__: [DEBUG] Got batch 195, extracting texts...
2026-01-20 02:49:52,985 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:51:19,070 [INFO] __main__: [DEBUG] Inference completed in 86.08s
2026-01-20 02:51:21,530 [INFO] __main__: [DEBUG] Got batch 196, extracting texts...
2026-01-20 02:51:21,530 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:52:34,095 [INFO] __main__: [DEBUG] Inference completed in 72.56s
2026-01-20 02:52:36,490 [INFO] __main__: [DEBUG] Got batch 197, extracting texts...
2026-01-20 02:52:36,490 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:53:44,670 [INFO] __main__: [DEBUG] Inference completed in 68.18s
2026-01-20 02:53:46,384 [INFO] __main__: [DEBUG] Got batch 198, extracting texts...
2026-01-20 02:53:46,384 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:54:47,055 [INFO] __main__: [DEBUG] Inference completed in 60.67s
2026-01-20 02:54:48,721 [INFO] __main__: [DEBUG] Got batch 199, extracting texts...
2026-01-20 02:54:48,721 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:55:53,757 [INFO] __main__: [DEBUG] Inference completed in 65.04s
2026-01-20 02:55:55,724 [INFO] __main__: [DEBUG] Got batch 200, extracting texts...
2026-01-20 02:55:55,725 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:57:20,034 [INFO] __main__: [DEBUG] Inference completed in 84.31s
2026-01-20 02:57:20,035 [INFO] __main__:    Processed 200 batches...
2026-01-20 02:57:20,035 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 200 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 02:57:36,054 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.78 GB reserved
2026-01-20 02:57:36,054 [INFO] __main__: [DEBUG] Got batch 201, extracting texts...
2026-01-20 02:57:36,054 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 02:59:04,540 [INFO] __main__: [DEBUG] Inference completed in 88.49s
2026-01-20 02:59:07,048 [INFO] __main__: [DEBUG] Got batch 202, extracting texts...
2026-01-20 02:59:07,048 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:00:40,448 [INFO] __main__: [DEBUG] Inference completed in 93.40s
2026-01-20 03:00:43,412 [INFO] __main__: [DEBUG] Got batch 203, extracting texts...
2026-01-20 03:00:43,413 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:01:52,965 [INFO] __main__: [DEBUG] Inference completed in 69.55s
2026-01-20 03:01:54,705 [INFO] __main__: [DEBUG] Got batch 204, extracting texts...
2026-01-20 03:01:54,705 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:02:56,820 [INFO] __main__: [DEBUG] Inference completed in 62.11s
2026-01-20 03:02:58,614 [INFO] __main__: [DEBUG] Got batch 205, extracting texts...
2026-01-20 03:02:58,615 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:04:22,269 [INFO] __main__: [DEBUG] Inference completed in 83.65s
2026-01-20 03:04:24,551 [INFO] __main__: [DEBUG] Got batch 206, extracting texts...
2026-01-20 03:04:24,552 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:06:09,994 [INFO] __main__: [DEBUG] Inference completed in 105.44s
2026-01-20 03:06:13,147 [INFO] __main__: [DEBUG] Got batch 207, extracting texts...
2026-01-20 03:06:13,148 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:07:35,854 [INFO] __main__: [DEBUG] Inference completed in 82.71s
2026-01-20 03:07:38,664 [INFO] __main__: [DEBUG] Got batch 208, extracting texts...
2026-01-20 03:07:38,665 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:09:21,748 [INFO] __main__: [DEBUG] Inference completed in 103.08s
2026-01-20 03:09:24,799 [INFO] __main__: [DEBUG] Got batch 209, extracting texts...
2026-01-20 03:09:24,800 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:10:13,694 [INFO] __main__: [DEBUG] Inference completed in 48.89s
2026-01-20 03:10:14,862 [INFO] __main__: [DEBUG] Got batch 210, extracting texts...
2026-01-20 03:10:14,862 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:11:10,608 [INFO] __main__: [DEBUG] Inference completed in 55.75s
2026-01-20 03:11:10,609 [INFO] __main__:    Processed 210 batches...
2026-01-20 03:11:10,609 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 210 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 03:11:26,101 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.83 GB reserved
2026-01-20 03:11:26,102 [INFO] __main__: [DEBUG] Got batch 211, extracting texts...
2026-01-20 03:11:26,102 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:12:36,681 [INFO] __main__: [DEBUG] Inference completed in 70.58s
2026-01-20 03:12:38,630 [INFO] __main__: [DEBUG] Got batch 212, extracting texts...
2026-01-20 03:12:38,630 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:13:35,126 [INFO] __main__: [DEBUG] Inference completed in 56.50s
2026-01-20 03:13:36,785 [INFO] __main__: [DEBUG] Got batch 213, extracting texts...
2026-01-20 03:13:36,786 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:14:52,326 [INFO] __main__: [DEBUG] Inference completed in 75.54s
2026-01-20 03:14:54,475 [INFO] __main__: [DEBUG] Got batch 214, extracting texts...
2026-01-20 03:14:54,476 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:15:42,539 [INFO] __main__: [DEBUG] Inference completed in 48.06s
2026-01-20 03:15:43,828 [INFO] __main__: [DEBUG] Got batch 215, extracting texts...
2026-01-20 03:15:43,828 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:16:49,246 [INFO] __main__: [DEBUG] Inference completed in 65.42s
2026-01-20 03:16:51,081 [INFO] __main__: [DEBUG] Got batch 216, extracting texts...
2026-01-20 03:16:51,082 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:17:56,513 [INFO] __main__: [DEBUG] Inference completed in 65.43s
2026-01-20 03:17:58,558 [INFO] __main__: [DEBUG] Got batch 217, extracting texts...
2026-01-20 03:17:58,558 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:19:47,587 [INFO] __main__: [DEBUG] Inference completed in 109.03s
2026-01-20 03:19:51,039 [INFO] __main__: [DEBUG] Got batch 218, extracting texts...
2026-01-20 03:19:51,039 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:21:14,209 [INFO] __main__: [DEBUG] Inference completed in 83.17s
2026-01-20 03:21:16,076 [INFO] __main__: [DEBUG] Got batch 219, extracting texts...
2026-01-20 03:21:16,077 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:22:20,732 [INFO] __main__: [DEBUG] Inference completed in 64.66s
2026-01-20 03:22:22,303 [INFO] __main__: [DEBUG] Got batch 220, extracting texts...
2026-01-20 03:22:22,304 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:23:49,110 [INFO] __main__: [DEBUG] Inference completed in 86.81s
2026-01-20 03:23:49,111 [INFO] __main__:    Processed 220 batches...
2026-01-20 03:23:49,111 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 220 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 03:24:05,812 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.93 GB reserved
2026-01-20 03:24:05,833 [INFO] __main__: [DEBUG] Got batch 221, extracting texts...
2026-01-20 03:24:05,833 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:25:36,021 [INFO] __main__: [DEBUG] Inference completed in 90.19s
2026-01-20 03:25:38,489 [INFO] __main__: [DEBUG] Got batch 222, extracting texts...
2026-01-20 03:25:38,489 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:26:30,843 [INFO] __main__: [DEBUG] Inference completed in 52.35s
2026-01-20 03:26:32,155 [INFO] __main__: [DEBUG] Got batch 223, extracting texts...
2026-01-20 03:26:32,155 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:28:21,786 [INFO] __main__: [DEBUG] Inference completed in 109.63s
2026-01-20 03:28:25,279 [INFO] __main__: [DEBUG] Got batch 224, extracting texts...
2026-01-20 03:28:25,280 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:29:25,968 [INFO] __main__: [DEBUG] Inference completed in 60.69s
2026-01-20 03:29:27,540 [INFO] __main__: [DEBUG] Got batch 225, extracting texts...
2026-01-20 03:29:27,541 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:31:05,381 [INFO] __main__: [DEBUG] Inference completed in 97.84s
2026-01-20 03:31:08,464 [INFO] __main__: [DEBUG] Got batch 226, extracting texts...
2026-01-20 03:31:08,464 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:32:46,838 [INFO] __main__: [DEBUG] Inference completed in 98.37s
2026-01-20 03:32:49,698 [INFO] __main__: [DEBUG] Got batch 227, extracting texts...
2026-01-20 03:32:49,698 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:33:49,393 [INFO] __main__: [DEBUG] Inference completed in 59.70s
2026-01-20 03:33:51,098 [INFO] __main__: [DEBUG] Got batch 228, extracting texts...
2026-01-20 03:33:51,099 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:34:50,241 [INFO] __main__: [DEBUG] Inference completed in 59.14s
2026-01-20 03:34:51,995 [INFO] __main__: [DEBUG] Got batch 229, extracting texts...
2026-01-20 03:34:51,996 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:35:40,193 [INFO] __main__: [DEBUG] Inference completed in 48.20s
2026-01-20 03:35:41,524 [INFO] __main__: [DEBUG] Got batch 230, extracting texts...
2026-01-20 03:35:41,524 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:36:59,555 [INFO] __main__: [DEBUG] Inference completed in 78.03s
2026-01-20 03:36:59,555 [INFO] __main__:    Processed 230 batches...
2026-01-20 03:36:59,556 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 230 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 03:37:15,553 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.90 GB reserved
2026-01-20 03:37:15,554 [INFO] __main__: [DEBUG] Got batch 231, extracting texts...
2026-01-20 03:37:15,554 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:38:06,414 [INFO] __main__: [DEBUG] Inference completed in 50.86s
2026-01-20 03:38:07,735 [INFO] __main__: [DEBUG] Got batch 232, extracting texts...
2026-01-20 03:38:07,735 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:38:54,686 [INFO] __main__: [DEBUG] Inference completed in 46.95s
2026-01-20 03:38:55,951 [INFO] __main__: [DEBUG] Got batch 233, extracting texts...
2026-01-20 03:38:55,951 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:40:07,611 [INFO] __main__: [DEBUG] Inference completed in 71.66s
2026-01-20 03:40:09,670 [INFO] __main__: [DEBUG] Got batch 234, extracting texts...
2026-01-20 03:40:09,671 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:41:33,383 [INFO] __main__: [DEBUG] Inference completed in 83.71s
2026-01-20 03:41:35,950 [INFO] __main__: [DEBUG] Got batch 235, extracting texts...
2026-01-20 03:41:35,950 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:42:25,569 [INFO] __main__: [DEBUG] Inference completed in 49.62s
2026-01-20 03:42:26,850 [INFO] __main__: [DEBUG] Got batch 236, extracting texts...
2026-01-20 03:42:26,851 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:43:37,551 [INFO] __main__: [DEBUG] Inference completed in 70.70s
2026-01-20 03:43:39,631 [INFO] __main__: [DEBUG] Got batch 237, extracting texts...
2026-01-20 03:43:39,632 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:44:29,420 [INFO] __main__: [DEBUG] Inference completed in 49.79s
2026-01-20 03:44:30,717 [INFO] __main__: [DEBUG] Got batch 238, extracting texts...
2026-01-20 03:44:30,717 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:45:36,643 [INFO] __main__: [DEBUG] Inference completed in 65.93s
2026-01-20 03:45:38,480 [INFO] __main__: [DEBUG] Got batch 239, extracting texts...
2026-01-20 03:45:38,481 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:46:43,196 [INFO] __main__: [DEBUG] Inference completed in 64.72s
2026-01-20 03:46:45,210 [INFO] __main__: [DEBUG] Got batch 240, extracting texts...
2026-01-20 03:46:45,211 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:47:47,806 [INFO] __main__: [DEBUG] Inference completed in 62.60s
2026-01-20 03:47:47,806 [INFO] __main__:    Processed 240 batches...
2026-01-20 03:47:47,807 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 240 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 03:48:02,948 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.85 GB reserved
2026-01-20 03:48:02,949 [INFO] __main__: [DEBUG] Got batch 241, extracting texts...
2026-01-20 03:48:02,949 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:48:45,049 [INFO] __main__: [DEBUG] Inference completed in 42.10s
2026-01-20 03:48:46,195 [INFO] __main__: [DEBUG] Got batch 242, extracting texts...
2026-01-20 03:48:46,196 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:49:39,468 [INFO] __main__: [DEBUG] Inference completed in 53.27s
2026-01-20 03:49:40,945 [INFO] __main__: [DEBUG] Got batch 243, extracting texts...
2026-01-20 03:49:40,946 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:50:20,398 [INFO] __main__: [DEBUG] Inference completed in 39.45s
2026-01-20 03:50:21,480 [INFO] __main__: [DEBUG] Got batch 244, extracting texts...
2026-01-20 03:50:21,480 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:51:31,012 [INFO] __main__: [DEBUG] Inference completed in 69.53s
2026-01-20 03:51:33,386 [INFO] __main__: [DEBUG] Got batch 245, extracting texts...
2026-01-20 03:51:33,387 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:52:50,980 [INFO] __main__: [DEBUG] Inference completed in 77.59s
2026-01-20 03:52:53,046 [INFO] __main__: [DEBUG] Got batch 246, extracting texts...
2026-01-20 03:52:53,046 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:54:19,962 [INFO] __main__: [DEBUG] Inference completed in 86.92s
2026-01-20 03:54:22,054 [INFO] __main__: [DEBUG] Got batch 247, extracting texts...
2026-01-20 03:54:22,054 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:55:20,257 [INFO] __main__: [DEBUG] Inference completed in 58.20s
2026-01-20 03:55:22,014 [INFO] __main__: [DEBUG] Got batch 248, extracting texts...
2026-01-20 03:55:22,014 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:57:34,591 [INFO] __main__: [DEBUG] Inference completed in 132.58s
2026-01-20 03:57:38,913 [INFO] __main__: [DEBUG] Got batch 249, extracting texts...
2026-01-20 03:57:38,914 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 03:58:53,116 [INFO] __main__: [DEBUG] Inference completed in 74.20s
2026-01-20 03:58:55,090 [INFO] __main__: [DEBUG] Got batch 250, extracting texts...
2026-01-20 03:58:55,090 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:00:26,559 [INFO] __main__: [DEBUG] Inference completed in 91.47s
2026-01-20 04:00:26,560 [INFO] __main__:    Processed 250 batches...
2026-01-20 04:00:26,560 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 250 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 04:00:43,133 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.94 GB reserved
2026-01-20 04:00:43,133 [INFO] __main__: [DEBUG] Got batch 251, extracting texts...
2026-01-20 04:00:43,133 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:02:03,991 [INFO] __main__: [DEBUG] Inference completed in 80.86s
2026-01-20 04:02:06,321 [INFO] __main__: [DEBUG] Got batch 252, extracting texts...
2026-01-20 04:02:06,321 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:05:03,567 [INFO] __main__: [DEBUG] Inference completed in 177.25s
2026-01-20 04:05:09,382 [INFO] __main__: [DEBUG] Got batch 253, extracting texts...
2026-01-20 04:05:09,382 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:06:26,342 [INFO] __main__: [DEBUG] Inference completed in 76.96s
2026-01-20 04:06:28,280 [INFO] __main__: [DEBUG] Got batch 254, extracting texts...
2026-01-20 04:06:28,281 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:07:18,018 [INFO] __main__: [DEBUG] Inference completed in 49.74s
2026-01-20 04:07:19,371 [INFO] __main__: [DEBUG] Got batch 255, extracting texts...
2026-01-20 04:07:19,371 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:08:41,757 [INFO] __main__: [DEBUG] Inference completed in 82.39s
2026-01-20 04:08:44,156 [INFO] __main__: [DEBUG] Got batch 256, extracting texts...
2026-01-20 04:08:44,157 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:09:43,351 [INFO] __main__: [DEBUG] Inference completed in 59.19s
2026-01-20 04:09:44,970 [INFO] __main__: [DEBUG] Got batch 257, extracting texts...
2026-01-20 04:09:44,970 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:11:36,261 [INFO] __main__: [DEBUG] Inference completed in 111.29s
2026-01-20 04:11:39,759 [INFO] __main__: [DEBUG] Got batch 258, extracting texts...
2026-01-20 04:11:39,760 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:12:30,071 [INFO] __main__: [DEBUG] Inference completed in 50.31s
2026-01-20 04:12:31,258 [INFO] __main__: [DEBUG] Got batch 259, extracting texts...
2026-01-20 04:12:31,259 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:14:02,801 [INFO] __main__: [DEBUG] Inference completed in 91.54s
2026-01-20 04:14:05,598 [INFO] __main__: [DEBUG] Got batch 260, extracting texts...
2026-01-20 04:14:05,599 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:15:05,675 [INFO] __main__: [DEBUG] Inference completed in 60.08s
2026-01-20 04:15:05,676 [INFO] __main__:    Processed 260 batches...
2026-01-20 04:15:05,676 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 260 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 04:15:21,065 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.84 GB reserved
2026-01-20 04:15:21,084 [INFO] __main__: [DEBUG] Got batch 261, extracting texts...
2026-01-20 04:15:21,084 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:16:16,513 [INFO] __main__: [DEBUG] Inference completed in 55.43s
2026-01-20 04:16:18,280 [INFO] __main__: [DEBUG] Got batch 262, extracting texts...
2026-01-20 04:16:18,281 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:17:08,192 [INFO] __main__: [DEBUG] Inference completed in 49.91s
2026-01-20 04:17:09,550 [INFO] __main__: [DEBUG] Got batch 263, extracting texts...
2026-01-20 04:17:09,550 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:18:20,590 [INFO] __main__: [DEBUG] Inference completed in 71.04s
2026-01-20 04:18:22,563 [INFO] __main__: [DEBUG] Got batch 264, extracting texts...
2026-01-20 04:18:22,563 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:19:01,222 [INFO] __main__: [DEBUG] Inference completed in 38.66s
2026-01-20 04:19:02,284 [INFO] __main__: [DEBUG] Got batch 265, extracting texts...
2026-01-20 04:19:02,285 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:20:18,607 [INFO] __main__: [DEBUG] Inference completed in 76.32s
2026-01-20 04:20:20,965 [INFO] __main__: [DEBUG] Got batch 266, extracting texts...
2026-01-20 04:20:20,966 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:21:56,307 [INFO] __main__: [DEBUG] Inference completed in 95.34s
2026-01-20 04:21:59,027 [INFO] __main__: [DEBUG] Got batch 267, extracting texts...
2026-01-20 04:21:59,027 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:22:57,939 [INFO] __main__: [DEBUG] Inference completed in 58.91s
2026-01-20 04:22:59,577 [INFO] __main__: [DEBUG] Got batch 268, extracting texts...
2026-01-20 04:22:59,577 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:24:36,357 [INFO] __main__: [DEBUG] Inference completed in 96.78s
2026-01-20 04:24:39,294 [INFO] __main__: [DEBUG] Got batch 269, extracting texts...
2026-01-20 04:24:39,294 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:25:18,525 [INFO] __main__: [DEBUG] Inference completed in 39.23s
2026-01-20 04:25:19,626 [INFO] __main__: [DEBUG] Got batch 270, extracting texts...
2026-01-20 04:25:19,626 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:26:24,932 [INFO] __main__: [DEBUG] Inference completed in 65.31s
2026-01-20 04:26:24,933 [INFO] __main__:    Processed 270 batches...
2026-01-20 04:26:24,933 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 270 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 04:26:40,497 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.91 GB reserved
2026-01-20 04:26:40,498 [INFO] __main__: [DEBUG] Got batch 271, extracting texts...
2026-01-20 04:26:40,498 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:28:02,558 [INFO] __main__: [DEBUG] Inference completed in 82.06s
2026-01-20 04:28:04,782 [INFO] __main__: [DEBUG] Got batch 272, extracting texts...
2026-01-20 04:28:04,782 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:29:22,168 [INFO] __main__: [DEBUG] Inference completed in 77.39s
2026-01-20 04:29:24,816 [INFO] __main__: [DEBUG] Got batch 273, extracting texts...
2026-01-20 04:29:24,816 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:30:10,116 [INFO] __main__: [DEBUG] Inference completed in 45.30s
2026-01-20 04:30:11,152 [INFO] __main__: [DEBUG] Got batch 274, extracting texts...
2026-01-20 04:30:11,152 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:31:21,441 [INFO] __main__: [DEBUG] Inference completed in 70.29s
2026-01-20 04:31:23,567 [INFO] __main__: [DEBUG] Got batch 275, extracting texts...
2026-01-20 04:31:23,568 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:32:04,707 [INFO] __main__: [DEBUG] Inference completed in 41.14s
2026-01-20 04:32:05,765 [INFO] __main__: [DEBUG] Got batch 276, extracting texts...
2026-01-20 04:32:05,766 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:32:45,588 [INFO] __main__: [DEBUG] Inference completed in 39.82s
2026-01-20 04:32:46,699 [INFO] __main__: [DEBUG] Got batch 277, extracting texts...
2026-01-20 04:32:46,699 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:33:58,904 [INFO] __main__: [DEBUG] Inference completed in 72.20s
2026-01-20 04:34:00,930 [INFO] __main__: [DEBUG] Got batch 278, extracting texts...
2026-01-20 04:34:00,931 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:34:53,099 [INFO] __main__: [DEBUG] Inference completed in 52.17s
2026-01-20 04:34:54,571 [INFO] __main__: [DEBUG] Got batch 279, extracting texts...
2026-01-20 04:34:54,571 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:35:58,681 [INFO] __main__: [DEBUG] Inference completed in 64.11s
2026-01-20 04:36:00,743 [INFO] __main__: [DEBUG] Got batch 280, extracting texts...
2026-01-20 04:36:00,743 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:36:41,142 [INFO] __main__: [DEBUG] Inference completed in 40.40s
2026-01-20 04:36:41,142 [INFO] __main__:    Processed 280 batches...
2026-01-20 04:36:41,142 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 280 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 04:36:55,765 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.81 GB reserved
2026-01-20 04:36:55,765 [INFO] __main__: [DEBUG] Got batch 281, extracting texts...
2026-01-20 04:36:55,765 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:38:08,654 [INFO] __main__: [DEBUG] Inference completed in 72.89s
2026-01-20 04:38:10,696 [INFO] __main__: [DEBUG] Got batch 282, extracting texts...
2026-01-20 04:38:10,697 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:39:08,701 [INFO] __main__: [DEBUG] Inference completed in 58.00s
2026-01-20 04:39:10,353 [INFO] __main__: [DEBUG] Got batch 283, extracting texts...
2026-01-20 04:39:10,353 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:40:19,311 [INFO] __main__: [DEBUG] Inference completed in 68.96s
2026-01-20 04:40:21,555 [INFO] __main__: [DEBUG] Got batch 284, extracting texts...
2026-01-20 04:40:21,556 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:41:24,652 [INFO] __main__: [DEBUG] Inference completed in 63.10s
2026-01-20 04:41:26,264 [INFO] __main__: [DEBUG] Got batch 285, extracting texts...
2026-01-20 04:41:26,265 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:43:14,512 [INFO] __main__: [DEBUG] Inference completed in 108.25s
2026-01-20 04:43:17,714 [INFO] __main__: [DEBUG] Got batch 286, extracting texts...
2026-01-20 04:43:17,715 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:44:14,326 [INFO] __main__: [DEBUG] Inference completed in 56.61s
2026-01-20 04:44:15,917 [INFO] __main__: [DEBUG] Got batch 287, extracting texts...
2026-01-20 04:44:15,917 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:45:18,076 [INFO] __main__: [DEBUG] Inference completed in 62.16s
2026-01-20 04:45:19,986 [INFO] __main__: [DEBUG] Got batch 288, extracting texts...
2026-01-20 04:45:19,986 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:46:27,575 [INFO] __main__: [DEBUG] Inference completed in 67.59s
2026-01-20 04:46:29,153 [INFO] __main__: [DEBUG] Got batch 289, extracting texts...
2026-01-20 04:46:29,153 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:47:52,388 [INFO] __main__: [DEBUG] Inference completed in 83.23s
2026-01-20 04:47:54,963 [INFO] __main__: [DEBUG] Got batch 290, extracting texts...
2026-01-20 04:47:54,963 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:49:02,305 [INFO] __main__: [DEBUG] Inference completed in 67.34s
2026-01-20 04:49:02,306 [INFO] __main__:    Processed 290 batches...
2026-01-20 04:49:02,306 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 290 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 04:49:17,553 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.75 GB reserved
2026-01-20 04:49:17,553 [INFO] __main__: [DEBUG] Got batch 291, extracting texts...
2026-01-20 04:49:17,553 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:50:36,809 [INFO] __main__: [DEBUG] Inference completed in 79.26s
2026-01-20 04:50:39,333 [INFO] __main__: [DEBUG] Got batch 292, extracting texts...
2026-01-20 04:50:39,334 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:52:43,801 [INFO] __main__: [DEBUG] Inference completed in 124.47s
2026-01-20 04:52:47,801 [INFO] __main__: [DEBUG] Got batch 293, extracting texts...
2026-01-20 04:52:47,802 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:53:55,101 [INFO] __main__: [DEBUG] Inference completed in 67.30s
2026-01-20 04:53:56,871 [INFO] __main__: [DEBUG] Got batch 294, extracting texts...
2026-01-20 04:53:56,872 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:55:27,497 [INFO] __main__: [DEBUG] Inference completed in 90.63s
2026-01-20 04:55:30,138 [INFO] __main__: [DEBUG] Got batch 295, extracting texts...
2026-01-20 04:55:30,138 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:56:44,829 [INFO] __main__: [DEBUG] Inference completed in 74.69s
2026-01-20 04:56:47,243 [INFO] __main__: [DEBUG] Got batch 296, extracting texts...
2026-01-20 04:56:47,243 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:57:58,186 [INFO] __main__: [DEBUG] Inference completed in 70.94s
2026-01-20 04:57:59,971 [INFO] __main__: [DEBUG] Got batch 297, extracting texts...
2026-01-20 04:57:59,972 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:59:00,637 [INFO] __main__: [DEBUG] Inference completed in 60.67s
2026-01-20 04:59:02,327 [INFO] __main__: [DEBUG] Got batch 298, extracting texts...
2026-01-20 04:59:02,327 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 04:59:49,885 [INFO] __main__: [DEBUG] Inference completed in 47.56s
2026-01-20 04:59:51,130 [INFO] __main__: [DEBUG] Got batch 299, extracting texts...
2026-01-20 04:59:51,130 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:00:49,468 [INFO] __main__: [DEBUG] Inference completed in 58.34s
2026-01-20 05:00:51,303 [INFO] __main__: [DEBUG] Got batch 300, extracting texts...
2026-01-20 05:00:51,303 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:01:40,280 [INFO] __main__: [DEBUG] Inference completed in 48.98s
2026-01-20 05:01:40,280 [INFO] __main__:    Processed 300 batches...
2026-01-20 05:01:40,280 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 300 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 05:01:55,272 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.72 GB reserved
2026-01-20 05:01:55,273 [INFO] __main__: [DEBUG] Got batch 301, extracting texts...
2026-01-20 05:01:55,273 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:03:37,373 [INFO] __main__: [DEBUG] Inference completed in 102.10s
2026-01-20 05:03:40,619 [INFO] __main__: [DEBUG] Got batch 302, extracting texts...
2026-01-20 05:03:40,619 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:04:30,354 [INFO] __main__: [DEBUG] Inference completed in 49.73s
2026-01-20 05:04:31,427 [INFO] __main__: [DEBUG] Got batch 303, extracting texts...
2026-01-20 05:04:31,427 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:06:09,289 [INFO] __main__: [DEBUG] Inference completed in 97.86s
2026-01-20 05:06:12,202 [INFO] __main__: [DEBUG] Got batch 304, extracting texts...
2026-01-20 05:06:12,202 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:07:35,732 [INFO] __main__: [DEBUG] Inference completed in 83.53s
2026-01-20 05:07:38,167 [INFO] __main__: [DEBUG] Got batch 305, extracting texts...
2026-01-20 05:07:38,168 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:08:49,050 [INFO] __main__: [DEBUG] Inference completed in 70.88s
2026-01-20 05:08:51,040 [INFO] __main__: [DEBUG] Got batch 306, extracting texts...
2026-01-20 05:08:51,041 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:09:50,806 [INFO] __main__: [DEBUG] Inference completed in 59.76s
2026-01-20 05:09:52,561 [INFO] __main__: [DEBUG] Got batch 307, extracting texts...
2026-01-20 05:09:52,561 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:11:28,624 [INFO] __main__: [DEBUG] Inference completed in 96.06s
2026-01-20 05:11:31,341 [INFO] __main__: [DEBUG] Got batch 308, extracting texts...
2026-01-20 05:11:31,342 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:12:39,061 [INFO] __main__: [DEBUG] Inference completed in 67.72s
2026-01-20 05:12:41,057 [INFO] __main__: [DEBUG] Got batch 309, extracting texts...
2026-01-20 05:12:41,058 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:13:21,828 [INFO] __main__: [DEBUG] Inference completed in 40.77s
2026-01-20 05:13:22,872 [INFO] __main__: [DEBUG] Got batch 310, extracting texts...
2026-01-20 05:13:22,873 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:14:19,744 [INFO] __main__: [DEBUG] Inference completed in 56.87s
2026-01-20 05:14:19,745 [INFO] __main__:    Processed 310 batches...
2026-01-20 05:14:19,745 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 310 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 05:14:35,339 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.83 GB reserved
2026-01-20 05:14:35,340 [INFO] __main__: [DEBUG] Got batch 311, extracting texts...
2026-01-20 05:14:35,340 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:15:58,254 [INFO] __main__: [DEBUG] Inference completed in 82.91s
2026-01-20 05:16:00,527 [INFO] __main__: [DEBUG] Got batch 312, extracting texts...
2026-01-20 05:16:00,527 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:17:17,345 [INFO] __main__: [DEBUG] Inference completed in 76.82s
2026-01-20 05:17:19,714 [INFO] __main__: [DEBUG] Got batch 313, extracting texts...
2026-01-20 05:17:19,715 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:18:25,969 [INFO] __main__: [DEBUG] Inference completed in 66.25s
2026-01-20 05:18:27,606 [INFO] __main__: [DEBUG] Got batch 314, extracting texts...
2026-01-20 05:18:27,606 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:19:34,535 [INFO] __main__: [DEBUG] Inference completed in 66.93s
2026-01-20 05:19:36,506 [INFO] __main__: [DEBUG] Got batch 315, extracting texts...
2026-01-20 05:19:36,506 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:20:46,967 [INFO] __main__: [DEBUG] Inference completed in 70.46s
2026-01-20 05:20:48,911 [INFO] __main__: [DEBUG] Got batch 316, extracting texts...
2026-01-20 05:20:48,911 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:22:15,034 [INFO] __main__: [DEBUG] Inference completed in 86.12s
2026-01-20 05:22:17,660 [INFO] __main__: [DEBUG] Got batch 317, extracting texts...
2026-01-20 05:22:17,661 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:23:23,771 [INFO] __main__: [DEBUG] Inference completed in 66.11s
2026-01-20 05:23:25,722 [INFO] __main__: [DEBUG] Got batch 318, extracting texts...
2026-01-20 05:23:25,723 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:24:21,171 [INFO] __main__: [DEBUG] Inference completed in 55.45s
2026-01-20 05:24:22,663 [INFO] __main__: [DEBUG] Got batch 319, extracting texts...
2026-01-20 05:24:22,663 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:25:38,774 [INFO] __main__: [DEBUG] Inference completed in 76.11s
2026-01-20 05:25:40,902 [INFO] __main__: [DEBUG] Got batch 320, extracting texts...
2026-01-20 05:25:40,903 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:26:48,322 [INFO] __main__: [DEBUG] Inference completed in 67.42s
2026-01-20 05:26:48,323 [INFO] __main__:    Processed 320 batches...
2026-01-20 05:26:48,323 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 320 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 05:27:04,137 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.88 GB reserved
2026-01-20 05:27:04,138 [INFO] __main__: [DEBUG] Got batch 321, extracting texts...
2026-01-20 05:27:04,138 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:27:52,461 [INFO] __main__: [DEBUG] Inference completed in 48.32s
2026-01-20 05:27:53,729 [INFO] __main__: [DEBUG] Got batch 322, extracting texts...
2026-01-20 05:27:53,729 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:29:20,938 [INFO] __main__: [DEBUG] Inference completed in 87.21s
2026-01-20 05:29:23,831 [INFO] __main__: [DEBUG] Got batch 323, extracting texts...
2026-01-20 05:29:23,831 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:30:27,495 [INFO] __main__: [DEBUG] Inference completed in 63.66s
2026-01-20 05:30:28,986 [INFO] __main__: [DEBUG] Got batch 324, extracting texts...
2026-01-20 05:30:28,986 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:31:59,658 [INFO] __main__: [DEBUG] Inference completed in 90.67s
2026-01-20 05:32:01,949 [INFO] __main__: [DEBUG] Got batch 325, extracting texts...
2026-01-20 05:32:01,949 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:33:11,005 [INFO] __main__: [DEBUG] Inference completed in 69.06s
2026-01-20 05:33:13,167 [INFO] __main__: [DEBUG] Got batch 326, extracting texts...
2026-01-20 05:33:13,167 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:34:09,453 [INFO] __main__: [DEBUG] Inference completed in 56.29s
2026-01-20 05:34:10,939 [INFO] __main__: [DEBUG] Got batch 327, extracting texts...
2026-01-20 05:34:10,939 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:35:13,364 [INFO] __main__: [DEBUG] Inference completed in 62.42s
2026-01-20 05:35:15,009 [INFO] __main__: [DEBUG] Got batch 328, extracting texts...
2026-01-20 05:35:15,009 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:36:18,323 [INFO] __main__: [DEBUG] Inference completed in 63.31s
2026-01-20 05:36:19,883 [INFO] __main__: [DEBUG] Got batch 329, extracting texts...
2026-01-20 05:36:19,883 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:38:14,940 [INFO] __main__: [DEBUG] Inference completed in 115.06s
2026-01-20 05:38:18,377 [INFO] __main__: [DEBUG] Got batch 330, extracting texts...
2026-01-20 05:38:18,378 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:39:03,833 [INFO] __main__: [DEBUG] Inference completed in 45.45s
2026-01-20 05:39:03,833 [INFO] __main__:    Processed 330 batches...
2026-01-20 05:39:03,833 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 330 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 05:39:18,759 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.91 GB reserved
2026-01-20 05:39:18,759 [INFO] __main__: [DEBUG] Got batch 331, extracting texts...
2026-01-20 05:39:18,759 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:40:11,428 [INFO] __main__: [DEBUG] Inference completed in 52.67s
2026-01-20 05:40:13,062 [INFO] __main__: [DEBUG] Got batch 332, extracting texts...
2026-01-20 05:40:13,063 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:41:29,673 [INFO] __main__: [DEBUG] Inference completed in 76.61s
2026-01-20 05:41:31,770 [INFO] __main__: [DEBUG] Got batch 333, extracting texts...
2026-01-20 05:41:31,770 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:42:35,032 [INFO] __main__: [DEBUG] Inference completed in 63.26s
2026-01-20 05:42:37,052 [INFO] __main__: [DEBUG] Got batch 334, extracting texts...
2026-01-20 05:42:37,052 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:43:25,917 [INFO] __main__: [DEBUG] Inference completed in 48.87s
2026-01-20 05:43:27,249 [INFO] __main__: [DEBUG] Got batch 335, extracting texts...
2026-01-20 05:43:27,250 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:44:56,327 [INFO] __main__: [DEBUG] Inference completed in 89.08s
2026-01-20 05:44:59,030 [INFO] __main__: [DEBUG] Got batch 336, extracting texts...
2026-01-20 05:44:59,030 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:45:50,853 [INFO] __main__: [DEBUG] Inference completed in 51.82s
2026-01-20 05:45:52,166 [INFO] __main__: [DEBUG] Got batch 337, extracting texts...
2026-01-20 05:45:52,166 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:47:22,901 [INFO] __main__: [DEBUG] Inference completed in 90.73s
2026-01-20 05:47:25,593 [INFO] __main__: [DEBUG] Got batch 338, extracting texts...
2026-01-20 05:47:25,594 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:48:42,894 [INFO] __main__: [DEBUG] Inference completed in 77.30s
2026-01-20 05:48:45,359 [INFO] __main__: [DEBUG] Got batch 339, extracting texts...
2026-01-20 05:48:45,360 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:49:23,809 [INFO] __main__: [DEBUG] Inference completed in 38.45s
2026-01-20 05:49:24,642 [INFO] __main__: [DEBUG] Got batch 340, extracting texts...
2026-01-20 05:49:24,643 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:51:03,599 [INFO] __main__: [DEBUG] Inference completed in 98.96s
2026-01-20 05:51:03,599 [INFO] __main__:    Processed 340 batches...
2026-01-20 05:51:03,600 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 340 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 05:51:20,557 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.97 GB reserved
2026-01-20 05:51:20,558 [INFO] __main__: [DEBUG] Got batch 341, extracting texts...
2026-01-20 05:51:20,558 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:52:11,704 [INFO] __main__: [DEBUG] Inference completed in 51.15s
2026-01-20 05:52:12,943 [INFO] __main__: [DEBUG] Got batch 342, extracting texts...
2026-01-20 05:52:12,944 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:53:22,332 [INFO] __main__: [DEBUG] Inference completed in 69.39s
2026-01-20 05:53:24,598 [INFO] __main__: [DEBUG] Got batch 343, extracting texts...
2026-01-20 05:53:24,598 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:54:45,200 [INFO] __main__: [DEBUG] Inference completed in 80.60s
2026-01-20 05:54:46,974 [INFO] __main__: [DEBUG] Got batch 344, extracting texts...
2026-01-20 05:54:46,974 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:55:26,752 [INFO] __main__: [DEBUG] Inference completed in 39.78s
2026-01-20 05:55:27,825 [INFO] __main__: [DEBUG] Got batch 345, extracting texts...
2026-01-20 05:55:27,825 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:56:23,062 [INFO] __main__: [DEBUG] Inference completed in 55.24s
2026-01-20 05:56:24,574 [INFO] __main__: [DEBUG] Got batch 346, extracting texts...
2026-01-20 05:56:24,574 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:57:27,379 [INFO] __main__: [DEBUG] Inference completed in 62.80s
2026-01-20 05:57:29,277 [INFO] __main__: [DEBUG] Got batch 347, extracting texts...
2026-01-20 05:57:29,277 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:59:01,010 [INFO] __main__: [DEBUG] Inference completed in 91.73s
2026-01-20 05:59:03,682 [INFO] __main__: [DEBUG] Got batch 348, extracting texts...
2026-01-20 05:59:03,682 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 05:59:50,048 [INFO] __main__: [DEBUG] Inference completed in 46.37s
2026-01-20 05:59:51,134 [INFO] __main__: [DEBUG] Got batch 349, extracting texts...
2026-01-20 05:59:51,135 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:01:18,612 [INFO] __main__: [DEBUG] Inference completed in 87.48s
2026-01-20 06:01:21,184 [INFO] __main__: [DEBUG] Got batch 350, extracting texts...
2026-01-20 06:01:21,184 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:04:02,134 [INFO] __main__: [DEBUG] Inference completed in 160.95s
2026-01-20 06:04:02,135 [INFO] __main__:    Processed 350 batches...
2026-01-20 06:04:02,136 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 350 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 06:04:21,881 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.93 GB reserved
2026-01-20 06:04:21,882 [INFO] __main__: [DEBUG] Got batch 351, extracting texts...
2026-01-20 06:04:21,882 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:05:47,834 [INFO] __main__: [DEBUG] Inference completed in 85.95s
2026-01-20 06:05:50,678 [INFO] __main__: [DEBUG] Got batch 352, extracting texts...
2026-01-20 06:05:50,678 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:06:38,722 [INFO] __main__: [DEBUG] Inference completed in 48.04s
2026-01-20 06:06:39,985 [INFO] __main__: [DEBUG] Got batch 353, extracting texts...
2026-01-20 06:06:39,986 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:07:28,139 [INFO] __main__: [DEBUG] Inference completed in 48.15s
2026-01-20 06:07:29,554 [INFO] __main__: [DEBUG] Got batch 354, extracting texts...
2026-01-20 06:07:29,554 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:10:18,854 [INFO] __main__: [DEBUG] Inference completed in 169.30s
2026-01-20 06:10:24,239 [INFO] __main__: [DEBUG] Got batch 355, extracting texts...
2026-01-20 06:10:24,239 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:12:37,746 [INFO] __main__: [DEBUG] Inference completed in 133.51s
2026-01-20 06:12:41,961 [INFO] __main__: [DEBUG] Got batch 356, extracting texts...
2026-01-20 06:12:41,961 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:14:13,285 [INFO] __main__: [DEBUG] Inference completed in 91.32s
2026-01-20 06:14:15,783 [INFO] __main__: [DEBUG] Got batch 357, extracting texts...
2026-01-20 06:14:15,783 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:15:22,608 [INFO] __main__: [DEBUG] Inference completed in 66.83s
2026-01-20 06:15:24,691 [INFO] __main__: [DEBUG] Got batch 358, extracting texts...
2026-01-20 06:15:24,691 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:16:25,174 [INFO] __main__: [DEBUG] Inference completed in 60.48s
2026-01-20 06:16:26,774 [INFO] __main__: [DEBUG] Got batch 359, extracting texts...
2026-01-20 06:16:26,774 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:17:35,442 [INFO] __main__: [DEBUG] Inference completed in 68.67s
2026-01-20 06:17:37,405 [INFO] __main__: [DEBUG] Got batch 360, extracting texts...
2026-01-20 06:17:37,405 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:18:10,827 [INFO] __main__: [DEBUG] Inference completed in 33.42s
2026-01-20 06:18:10,828 [INFO] __main__:    Processed 360 batches...
2026-01-20 06:18:10,828 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 360 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 06:18:25,999 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.75 GB reserved
2026-01-20 06:18:26,000 [INFO] __main__: [DEBUG] Got batch 361, extracting texts...
2026-01-20 06:18:26,000 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:19:47,331 [INFO] __main__: [DEBUG] Inference completed in 81.33s
2026-01-20 06:19:49,963 [INFO] __main__: [DEBUG] Got batch 362, extracting texts...
2026-01-20 06:19:49,964 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:21:31,117 [INFO] __main__: [DEBUG] Inference completed in 101.15s
2026-01-20 06:21:34,209 [INFO] __main__: [DEBUG] Got batch 363, extracting texts...
2026-01-20 06:21:34,210 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:22:53,161 [INFO] __main__: [DEBUG] Inference completed in 78.95s
2026-01-20 06:22:55,291 [INFO] __main__: [DEBUG] Got batch 364, extracting texts...
2026-01-20 06:22:55,293 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:24:52,374 [INFO] __main__: [DEBUG] Inference completed in 117.08s
2026-01-20 06:24:55,877 [INFO] __main__: [DEBUG] Got batch 365, extracting texts...
2026-01-20 06:24:55,878 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:25:51,836 [INFO] __main__: [DEBUG] Inference completed in 55.96s
2026-01-20 06:25:53,431 [INFO] __main__: [DEBUG] Got batch 366, extracting texts...
2026-01-20 06:25:53,431 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:27:10,544 [INFO] __main__: [DEBUG] Inference completed in 77.11s
2026-01-20 06:27:12,686 [INFO] __main__: [DEBUG] Got batch 367, extracting texts...
2026-01-20 06:27:12,687 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:28:07,280 [INFO] __main__: [DEBUG] Inference completed in 54.59s
2026-01-20 06:28:08,816 [INFO] __main__: [DEBUG] Got batch 368, extracting texts...
2026-01-20 06:28:08,817 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:29:11,799 [INFO] __main__: [DEBUG] Inference completed in 62.98s
2026-01-20 06:29:13,510 [INFO] __main__: [DEBUG] Got batch 369, extracting texts...
2026-01-20 06:29:13,510 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:31:01,662 [INFO] __main__: [DEBUG] Inference completed in 108.15s
2026-01-20 06:31:05,112 [INFO] __main__: [DEBUG] Got batch 370, extracting texts...
2026-01-20 06:31:05,113 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:32:56,831 [INFO] __main__: [DEBUG] Inference completed in 111.72s
2026-01-20 06:32:56,832 [INFO] __main__:    Processed 370 batches...
2026-01-20 06:32:56,832 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 370 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 06:33:15,645 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 7.01 GB reserved
2026-01-20 06:33:15,646 [INFO] __main__: [DEBUG] Got batch 371, extracting texts...
2026-01-20 06:33:15,646 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:35:01,348 [INFO] __main__: [DEBUG] Inference completed in 105.70s
2026-01-20 06:35:04,586 [INFO] __main__: [DEBUG] Got batch 372, extracting texts...
2026-01-20 06:35:04,587 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:36:04,813 [INFO] __main__: [DEBUG] Inference completed in 60.23s
2026-01-20 06:36:06,329 [INFO] __main__: [DEBUG] Got batch 373, extracting texts...
2026-01-20 06:36:06,329 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:37:00,212 [INFO] __main__: [DEBUG] Inference completed in 53.88s
2026-01-20 06:37:01,697 [INFO] __main__: [DEBUG] Got batch 374, extracting texts...
2026-01-20 06:37:01,697 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:37:48,177 [INFO] __main__: [DEBUG] Inference completed in 46.48s
2026-01-20 06:37:49,337 [INFO] __main__: [DEBUG] Got batch 375, extracting texts...
2026-01-20 06:37:49,337 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:38:36,743 [INFO] __main__: [DEBUG] Inference completed in 47.41s
2026-01-20 06:38:37,962 [INFO] __main__: [DEBUG] Got batch 376, extracting texts...
2026-01-20 06:38:37,962 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:39:38,980 [INFO] __main__: [DEBUG] Inference completed in 61.02s
2026-01-20 06:39:40,756 [INFO] __main__: [DEBUG] Got batch 377, extracting texts...
2026-01-20 06:39:40,756 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:41:13,122 [INFO] __main__: [DEBUG] Inference completed in 92.37s
2026-01-20 06:41:15,804 [INFO] __main__: [DEBUG] Got batch 378, extracting texts...
2026-01-20 06:41:15,805 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:42:07,269 [INFO] __main__: [DEBUG] Inference completed in 51.46s
2026-01-20 06:42:08,625 [INFO] __main__: [DEBUG] Got batch 379, extracting texts...
2026-01-20 06:42:08,625 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:43:44,759 [INFO] __main__: [DEBUG] Inference completed in 96.13s
2026-01-20 06:43:47,633 [INFO] __main__: [DEBUG] Got batch 380, extracting texts...
2026-01-20 06:43:47,633 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:44:51,325 [INFO] __main__: [DEBUG] Inference completed in 63.69s
2026-01-20 06:44:51,326 [INFO] __main__:    Processed 380 batches...
2026-01-20 06:44:51,326 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 380 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 06:45:07,889 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.85 GB reserved
2026-01-20 06:45:07,890 [INFO] __main__: [DEBUG] Got batch 381, extracting texts...
2026-01-20 06:45:07,890 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:45:52,437 [INFO] __main__: [DEBUG] Inference completed in 44.55s
2026-01-20 06:45:53,550 [INFO] __main__: [DEBUG] Got batch 382, extracting texts...
2026-01-20 06:45:53,550 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:46:58,738 [INFO] __main__: [DEBUG] Inference completed in 65.19s
2026-01-20 06:47:00,823 [INFO] __main__: [DEBUG] Got batch 383, extracting texts...
2026-01-20 06:47:00,824 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:48:03,510 [INFO] __main__: [DEBUG] Inference completed in 62.69s
2026-01-20 06:48:05,104 [INFO] __main__: [DEBUG] Got batch 384, extracting texts...
2026-01-20 06:48:05,104 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:48:56,504 [INFO] __main__: [DEBUG] Inference completed in 51.40s
2026-01-20 06:48:57,956 [INFO] __main__: [DEBUG] Got batch 385, extracting texts...
2026-01-20 06:48:57,956 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:50:05,103 [INFO] __main__: [DEBUG] Inference completed in 67.15s
2026-01-20 06:50:06,979 [INFO] __main__: [DEBUG] Got batch 386, extracting texts...
2026-01-20 06:50:06,979 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:51:19,237 [INFO] __main__: [DEBUG] Inference completed in 72.26s
2026-01-20 06:51:21,382 [INFO] __main__: [DEBUG] Got batch 387, extracting texts...
2026-01-20 06:51:21,383 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:53:17,851 [INFO] __main__: [DEBUG] Inference completed in 116.47s
2026-01-20 06:53:21,506 [INFO] __main__: [DEBUG] Got batch 388, extracting texts...
2026-01-20 06:53:21,506 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:54:03,195 [INFO] __main__: [DEBUG] Inference completed in 41.69s
2026-01-20 06:54:04,219 [INFO] __main__: [DEBUG] Got batch 389, extracting texts...
2026-01-20 06:54:04,220 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:55:22,808 [INFO] __main__: [DEBUG] Inference completed in 78.59s
2026-01-20 06:55:25,109 [INFO] __main__: [DEBUG] Got batch 390, extracting texts...
2026-01-20 06:55:25,109 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:57:17,602 [INFO] __main__: [DEBUG] Inference completed in 112.49s
2026-01-20 06:57:17,603 [INFO] __main__:    Processed 390 batches...
2026-01-20 06:57:17,603 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 390 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 06:57:36,474 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.95 GB reserved
2026-01-20 06:57:36,475 [INFO] __main__: [DEBUG] Got batch 391, extracting texts...
2026-01-20 06:57:36,475 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:58:25,915 [INFO] __main__: [DEBUG] Inference completed in 49.44s
2026-01-20 06:58:27,002 [INFO] __main__: [DEBUG] Got batch 392, extracting texts...
2026-01-20 06:58:27,002 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 06:59:32,049 [INFO] __main__: [DEBUG] Inference completed in 65.05s
2026-01-20 06:59:34,004 [INFO] __main__: [DEBUG] Got batch 393, extracting texts...
2026-01-20 06:59:34,004 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:01:14,501 [INFO] __main__: [DEBUG] Inference completed in 100.50s
2026-01-20 07:01:17,683 [INFO] __main__: [DEBUG] Got batch 394, extracting texts...
2026-01-20 07:01:17,683 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:02:11,340 [INFO] __main__: [DEBUG] Inference completed in 53.66s
2026-01-20 07:02:12,564 [INFO] __main__: [DEBUG] Got batch 395, extracting texts...
2026-01-20 07:02:12,565 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:02:52,670 [INFO] __main__: [DEBUG] Inference completed in 40.10s
2026-01-20 07:02:53,715 [INFO] __main__: [DEBUG] Got batch 396, extracting texts...
2026-01-20 07:02:53,715 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:04:10,976 [INFO] __main__: [DEBUG] Inference completed in 77.26s
2026-01-20 07:04:13,503 [INFO] __main__: [DEBUG] Got batch 397, extracting texts...
2026-01-20 07:04:13,503 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:05:10,112 [INFO] __main__: [DEBUG] Inference completed in 56.61s
2026-01-20 07:05:11,626 [INFO] __main__: [DEBUG] Got batch 398, extracting texts...
2026-01-20 07:05:11,626 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:06:20,528 [INFO] __main__: [DEBUG] Inference completed in 68.90s
2026-01-20 07:06:22,378 [INFO] __main__: [DEBUG] Got batch 399, extracting texts...
2026-01-20 07:06:22,378 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:07:28,482 [INFO] __main__: [DEBUG] Inference completed in 66.10s
2026-01-20 07:07:30,281 [INFO] __main__: [DEBUG] Got batch 400, extracting texts...
2026-01-20 07:07:30,281 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:08:24,951 [INFO] __main__: [DEBUG] Inference completed in 54.67s
2026-01-20 07:08:24,952 [INFO] __main__:    Processed 400 batches...
2026-01-20 07:08:24,952 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 400 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 07:08:41,386 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.73 GB reserved
2026-01-20 07:08:41,387 [INFO] __main__: [DEBUG] Got batch 401, extracting texts...
2026-01-20 07:08:41,387 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:09:26,702 [INFO] __main__: [DEBUG] Inference completed in 45.31s
2026-01-20 07:09:27,928 [INFO] __main__: [DEBUG] Got batch 402, extracting texts...
2026-01-20 07:09:27,928 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:10:28,892 [INFO] __main__: [DEBUG] Inference completed in 60.96s
2026-01-20 07:10:30,501 [INFO] __main__: [DEBUG] Got batch 403, extracting texts...
2026-01-20 07:10:30,501 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:12:22,016 [INFO] __main__: [DEBUG] Inference completed in 111.51s
2026-01-20 07:12:25,545 [INFO] __main__: [DEBUG] Got batch 404, extracting texts...
2026-01-20 07:12:25,545 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:13:06,148 [INFO] __main__: [DEBUG] Inference completed in 40.60s
2026-01-20 07:13:07,488 [INFO] __main__: [DEBUG] Got batch 405, extracting texts...
2026-01-20 07:13:07,488 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:14:29,499 [INFO] __main__: [DEBUG] Inference completed in 82.01s
2026-01-20 07:14:32,081 [INFO] __main__: [DEBUG] Got batch 406, extracting texts...
2026-01-20 07:14:32,081 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:15:17,640 [INFO] __main__: [DEBUG] Inference completed in 45.56s
2026-01-20 07:15:18,743 [INFO] __main__: [DEBUG] Got batch 407, extracting texts...
2026-01-20 07:15:18,743 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:16:14,624 [INFO] __main__: [DEBUG] Inference completed in 55.88s
2026-01-20 07:16:16,291 [INFO] __main__: [DEBUG] Got batch 408, extracting texts...
2026-01-20 07:16:16,292 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:17:00,099 [INFO] __main__: [DEBUG] Inference completed in 43.81s
2026-01-20 07:17:01,198 [INFO] __main__: [DEBUG] Got batch 409, extracting texts...
2026-01-20 07:17:01,198 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:18:52,358 [INFO] __main__: [DEBUG] Inference completed in 111.16s
2026-01-20 07:18:55,849 [INFO] __main__: [DEBUG] Got batch 410, extracting texts...
2026-01-20 07:18:55,850 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:20:34,841 [INFO] __main__: [DEBUG] Inference completed in 98.99s
2026-01-20 07:20:34,841 [INFO] __main__:    Processed 410 batches...
2026-01-20 07:20:34,842 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 410 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 07:20:52,856 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.97 GB reserved
2026-01-20 07:20:52,857 [INFO] __main__: [DEBUG] Got batch 411, extracting texts...
2026-01-20 07:20:52,857 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:22:10,427 [INFO] __main__: [DEBUG] Inference completed in 77.57s
2026-01-20 07:22:12,839 [INFO] __main__: [DEBUG] Got batch 412, extracting texts...
2026-01-20 07:22:12,839 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:23:03,002 [INFO] __main__: [DEBUG] Inference completed in 50.16s
2026-01-20 07:23:04,198 [INFO] __main__: [DEBUG] Got batch 413, extracting texts...
2026-01-20 07:23:04,198 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:24:08,673 [INFO] __main__: [DEBUG] Inference completed in 64.47s
2026-01-20 07:24:10,454 [INFO] __main__: [DEBUG] Got batch 414, extracting texts...
2026-01-20 07:24:10,455 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:25:32,900 [INFO] __main__: [DEBUG] Inference completed in 82.44s
2026-01-20 07:25:35,373 [INFO] __main__: [DEBUG] Got batch 415, extracting texts...
2026-01-20 07:25:35,373 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:26:32,944 [INFO] __main__: [DEBUG] Inference completed in 57.57s
2026-01-20 07:26:34,685 [INFO] __main__: [DEBUG] Got batch 416, extracting texts...
2026-01-20 07:26:34,686 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:27:35,497 [INFO] __main__: [DEBUG] Inference completed in 60.81s
2026-01-20 07:27:37,140 [INFO] __main__: [DEBUG] Got batch 417, extracting texts...
2026-01-20 07:27:37,141 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:28:34,364 [INFO] __main__: [DEBUG] Inference completed in 57.22s
2026-01-20 07:28:35,857 [INFO] __main__: [DEBUG] Got batch 418, extracting texts...
2026-01-20 07:28:35,857 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:29:28,805 [INFO] __main__: [DEBUG] Inference completed in 52.95s
2026-01-20 07:29:30,255 [INFO] __main__: [DEBUG] Got batch 419, extracting texts...
2026-01-20 07:29:30,255 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:30:17,008 [INFO] __main__: [DEBUG] Inference completed in 46.75s
2026-01-20 07:30:18,234 [INFO] __main__: [DEBUG] Got batch 420, extracting texts...
2026-01-20 07:30:18,235 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:31:23,438 [INFO] __main__: [DEBUG] Inference completed in 65.20s
2026-01-20 07:31:23,439 [INFO] __main__:    Processed 420 batches...
2026-01-20 07:31:23,439 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 420 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 07:31:40,190 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.86 GB reserved
2026-01-20 07:31:40,190 [INFO] __main__: [DEBUG] Got batch 421, extracting texts...
2026-01-20 07:31:40,190 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:32:46,682 [INFO] __main__: [DEBUG] Inference completed in 66.49s
2026-01-20 07:32:48,614 [INFO] __main__: [DEBUG] Got batch 422, extracting texts...
2026-01-20 07:32:48,615 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:33:47,922 [INFO] __main__: [DEBUG] Inference completed in 59.31s
2026-01-20 07:33:49,403 [INFO] __main__: [DEBUG] Got batch 423, extracting texts...
2026-01-20 07:33:49,404 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:35:31,343 [INFO] __main__: [DEBUG] Inference completed in 101.94s
2026-01-20 07:35:34,590 [INFO] __main__: [DEBUG] Got batch 424, extracting texts...
2026-01-20 07:35:34,590 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:36:49,182 [INFO] __main__: [DEBUG] Inference completed in 74.59s
2026-01-20 07:36:50,675 [INFO] __main__: [DEBUG] Got batch 425, extracting texts...
2026-01-20 07:36:50,675 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:37:45,768 [INFO] __main__: [DEBUG] Inference completed in 55.09s
2026-01-20 07:37:47,100 [INFO] __main__: [DEBUG] Got batch 426, extracting texts...
2026-01-20 07:37:47,100 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:39:15,017 [INFO] __main__: [DEBUG] Inference completed in 87.92s
2026-01-20 07:39:17,670 [INFO] __main__: [DEBUG] Got batch 427, extracting texts...
2026-01-20 07:39:17,670 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:40:35,549 [INFO] __main__: [DEBUG] Inference completed in 77.88s
2026-01-20 07:40:38,052 [INFO] __main__: [DEBUG] Got batch 428, extracting texts...
2026-01-20 07:40:38,052 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:42:38,053 [INFO] __main__: [DEBUG] Inference completed in 120.00s
2026-01-20 07:42:41,913 [INFO] __main__: [DEBUG] Got batch 429, extracting texts...
2026-01-20 07:42:41,914 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:43:50,986 [INFO] __main__: [DEBUG] Inference completed in 69.07s
2026-01-20 07:43:52,798 [INFO] __main__: [DEBUG] Got batch 430, extracting texts...
2026-01-20 07:43:52,799 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:46:21,022 [INFO] __main__: [DEBUG] Inference completed in 148.22s
2026-01-20 07:46:21,025 [INFO] __main__:    Processed 430 batches...
2026-01-20 07:46:21,025 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 430 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 07:46:41,408 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 7.08 GB reserved
2026-01-20 07:46:41,429 [INFO] __main__: [DEBUG] Got batch 431, extracting texts...
2026-01-20 07:46:41,430 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:48:10,297 [INFO] __main__: [DEBUG] Inference completed in 88.87s
2026-01-20 07:48:12,722 [INFO] __main__: [DEBUG] Got batch 432, extracting texts...
2026-01-20 07:48:12,722 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:49:29,528 [INFO] __main__: [DEBUG] Inference completed in 76.81s
2026-01-20 07:49:31,779 [INFO] __main__: [DEBUG] Got batch 433, extracting texts...
2026-01-20 07:49:31,779 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:50:36,711 [INFO] __main__: [DEBUG] Inference completed in 64.93s
2026-01-20 07:50:38,449 [INFO] __main__: [DEBUG] Got batch 434, extracting texts...
2026-01-20 07:50:38,449 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:51:41,137 [INFO] __main__: [DEBUG] Inference completed in 62.69s
2026-01-20 07:51:42,976 [INFO] __main__: [DEBUG] Got batch 435, extracting texts...
2026-01-20 07:51:42,977 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:52:36,196 [INFO] __main__: [DEBUG] Inference completed in 53.22s
2026-01-20 07:52:37,708 [INFO] __main__: [DEBUG] Got batch 436, extracting texts...
2026-01-20 07:52:37,709 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:54:16,579 [INFO] __main__: [DEBUG] Inference completed in 98.87s
2026-01-20 07:54:19,617 [INFO] __main__: [DEBUG] Got batch 437, extracting texts...
2026-01-20 07:54:19,617 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:55:11,417 [INFO] __main__: [DEBUG] Inference completed in 51.80s
2026-01-20 07:55:12,640 [INFO] __main__: [DEBUG] Got batch 438, extracting texts...
2026-01-20 07:55:12,640 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:56:41,351 [INFO] __main__: [DEBUG] Inference completed in 88.71s
2026-01-20 07:56:44,008 [INFO] __main__: [DEBUG] Got batch 439, extracting texts...
2026-01-20 07:56:44,008 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:57:53,266 [INFO] __main__: [DEBUG] Inference completed in 69.26s
2026-01-20 07:57:55,134 [INFO] __main__: [DEBUG] Got batch 440, extracting texts...
2026-01-20 07:57:55,135 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 07:58:54,382 [INFO] __main__: [DEBUG] Inference completed in 59.25s
2026-01-20 07:58:54,383 [INFO] __main__:    Processed 440 batches...
2026-01-20 07:58:54,383 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 440 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 07:59:11,048 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.83 GB reserved
2026-01-20 07:59:11,049 [INFO] __main__: [DEBUG] Got batch 441, extracting texts...
2026-01-20 07:59:11,049 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:00:18,353 [INFO] __main__: [DEBUG] Inference completed in 67.30s
2026-01-20 08:00:19,976 [INFO] __main__: [DEBUG] Got batch 442, extracting texts...
2026-01-20 08:00:19,977 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:01:41,870 [INFO] __main__: [DEBUG] Inference completed in 81.89s
2026-01-20 08:01:43,733 [INFO] __main__: [DEBUG] Got batch 443, extracting texts...
2026-01-20 08:01:43,733 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:02:38,389 [INFO] __main__: [DEBUG] Inference completed in 54.66s
2026-01-20 08:02:39,932 [INFO] __main__: [DEBUG] Got batch 444, extracting texts...
2026-01-20 08:02:39,933 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:03:26,928 [INFO] __main__: [DEBUG] Inference completed in 46.99s
2026-01-20 08:03:28,091 [INFO] __main__: [DEBUG] Got batch 445, extracting texts...
2026-01-20 08:03:28,092 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:04:40,919 [INFO] __main__: [DEBUG] Inference completed in 72.83s
2026-01-20 08:04:43,377 [INFO] __main__: [DEBUG] Got batch 446, extracting texts...
2026-01-20 08:04:43,378 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:06:00,295 [INFO] __main__: [DEBUG] Inference completed in 76.92s
2026-01-20 08:06:02,252 [INFO] __main__: [DEBUG] Got batch 447, extracting texts...
2026-01-20 08:06:02,253 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:07:12,864 [INFO] __main__: [DEBUG] Inference completed in 70.61s
2026-01-20 08:07:15,170 [INFO] __main__: [DEBUG] Got batch 448, extracting texts...
2026-01-20 08:07:15,170 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:08:29,511 [INFO] __main__: [DEBUG] Inference completed in 74.34s
2026-01-20 08:08:31,366 [INFO] __main__: [DEBUG] Got batch 449, extracting texts...
2026-01-20 08:08:31,367 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:09:28,170 [INFO] __main__: [DEBUG] Inference completed in 56.80s
2026-01-20 08:09:29,734 [INFO] __main__: [DEBUG] Got batch 450, extracting texts...
2026-01-20 08:09:29,735 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:10:34,163 [INFO] __main__: [DEBUG] Inference completed in 64.43s
2026-01-20 08:10:34,164 [INFO] __main__:    Processed 450 batches...
2026-01-20 08:10:34,164 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 450 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 08:10:50,959 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.86 GB reserved
2026-01-20 08:10:50,960 [INFO] __main__: [DEBUG] Got batch 451, extracting texts...
2026-01-20 08:10:50,960 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:11:32,595 [INFO] __main__: [DEBUG] Inference completed in 41.64s
2026-01-20 08:11:33,648 [INFO] __main__: [DEBUG] Got batch 452, extracting texts...
2026-01-20 08:11:33,649 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:12:30,990 [INFO] __main__: [DEBUG] Inference completed in 57.34s
2026-01-20 08:12:32,633 [INFO] __main__: [DEBUG] Got batch 453, extracting texts...
2026-01-20 08:12:32,634 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:13:38,491 [INFO] __main__: [DEBUG] Inference completed in 65.86s
2026-01-20 08:13:40,541 [INFO] __main__: [DEBUG] Got batch 454, extracting texts...
2026-01-20 08:13:40,541 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:14:36,484 [INFO] __main__: [DEBUG] Inference completed in 55.94s
2026-01-20 08:14:37,870 [INFO] __main__: [DEBUG] Got batch 455, extracting texts...
2026-01-20 08:14:37,870 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:15:19,931 [INFO] __main__: [DEBUG] Inference completed in 42.06s
2026-01-20 08:15:20,695 [INFO] __main__: [DEBUG] Got batch 456, extracting texts...
2026-01-20 08:15:20,695 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:16:35,788 [INFO] __main__: [DEBUG] Inference completed in 75.09s
2026-01-20 08:16:37,947 [INFO] __main__: [DEBUG] Got batch 457, extracting texts...
2026-01-20 08:16:37,947 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:17:15,889 [INFO] __main__: [DEBUG] Inference completed in 37.94s
2026-01-20 08:17:16,812 [INFO] __main__: [DEBUG] Got batch 458, extracting texts...
2026-01-20 08:17:16,813 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:18:29,832 [INFO] __main__: [DEBUG] Inference completed in 73.02s
2026-01-20 08:18:32,153 [INFO] __main__: [DEBUG] Got batch 459, extracting texts...
2026-01-20 08:18:32,154 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:19:34,546 [INFO] __main__: [DEBUG] Inference completed in 62.39s
2026-01-20 08:19:35,985 [INFO] __main__: [DEBUG] Got batch 460, extracting texts...
2026-01-20 08:19:35,986 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:20:23,913 [INFO] __main__: [DEBUG] Inference completed in 47.93s
2026-01-20 08:20:23,913 [INFO] __main__:    Processed 460 batches...
2026-01-20 08:20:23,913 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 460 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 08:20:39,929 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.80 GB reserved
2026-01-20 08:20:39,930 [INFO] __main__: [DEBUG] Got batch 461, extracting texts...
2026-01-20 08:20:39,930 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:21:19,771 [INFO] __main__: [DEBUG] Inference completed in 39.84s
2026-01-20 08:21:20,894 [INFO] __main__: [DEBUG] Got batch 462, extracting texts...
2026-01-20 08:21:20,894 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:22:25,365 [INFO] __main__: [DEBUG] Inference completed in 64.47s
2026-01-20 08:22:27,247 [INFO] __main__: [DEBUG] Got batch 463, extracting texts...
2026-01-20 08:22:27,247 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:23:13,329 [INFO] __main__: [DEBUG] Inference completed in 46.08s
2026-01-20 08:23:14,558 [INFO] __main__: [DEBUG] Got batch 464, extracting texts...
2026-01-20 08:23:14,558 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:24:17,098 [INFO] __main__: [DEBUG] Inference completed in 62.54s
2026-01-20 08:24:18,899 [INFO] __main__: [DEBUG] Got batch 465, extracting texts...
2026-01-20 08:24:18,899 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:26:08,780 [INFO] __main__: [DEBUG] Inference completed in 109.88s
2026-01-20 08:26:12,067 [INFO] __main__: [DEBUG] Got batch 466, extracting texts...
2026-01-20 08:26:12,067 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:27:09,933 [INFO] __main__: [DEBUG] Inference completed in 57.87s
2026-01-20 08:27:11,540 [INFO] __main__: [DEBUG] Got batch 467, extracting texts...
2026-01-20 08:27:11,540 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:28:22,368 [INFO] __main__: [DEBUG] Inference completed in 70.83s
2026-01-20 08:28:24,571 [INFO] __main__: [DEBUG] Got batch 468, extracting texts...
2026-01-20 08:28:24,571 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:29:39,475 [INFO] __main__: [DEBUG] Inference completed in 74.90s
2026-01-20 08:29:41,424 [INFO] __main__: [DEBUG] Got batch 469, extracting texts...
2026-01-20 08:29:41,425 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:30:40,263 [INFO] __main__: [DEBUG] Inference completed in 58.84s
2026-01-20 08:30:42,002 [INFO] __main__: [DEBUG] Got batch 470, extracting texts...
2026-01-20 08:30:42,002 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:31:56,456 [INFO] __main__: [DEBUG] Inference completed in 74.45s
2026-01-20 08:31:56,456 [INFO] __main__:    Processed 470 batches...
2026-01-20 08:31:56,456 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 470 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 08:32:13,370 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.88 GB reserved
2026-01-20 08:32:13,371 [INFO] __main__: [DEBUG] Got batch 471, extracting texts...
2026-01-20 08:32:13,371 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:33:19,966 [INFO] __main__: [DEBUG] Inference completed in 66.59s
2026-01-20 08:33:22,151 [INFO] __main__: [DEBUG] Got batch 472, extracting texts...
2026-01-20 08:33:22,151 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:34:08,660 [INFO] __main__: [DEBUG] Inference completed in 46.51s
2026-01-20 08:34:09,786 [INFO] __main__: [DEBUG] Got batch 473, extracting texts...
2026-01-20 08:34:09,786 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:36:24,755 [INFO] __main__: [DEBUG] Inference completed in 134.97s
2026-01-20 08:36:29,222 [INFO] __main__: [DEBUG] Got batch 474, extracting texts...
2026-01-20 08:36:29,222 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:38:07,538 [INFO] __main__: [DEBUG] Inference completed in 98.32s
2026-01-20 08:38:10,735 [INFO] __main__: [DEBUG] Got batch 475, extracting texts...
2026-01-20 08:38:10,736 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:39:03,036 [INFO] __main__: [DEBUG] Inference completed in 52.30s
2026-01-20 08:39:04,207 [INFO] __main__: [DEBUG] Got batch 476, extracting texts...
2026-01-20 08:39:04,207 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:40:04,669 [INFO] __main__: [DEBUG] Inference completed in 60.46s
2026-01-20 08:40:06,355 [INFO] __main__: [DEBUG] Got batch 477, extracting texts...
2026-01-20 08:40:06,355 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:41:16,111 [INFO] __main__: [DEBUG] Inference completed in 69.76s
2026-01-20 08:41:17,688 [INFO] __main__: [DEBUG] Got batch 478, extracting texts...
2026-01-20 08:41:17,689 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:42:37,119 [INFO] __main__: [DEBUG] Inference completed in 79.43s
2026-01-20 08:42:39,025 [INFO] __main__: [DEBUG] Got batch 479, extracting texts...
2026-01-20 08:42:39,025 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:44:01,716 [INFO] __main__: [DEBUG] Inference completed in 82.69s
2026-01-20 08:44:04,379 [INFO] __main__: [DEBUG] Got batch 480, extracting texts...
2026-01-20 08:44:04,379 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:44:54,637 [INFO] __main__: [DEBUG] Inference completed in 50.26s
2026-01-20 08:44:54,637 [INFO] __main__:    Processed 480 batches...
2026-01-20 08:44:54,637 [INFO] __main__: ðŸ’¾ Dumping heaps at batch 480 to: /mnt/evafs/groups/mi2lab/akaniasty/Mi-Crow/experiments/slurm_sae_pipeline/store/runs/top_texts_collection_20260119_225330
2026-01-20 08:45:10,548 [INFO] __main__: ðŸ’¾ CUDA memory: 6.57 GB allocated, 6.81 GB reserved
2026-01-20 08:45:10,548 [INFO] __main__: [DEBUG] Got batch 481, extracting texts...
2026-01-20 08:45:10,548 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:47:04,898 [INFO] __main__: [DEBUG] Inference completed in 114.35s
2026-01-20 08:47:08,261 [INFO] __main__: [DEBUG] Got batch 482, extracting texts...
2026-01-20 08:47:08,262 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:48:10,946 [INFO] __main__: [DEBUG] Inference completed in 62.68s
2026-01-20 08:48:12,710 [INFO] __main__: [DEBUG] Got batch 483, extracting texts...
2026-01-20 08:48:12,711 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:49:13,506 [INFO] __main__: [DEBUG] Inference completed in 60.80s
2026-01-20 08:49:15,365 [INFO] __main__: [DEBUG] Got batch 484, extracting texts...
2026-01-20 08:49:15,365 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
2026-01-20 08:49:55,602 [INFO] __main__: [DEBUG] Inference completed in 40.24s
2026-01-20 08:49:56,593 [INFO] __main__: [DEBUG] Got batch 485, extracting texts...
2026-01-20 08:49:56,593 [INFO] __main__: [DEBUG] Extracted 32 texts, starting inference...
slurmstepd: error: *** JOB 1524133 ON hopper CANCELLED AT 2026-01-20T08:50:18 DUE TO TIME LIMIT ***
