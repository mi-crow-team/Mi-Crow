{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Amber","text":"<p>Amber is a Python package for explaining and steering LLM behavior (SAE, concepts).</p> <ul> <li>Repo: GitHub</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>Amber's public Python API is documented automatically from docstrings.</p> <p>The top-level <code>amber</code> package is intentionally minimal (it only exports things like <code>ping</code>). The real functionality lives in subpackages, which are documented below.</p>"},{"location":"api/#core-language-model-api","title":"Core language model API","text":""},{"location":"api/#amber.language_model","title":"amber.language_model","text":""},{"location":"api/#mechanistic-interpretability-sae-concepts","title":"Mechanistic interpretability (SAE, concepts)","text":""},{"location":"api/#amber.mechanistic.sae","title":"amber.mechanistic.sae","text":""},{"location":"api/#datasets","title":"Datasets","text":""},{"location":"api/#amber.datasets","title":"amber.datasets","text":""},{"location":"api/#amber.datasets.BaseDataset","title":"BaseDataset","text":"<pre><code>BaseDataset(\n    ds, store, loading_strategy=LoadingStrategy.MEMORY\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for datasets with support for multiple sources, loading strategies, and Store integration.</p> <p>Loading Strategies: - MEMORY: Load entire dataset into memory (fastest random access, highest memory usage) - DYNAMIC_LOAD: Save to disk, read dynamically via memory-mapped Arrow files   (supports len/getitem, lower memory usage) - ITERABLE_ONLY: True streaming mode using IterableDataset   (lowest memory, no len/getitem support, no stratification and limit support)</p> <p>Initialize dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance for caching/persistence</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>How to load data (MEMORY, DYNAMIC_LOAD, or ITERABLE_ONLY)</p> <code>MEMORY</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is None, loading_strategy is invalid, or dataset operations fail</p> <code>OSError</code> <p>If file system operations fail</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n):\n    \"\"\"\n    Initialize dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance for caching/persistence\n        loading_strategy: How to load data (MEMORY, DYNAMIC_LOAD, or ITERABLE_ONLY)\n\n    Raises:\n        ValueError: If store is None, loading_strategy is invalid, or dataset operations fail\n        OSError: If file system operations fail\n    \"\"\"\n    self._validate_initialization_params(store, loading_strategy)\n\n    self._store = store\n    self._loading_strategy = loading_strategy\n    self._dataset_dir: Path = Path(store.base_path) / store.dataset_prefix\n\n    is_iterable_input = isinstance(ds, IterableDataset)\n\n    if loading_strategy == LoadingStrategy.MEMORY:\n        # MEMORY: Convert to Dataset if needed, save to disk, load fully into memory\n        self._is_iterable = False\n        if is_iterable_input:\n            ds = Dataset.from_generator(lambda: iter(ds))\n        self._ds = self._save_and_load_dataset(ds, use_memory_mapping=False)\n    elif loading_strategy == LoadingStrategy.DYNAMIC_LOAD:\n        # DYNAMIC_LOAD: Save to disk, use memory-mapped Arrow files (supports len/getitem)\n        self._is_iterable = False\n        if is_iterable_input:\n            ds = Dataset.from_generator(lambda: iter(ds))\n        self._ds = self._save_and_load_dataset(ds, use_memory_mapping=True)\n    elif loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        # ITERABLE_ONLY: Convert to IterableDataset, don't save to disk (no len/getitem)\n        if not is_iterable_input:\n            ds = ds.to_iterable_dataset()\n        self._is_iterable = True\n        self._ds = ds\n        # Don't save to disk for iterable-only mode\n    else:\n        raise ValueError(\n            f\"Unknown loading strategy: {loading_strategy}. Must be one of: {[s.value for s in LoadingStrategy]}\"\n        )\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.is_streaming","title":"is_streaming  <code>property</code>","text":"<pre><code>is_streaming\n</code></pre> <p>Whether this dataset is streaming (DYNAMIC_LOAD or ITERABLE_ONLY).</p>"},{"location":"api/#amber.datasets.BaseDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get item(s) by index.</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, idx: IndexLike) -&gt; Any:\n    \"\"\"Get item(s) by index.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the number of items in the dataset.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.extract_texts_from_batch","title":"extract_texts_from_batch  <code>abstractmethod</code>","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Any]</code> <p>A batch as returned by iter_batches()</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of text strings ready for model inference</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef extract_texts_from_batch(self, batch: List[Any]) -&gt; List[str]:\n    \"\"\"Extract text strings from a batch.\n\n    Args:\n        batch: A batch as returned by iter_batches()\n\n    Returns:\n        List of text strings ready for model inference\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    delimiter=\",\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na_columns=None,\n    **kwargs\n)\n</code></pre> <p>Load dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to check for None/empty values</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>ValueError</code> <p>If store is None or source is invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na_columns: Optional[List[str]] = None,\n    **kwargs,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na_columns: Optional list of columns to check for None/empty values\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        ValueError: If store is None or source is invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.ITERABLE_ONLY\n    if (stratify_by or drop_na_columns) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for ITERABLE_ONLY datasets.\")\n\n    ds = cls._load_csv_source(\n        source,\n        delimiter=delimiter,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    if not use_streaming and (stratify_by or drop_na_columns):\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    repo_id,\n    store,\n    *,\n    split=\"train\",\n    loading_strategy=LoadingStrategy.MEMORY,\n    revision=None,\n    streaming=None,\n    filters=None,\n    limit=None,\n    stratify_by=None,\n    stratify_seed=None,\n    **kwargs\n)\n</code></pre> <p>Load dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split (e.g., \"train\", \"validation\")</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy (MEMORY, DYNAMIC_LOAD, or ITERABLE_ONLY)</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision/branch/tag</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming (if None, uses loading_strategy)</p> <code>None</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict of column-&gt;value pairs used for exact-match filtering</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional maximum number of rows to keep (applied after filtering/stratification)</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for deterministic stratification</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If repo_id is empty or store is None</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    streaming: Optional[bool] = None,\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    **kwargs,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split (e.g., \"train\", \"validation\")\n        loading_strategy: Loading strategy (MEMORY, DYNAMIC_LOAD, or ITERABLE_ONLY)\n        revision: Optional git revision/branch/tag\n        streaming: Optional override for streaming (if None, uses loading_strategy)\n        filters: Optional dict of column-&gt;value pairs used for exact-match filtering\n        limit: Optional maximum number of rows to keep (applied after filtering/stratification)\n        stratify_by: Optional column to use for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for deterministic stratification\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        ValueError: If repo_id is empty or store is None\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if not repo_id or not isinstance(repo_id, str) or not repo_id.strip():\n        raise ValueError(f\"repo_id must be a non-empty string, got: {repo_id!r}\")\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    # Determine if we should use streaming for HuggingFace load_dataset\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.ITERABLE_ONLY)\n\n    if stratify_by and loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\"Stratification is not supported for ITERABLE_ONLY datasets.\")\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load dataset from HuggingFace Hub: repo_id={repo_id!r}, \"\n            f\"split={split!r}, revision={revision!r}. Error: {e}\"\n        ) from e\n\n    if use_streaming:\n        if filters or limit or stratify_by:\n            raise NotImplementedError(\n                \"filters, limit, and stratification are not supported when streaming datasets. \"\n                \"Choose MEMORY or DYNAMIC_LOAD loading strategy instead.\"\n            )\n    else:\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            filters=filters,\n            limit=limit,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na_columns=None,\n    **kwargs\n)\n</code></pre> <p>Load dataset from JSON or JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text (for JSON objects)</p> <code>'text'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to check for None/empty values</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>ValueError</code> <p>If store is None or source is invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na_columns: Optional[List[str]] = None,\n    **kwargs,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from JSON or JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text (for JSON objects)\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na_columns: Optional list of columns to check for None/empty values\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        ValueError: If store is None or source is invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.ITERABLE_ONLY\n    if (stratify_by or drop_na_columns) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for ITERABLE_ONLY datasets.\")\n\n    ds = cls._load_json_source(\n        source,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    if not use_streaming and (stratify_by or drop_na_columns):\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.get_all_texts","title":"get_all_texts  <code>abstractmethod</code>","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all text strings in the dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not supported for streaming datasets</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef get_all_texts(self) -&gt; List[str]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings in the dataset\n\n    Raises:\n        NotImplementedError: If not supported for streaming datasets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.get_batch","title":"get_batch","text":"<pre><code>get_batch(start, batch_size)\n</code></pre> <p>Get a contiguous batch of items.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting index</p> required <code>batch_size</code> <code>int</code> <p>Number of items to retrieve</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>def get_batch(self, start: int, batch_size: int) -&gt; List[Any]:\n    \"\"\"\n    Get a contiguous batch of items.\n\n    Args:\n        start: Starting index\n        batch_size: Number of items to retrieve\n\n    Returns:\n        List of items\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\"get_batch not supported for ITERABLE_ONLY datasets. Use iter_batches instead.\")\n    if batch_size &lt;= 0:\n        return []\n    end = min(start + batch_size, len(self))\n    if start &gt;= end:\n        return []\n    return self[start:end]\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.head","title":"head","text":"<pre><code>head(n=5)\n</code></pre> <p>Get first n items.</p> <p>Works for all loading strategies.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to retrieve (default: 5)</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of first n items</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>def head(self, n: int = 5) -&gt; List[Any]:\n    \"\"\"\n    Get first n items.\n\n    Works for all loading strategies.\n\n    Args:\n        n: Number of items to retrieve (default: 5)\n\n    Returns:\n        List of first n items\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        items = []\n        for i, item in enumerate(self.iter_items()):\n            if i &gt;= n:\n                break\n            items.append(item)\n        return items\n    return self[:n]\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.iter_batches","title":"iter_batches  <code>abstractmethod</code>","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over items in batches.</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef iter_batches(self, batch_size: int) -&gt; Iterator[List[Any]]:\n    \"\"\"Iterate over items in batches.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.iter_items","title":"iter_items  <code>abstractmethod</code>","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over items one by one.</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef iter_items(self) -&gt; Iterator[Any]:\n    \"\"\"Iterate over items one by one.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#amber.datasets.BaseDataset.sample","title":"sample","text":"<pre><code>sample(n=5)\n</code></pre> <p>Get n random items from the dataset.</p> <p>Works for MEMORY and DYNAMIC_LOAD strategies only.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to sample</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of n randomly sampled items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> Source code in <code>src/amber/datasets/base_dataset.py</code> <pre><code>def sample(self, n: int = 5) -&gt; List[Any]:\n    \"\"\"\n    Get n random items from the dataset.\n\n    Works for MEMORY and DYNAMIC_LOAD strategies only.\n\n    Args:\n        n: Number of items to sample\n\n    Returns:\n        List of n randomly sampled items\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\n            \"sample() not supported for ITERABLE_ONLY datasets. Use iter_items() and sample manually.\"\n        )\n\n    dataset_len = len(self)\n    if n &lt;= 0:\n        return []\n    if n &gt;= dataset_len:\n        # Return all items in random order\n        indices = list(range(dataset_len))\n        random.shuffle(indices)\n        return [self[i] for i in indices]\n\n    # Sample n random indices\n    indices = random.sample(range(dataset_len), n)\n    # Use __getitem__ with list of indices\n    return self[indices]\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset","title":"ClassificationDataset","text":"<pre><code>ClassificationDataset(\n    ds,\n    store,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    category_field=\"category\",\n)\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Classification dataset with text and category/label columns. Each item is a dict with 'text' and label column(s) as keys. Supports single or multiple label columns.</p> <p>Initialize classification dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label.           Can be a single string or a list of strings for multiple labels.</p> <code>'category'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_field or category_field is empty, or fields not found in dataset</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n):\n    \"\"\"\n    Initialize classification dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label.\n                      Can be a single string or a list of strings for multiple labels.\n\n    Raises:\n        ValueError: If text_field or category_field is empty, or fields not found in dataset\n    \"\"\"\n    self._validate_text_field(text_field)\n\n    # Normalize category_field to list\n    if isinstance(category_field, str):\n        self._category_fields = [category_field]\n    else:\n        self._category_fields = list(category_field)\n\n    self._validate_category_fields(self._category_fields)\n\n    # Validate dataset\n    is_iterable = isinstance(ds, IterableDataset)\n    if not is_iterable:\n        if text_field not in ds.column_names:\n            raise ValueError(f\"Dataset must have a '{text_field}' column; got columns: {ds.column_names}\")\n        for cat_field in self._category_fields:\n            if cat_field not in ds.column_names:\n                raise ValueError(f\"Dataset must have a '{cat_field}' column; got columns: {ds.column_names}\")\n        # Set format with all required columns\n        format_columns = [text_field] + self._category_fields\n        ds.set_format(\"python\", columns=format_columns)\n\n    self._text_field = text_field\n    self._category_field = category_field  # Keep original for backward compatibility\n    super().__init__(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get item(s) by index. Returns dict with 'text' and label column(s) as keys.</p> <p>For single label: {\"text\": \"...\", \"category\": \"...\"} For multiple labels: {\"text\": \"...\", \"label1\": \"...\", \"label2\": \"...\"}</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>IndexLike</code> <p>Index (int), slice, or sequence of indices</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>Single item dict or list of item dicts</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> <code>IndexError</code> <p>If index is out of bounds</p> <code>ValueError</code> <p>If dataset is empty</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def __getitem__(self, idx: IndexLike) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Get item(s) by index. Returns dict with 'text' and label column(s) as keys.\n\n    For single label: {\"text\": \"...\", \"category\": \"...\"}\n    For multiple labels: {\"text\": \"...\", \"label1\": \"...\", \"label2\": \"...\"}\n\n    Args:\n        idx: Index (int), slice, or sequence of indices\n\n    Returns:\n        Single item dict or list of item dicts\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n        IndexError: If index is out of bounds\n        ValueError: If dataset is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\n            \"Indexing not supported for ITERABLE_ONLY datasets. Use iter_items or iter_batches.\"\n        )\n\n    dataset_len = len(self)\n    if dataset_len == 0:\n        raise ValueError(\"Cannot index into empty dataset\")\n\n    if isinstance(idx, int):\n        if idx &lt; 0:\n            idx = dataset_len + idx\n        if idx &lt; 0 or idx &gt;= dataset_len:\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {dataset_len}\")\n        row = self._ds[idx]\n        return self._extract_item_from_row(row)\n\n    if isinstance(idx, slice):\n        start, stop, step = idx.indices(dataset_len)\n        if step != 1:\n            indices = list(range(start, stop, step))\n            selected = self._ds.select(indices)\n        else:\n            selected = self._ds.select(range(start, stop))\n        return [self._extract_item_from_row(row) for row in selected]\n\n    if isinstance(idx, Sequence):\n        # Validate all indices are in bounds\n        invalid_indices = [i for i in idx if not (0 &lt;= i &lt; dataset_len)]\n        if invalid_indices:\n            raise IndexError(f\"Indices out of bounds: {invalid_indices} (dataset length: {dataset_len})\")\n        selected = self._ds.select(list(idx))\n        return [self._extract_item_from_row(row) for row in selected]\n\n    raise TypeError(f\"Invalid index type: {type(idx)}\")\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of items in the dataset.\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\"len() not supported for ITERABLE_ONLY datasets\")\n    return self._ds.num_rows\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.extract_texts_from_batch","title":"extract_texts_from_batch","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch of classification items.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Dict[str, Any]]</code> <p>List of dicts with 'text' and category fields</p> required <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of text strings from the batch</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'text' key is not found in any batch item</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def extract_texts_from_batch(self, batch: List[Dict[str, Any]]) -&gt; List[Optional[str]]:\n    \"\"\"Extract text strings from a batch of classification items.\n\n    Args:\n        batch: List of dicts with 'text' and category fields\n\n    Returns:\n        List of text strings from the batch\n\n    Raises:\n        ValueError: If 'text' key is not found in any batch item\n    \"\"\"\n    texts = []\n    for item in batch:\n        if \"text\" not in item:\n            raise ValueError(f\"'text' key not found in batch item. Available keys: {list(item.keys())}\")\n        texts.append(item[\"text\"])\n    return texts\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    category_field=\"category\",\n    delimiter=\",\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load classification dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column used for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = None\n    if drop_na:\n        cat_fields = [category_field] if isinstance(category_field, str) else category_field\n        drop_na_columns = [text_field] + list(cat_fields)\n\n    dataset = super().from_csv(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        delimiter=delimiter,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    repo_id,\n    store,\n    *,\n    split=\"train\",\n    loading_strategy=LoadingStrategy.MEMORY,\n    revision=None,\n    text_field=\"text\",\n    category_field=\"category\",\n    filters=None,\n    limit=None,\n    stratify_by=None,\n    stratify_seed=None,\n    streaming=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load classification dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision</p> <code>None</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional filters to apply (dict of column: value)</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of rows</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    streaming: Optional[bool] = None,\n    drop_na: bool = False,\n    **kwargs,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split\n        loading_strategy: Loading strategy\n        revision: Optional git revision\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n        filters: Optional filters to apply (dict of column: value)\n        limit: Optional limit on number of rows\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        streaming: Optional override for streaming\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        ValueError: If parameters are invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.ITERABLE_ONLY)\n\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\n            \"Stratification and drop_na are not supported for streaming datasets. Use MEMORY or DYNAMIC_LOAD.\"\n        )\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n\n        if use_streaming:\n            if filters or limit:\n                raise NotImplementedError(\n                    \"filters and limit are not supported when streaming datasets. Choose MEMORY or DYNAMIC_LOAD.\"\n                )\n        else:\n            drop_na_columns = None\n            if drop_na:\n                cat_fields = [category_field] if isinstance(category_field, str) else category_field\n                drop_na_columns = [text_field] + list(cat_fields)\n\n            ds = cls._postprocess_non_streaming_dataset(\n                ds,\n                filters=filters,\n                limit=limit,\n                stratify_by=stratify_by,\n                stratify_seed=stratify_seed,\n                drop_na_columns=drop_na_columns,\n            )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load classification dataset from HuggingFace Hub: \"\n            f\"repo_id={repo_id!r}, split={split!r}, text_field={text_field!r}, \"\n            f\"category_field={category_field!r}. Error: {e}\"\n        ) from e\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    category_field=\"category\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load classification dataset from JSON/JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the field(s) containing category/label</p> <code>'category'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from JSON/JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text\n        category_field: Name(s) of the field(s) containing category/label\n        stratify_by: Optional column used for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = None\n    if drop_na:\n        cat_fields = [category_field] if isinstance(category_field, str) else category_field\n        drop_na_columns = [text_field] + list(cat_fields)\n\n    dataset = super().from_json(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.get_all_texts","title":"get_all_texts","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of all text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY and dataset is very large</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def get_all_texts(self) -&gt; List[Optional[str]]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY and dataset is very large\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        return [item[\"text\"] for item in self.iter_items()]\n    return list(self._ds[self._text_field])\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.get_categories","title":"get_categories","text":"<pre><code>get_categories()\n</code></pre> <p>Get unique categories in the dataset, excluding None values.</p> <p>Returns:</p> Type Description <code>Union[List[Any], Dict[str, List[Any]]]</code> <ul> <li>For single label column: List of unique category values</li> </ul> <code>Union[List[Any], Dict[str, List[Any]]]</code> <ul> <li>For multiple label columns: Dict mapping column name to list of unique categories</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY and dataset is large</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def get_categories(self) -&gt; Union[List[Any], Dict[str, List[Any]]]:  # noqa: C901\n    \"\"\"\n    Get unique categories in the dataset, excluding None values.\n\n    Returns:\n        - For single label column: List of unique category values\n        - For multiple label columns: Dict mapping column name to list of unique categories\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY and dataset is large\n    \"\"\"\n    if len(self._category_fields) == 1:\n        # Single label: return list for backward compatibility\n        cat_field = self._category_fields[0]\n        if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n            categories = set()\n            for item in self.iter_items():\n                cat = item[cat_field]\n                if cat is not None:\n                    categories.add(cat)\n            return sorted(list(categories))  # noqa: C414\n        categories = [cat for cat in set(self._ds[cat_field]) if cat is not None]\n        return sorted(categories)\n    else:\n        # Multiple labels: return dict\n        result = {}\n        if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n            # Collect categories from all items\n            category_sets = {field: set() for field in self._category_fields}\n            for item in self.iter_items():\n                for field in self._category_fields:\n                    cat = item[field]\n                    if cat is not None:\n                        category_sets[field].add(cat)\n            for field in self._category_fields:\n                result[field] = sorted(list(category_sets[field]))  # noqa: C414\n        else:\n            # Use direct column access\n            for field in self._category_fields:\n                categories = [cat for cat in set(self._ds[field]) if cat is not None]\n                result[field] = sorted(categories)\n        return result\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.get_categories_for_texts","title":"get_categories_for_texts","text":"<pre><code>get_categories_for_texts(texts)\n</code></pre> <p>Get categories for given texts (if texts match dataset texts).</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[Optional[str]]</code> <p>List of text strings to look up</p> required <p>Returns:</p> Type Description <code>Union[List[Any], List[Dict[str, Any]]]</code> <ul> <li>For single label column: List of category values (one per text)</li> </ul> <code>Union[List[Any], List[Dict[str, Any]]]</code> <ul> <li>For multiple label columns: List of dicts with label columns as keys</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> <code>ValueError</code> <p>If texts list is empty</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def get_categories_for_texts(self, texts: List[Optional[str]]) -&gt; Union[List[Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Get categories for given texts (if texts match dataset texts).\n\n    Args:\n        texts: List of text strings to look up\n\n    Returns:\n        - For single label column: List of category values (one per text)\n        - For multiple label columns: List of dicts with label columns as keys\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n        ValueError: If texts list is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\"get_categories_for_texts not supported for ITERABLE_ONLY datasets\")\n\n    if not texts:\n        raise ValueError(\"texts list cannot be empty\")\n\n    if len(self._category_fields) == 1:\n        # Single label: return list for backward compatibility\n        cat_field = self._category_fields[0]\n        text_to_category = {row[self._text_field]: row[cat_field] for row in self._ds}\n        return [text_to_category.get(text) for text in texts]\n    else:\n        # Multiple labels: return list of dicts\n        text_to_categories = {\n            row[self._text_field]: {field: row[field] for field in self._category_fields} for row in self._ds\n        }\n        return [text_to_categories.get(text) for text in texts]\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over items in batches. Each batch is a list of dicts with 'text' and label column(s) as keys.</p> <p>For single label: [{\"text\": \"...\", \"category_column_1\": \"...\"}, ...] For multiple labels: [{\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}, ...]</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items per batch</p> required <p>Yields:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Lists of item dictionaries (batches)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt;= 0 or required fields are not found in any row</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Iterator[List[Dict[str, Any]]]:\n    \"\"\"\n    Iterate over items in batches. Each batch is a list of dicts with 'text' and label column(s) as keys.\n\n    For single label: [{\"text\": \"...\", \"category_column_1\": \"...\"}, ...]\n    For multiple labels: [{\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}, ...]\n\n    Args:\n        batch_size: Number of items per batch\n\n    Yields:\n        Lists of item dictionaries (batches)\n\n    Raises:\n        ValueError: If batch_size &lt;= 0 or required fields are not found in any row\n    \"\"\"\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be &gt; 0, got: {batch_size}\")\n\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        batch = []\n        for row in self._ds:\n            batch.append(self._extract_item_from_row(row))\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n    else:\n        # Use select to get batches with proper format\n        for i in range(0, len(self), batch_size):\n            end = min(i + batch_size, len(self))\n            batch_list = self[i:end]\n            yield batch_list\n</code></pre>"},{"location":"api/#amber.datasets.ClassificationDataset.iter_items","title":"iter_items","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over items one by one. Yields dict with 'text' and label column(s) as keys.</p> <p>For single label: {\"text\": \"...\", \"category_column_1\": \"...\"} For multiple labels: {\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Item dictionaries with text and category fields</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are not found in any row</p> Source code in <code>src/amber/datasets/classification_dataset.py</code> <pre><code>def iter_items(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Iterate over items one by one. Yields dict with 'text' and label column(s) as keys.\n\n    For single label: {\"text\": \"...\", \"category_column_1\": \"...\"}\n    For multiple labels: {\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}\n\n    Yields:\n        Item dictionaries with text and category fields\n\n    Raises:\n        ValueError: If required fields are not found in any row\n    \"\"\"\n    for row in self._ds:\n        yield self._extract_item_from_row(row)\n</code></pre>"},{"location":"api/#amber.datasets.LoadingStrategy","title":"LoadingStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Strategy for loading dataset data.</p> <p>Choose the best strategy for your use case:</p> <ul> <li> <p>MEMORY: Load entire dataset into memory (fastest random access, highest memory usage)   Best for: Small datasets that fit in memory, when you need fast random access</p> </li> <li> <p>DYNAMIC_LOAD: Save to disk, read dynamically via memory-mapped Arrow files   (supports len/getitem, lower memory usage)   Best for: Large datasets that don't fit in memory, when you need random access</p> </li> <li> <p>ITERABLE_ONLY: True streaming mode using IterableDataset (lowest memory, no len/getitem support)   Best for: Very large datasets, when you only need sequential iteration</p> </li> </ul>"},{"location":"api/#amber.datasets.TextDataset","title":"TextDataset","text":"<pre><code>TextDataset(\n    ds,\n    store,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n)\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Text-only dataset with support for multiple sources and loading strategies. Each item is a string (text snippet).</p> <p>Initialize text dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_field is empty or not found in dataset</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n):\n    \"\"\"\n    Initialize text dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n\n    Raises:\n        ValueError: If text_field is empty or not found in dataset\n    \"\"\"\n    self._validate_text_field(text_field)\n\n    # Validate and prepare dataset\n    is_iterable = isinstance(ds, IterableDataset)\n    if not is_iterable:\n        if text_field not in ds.column_names:\n            raise ValueError(f\"Dataset must have a '{text_field}' column; got columns: {ds.column_names}\")\n        # Keep only text column for memory efficiency\n        columns_to_remove = [c for c in ds.column_names if c != text_field]\n        if columns_to_remove:\n            ds = ds.remove_columns(columns_to_remove)\n        if text_field != \"text\":\n            ds = ds.rename_column(text_field, \"text\")\n        ds.set_format(\"python\", columns=[\"text\"])\n\n    self._text_field = text_field\n    super().__init__(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get text item(s) by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>IndexLike</code> <p>Index (int), slice, or sequence of indices</p> required <p>Returns:</p> Type Description <code>Union[Optional[str], List[Optional[str]]]</code> <p>Single text string or list of text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> <code>IndexError</code> <p>If index is out of bounds</p> <code>ValueError</code> <p>If dataset is empty</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def __getitem__(self, idx: IndexLike) -&gt; Union[Optional[str], List[Optional[str]]]:\n    \"\"\"\n    Get text item(s) by index.\n\n    Args:\n        idx: Index (int), slice, or sequence of indices\n\n    Returns:\n        Single text string or list of text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n        IndexError: If index is out of bounds\n        ValueError: If dataset is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\n            \"Indexing not supported for ITERABLE_ONLY datasets. Use iter_items or iter_batches.\"\n        )\n\n    dataset_len = len(self)\n    if dataset_len == 0:\n        raise ValueError(\"Cannot index into empty dataset\")\n\n    if isinstance(idx, int):\n        if idx &lt; 0:\n            idx = dataset_len + idx\n        if idx &lt; 0 or idx &gt;= dataset_len:\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {dataset_len}\")\n        return self._ds[idx][\"text\"]\n\n    if isinstance(idx, slice):\n        start, stop, step = idx.indices(dataset_len)\n        if step != 1:\n            indices = list(range(start, stop, step))\n            out = self._ds.select(indices)[\"text\"]\n        else:\n            out = self._ds.select(range(start, stop))[\"text\"]\n        return list(out)\n\n    if isinstance(idx, Sequence):\n        # Validate all indices are in bounds\n        invalid_indices = [i for i in idx if not (0 &lt;= i &lt; dataset_len)]\n        if invalid_indices:\n            raise IndexError(f\"Indices out of bounds: {invalid_indices} (dataset length: {dataset_len})\")\n        out = self._ds.select(list(idx))[\"text\"]\n        return list(out)\n\n    raise TypeError(f\"Invalid index type: {type(idx)}\")\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of items in the dataset.\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        raise NotImplementedError(\"len() not supported for ITERABLE_ONLY datasets\")\n    return self._ds.num_rows\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.extract_texts_from_batch","title":"extract_texts_from_batch","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch.</p> <p>For TextDataset, batch items are already strings, so return as-is.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Optional[str]]</code> <p>List of text strings</p> required <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of text strings (same as input)</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def extract_texts_from_batch(self, batch: List[Optional[str]]) -&gt; List[Optional[str]]:\n    \"\"\"Extract text strings from a batch.\n\n    For TextDataset, batch items are already strings, so return as-is.\n\n    Args:\n        batch: List of text strings\n\n    Returns:\n        List of text strings (same as input)\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    delimiter=\",\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load text dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column to use for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = [text_field] if drop_na else None\n    dataset = super().from_csv(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        delimiter=delimiter,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    repo_id,\n    store,\n    *,\n    split=\"train\",\n    loading_strategy=LoadingStrategy.MEMORY,\n    revision=None,\n    text_field=\"text\",\n    filters=None,\n    limit=None,\n    stratify_by=None,\n    stratify_seed=None,\n    streaming=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load text dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision</p> <code>None</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional filters to apply (dict of column: value)</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of rows</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for deterministic stratification</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    text_field: str = \"text\",\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    streaming: Optional[bool] = None,\n    drop_na: bool = False,\n    **kwargs,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split\n        loading_strategy: Loading strategy\n        revision: Optional git revision\n        text_field: Name of the column containing text\n        filters: Optional filters to apply (dict of column: value)\n        limit: Optional limit on number of rows\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for deterministic stratification\n        streaming: Optional override for streaming\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        ValueError: If parameters are invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.ITERABLE_ONLY)\n\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\n            \"Stratification and drop_na are not supported for streaming datasets. Use MEMORY or DYNAMIC_LOAD.\"\n        )\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n\n        if use_streaming:\n            if filters or limit:\n                raise NotImplementedError(\n                    \"filters and limit are not supported when streaming datasets. Choose MEMORY or DYNAMIC_LOAD.\"\n                )\n        else:\n            drop_na_columns = [text_field] if drop_na else None\n            ds = cls._postprocess_non_streaming_dataset(\n                ds,\n                filters=filters,\n                limit=limit,\n                stratify_by=stratify_by,\n                stratify_seed=stratify_seed,\n                drop_na_columns=drop_na_columns,\n            )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load text dataset from HuggingFace Hub: \"\n            f\"repo_id={repo_id!r}, split={split!r}, text_field={text_field!r}. \"\n            f\"Error: {e}\"\n        ) from e\n\n    return cls(ds, store=store, loading_strategy=loading_strategy, text_field=text_field)\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load text dataset from JSON/JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text</p> <code>'text'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from JSON/JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text\n        stratify_by: Optional column to use for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = [text_field] if drop_na else None\n    dataset = super().from_json(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    # Re-initialize with text_field\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    recursive=True\n)\n</code></pre> <p>Load from a local directory or file(s).</p> Supported <ul> <li>Directory of .txt files (each file becomes one example)</li> <li>JSONL/JSON/CSV/TSV files with a text column</li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column/field containing text</p> <code>'text'</code> <code>recursive</code> <code>bool</code> <p>Whether to recursively search directories for .txt files</p> <code>True</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If source path doesn't exist</p> <code>ValueError</code> <p>If source is invalid or unsupported file type</p> <code>RuntimeError</code> <p>If file operations fail</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_local(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    recursive: bool = True,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load from a local directory or file(s).\n\n    Supported:\n      - Directory of .txt files (each file becomes one example)\n      - JSONL/JSON/CSV/TSV files with a text column\n\n    Args:\n        source: Path to directory or file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column/field containing text\n        recursive: Whether to recursively search directories for .txt files\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If source path doesn't exist\n        ValueError: If source is invalid or unsupported file type\n        RuntimeError: If file operations fail\n    \"\"\"\n    p = Path(source)\n    if not p.exists():\n        raise FileNotFoundError(f\"Source path does not exist: {source}\")\n\n    if p.is_dir():\n        txts: List[str] = []\n        pattern = \"**/*.txt\" if recursive else \"*.txt\"\n        try:\n            for fp in sorted(p.glob(pattern)):\n                txts.append(fp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n        except OSError as e:\n            raise RuntimeError(f\"Failed to read text files from directory {source}. Error: {e}\") from e\n\n        if not txts:\n            raise ValueError(f\"No .txt files found in directory: {source} (recursive={recursive})\")\n\n        ds = Dataset.from_dict({\"text\": txts})\n    else:\n        suffix = p.suffix.lower()\n        if suffix in {\".jsonl\", \".json\"}:\n            return cls.from_json(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n            )\n        elif suffix in {\".csv\"}:\n            return cls.from_csv(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n            )\n        elif suffix in {\".tsv\"}:\n            return cls.from_csv(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n                delimiter=\"\\t\",\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported file type: {suffix} for source: {source}. \"\n                f\"Use directory of .txt, or JSON/JSONL/CSV/TSV.\"\n            )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy, text_field=text_field)\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.get_all_texts","title":"get_all_texts","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of all text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is ITERABLE_ONLY</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def get_all_texts(self) -&gt; List[Optional[str]]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is ITERABLE_ONLY\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        return list(self.iter_items())\n    return list(self._ds[\"text\"])\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over text items in batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items per batch</p> required <p>Yields:</p> Type Description <code>List[Optional[str]]</code> <p>Lists of text strings (batches)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt;= 0 or text field is not found in any row</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Iterator[List[Optional[str]]]:\n    \"\"\"\n    Iterate over text items in batches.\n\n    Args:\n        batch_size: Number of items per batch\n\n    Yields:\n        Lists of text strings (batches)\n\n    Raises:\n        ValueError: If batch_size &lt;= 0 or text field is not found in any row\n    \"\"\"\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be &gt; 0, got: {batch_size}\")\n\n    if self._loading_strategy == LoadingStrategy.ITERABLE_ONLY:\n        batch = []\n        for row in self._ds:\n            batch.append(self._extract_text_from_row(row))\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n    else:\n        for batch in self._ds.iter(batch_size=batch_size):\n            yield list(batch[\"text\"])\n</code></pre>"},{"location":"api/#amber.datasets.TextDataset.iter_items","title":"iter_items","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over text items one by one.</p> <p>Yields:</p> Type Description <code>Optional[str]</code> <p>Text strings from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text field is not found in any row</p> Source code in <code>src/amber/datasets/text_dataset.py</code> <pre><code>def iter_items(self) -&gt; Iterator[Optional[str]]:\n    \"\"\"\n    Iterate over text items one by one.\n\n    Yields:\n        Text strings from the dataset\n\n    Raises:\n        ValueError: If text field is not found in any row\n    \"\"\"\n    for row in self._ds:\n        yield self._extract_text_from_row(row)\n</code></pre>"},{"location":"api/#store-persistence","title":"Store / persistence","text":""},{"location":"api/#amber.store","title":"amber.store","text":""},{"location":"api/#amber.store.LocalStore","title":"LocalStore","text":"<pre><code>LocalStore(\n    base_path=\"\",\n    runs_prefix=\"runs\",\n    dataset_prefix=\"datasets\",\n    model_prefix=\"models\",\n)\n</code></pre> <p>               Bases: <code>Store</code></p> <p>Local filesystem implementation of Store interface.</p> <p>Initialize LocalStore.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path | str</code> <p>Base directory path for the store</p> <code>''</code> <code>runs_prefix</code> <code>str</code> <p>Prefix for runs directory</p> <code>'runs'</code> <code>dataset_prefix</code> <code>str</code> <p>Prefix for datasets directory</p> <code>'datasets'</code> <code>model_prefix</code> <code>str</code> <p>Prefix for models directory</p> <code>'models'</code> Source code in <code>src/amber/store/local_store.py</code> <pre><code>def __init__(\n        self,\n        base_path: Path | str = '',\n        runs_prefix: str = \"runs\",\n        dataset_prefix: str = \"datasets\",\n        model_prefix: str = \"models\",\n):\n    \"\"\"Initialize LocalStore.\n\n    Args:\n        base_path: Base directory path for the store\n        runs_prefix: Prefix for runs directory\n        dataset_prefix: Prefix for datasets directory\n        model_prefix: Prefix for models directory\n    \"\"\"\n    super().__init__(base_path, runs_prefix, dataset_prefix, model_prefix)\n</code></pre>"},{"location":"api/#amber.store.Store","title":"Store","text":"<pre><code>Store(\n    base_path=\"\",\n    runs_prefix=\"runs\",\n    dataset_prefix=\"datasets\",\n    model_prefix=\"models\",\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract store optimized for tensor batches grouped by run_id.</p> <p>This interface intentionally excludes generic bytes/JSON APIs. Implementations should focus on efficient safetensors-backed IO.</p> <p>The store organizes data hierarchically: - Runs: Top-level grouping by run_id - Batches: Within each run, data is organized by batch_index - Layers: Within each batch, tensors are organized by layer_signature - Keys: Within each layer, tensors are identified by key (e.g., \"activations\")</p> <p>Initialize Store.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path | str</code> <p>Base directory path for the store</p> <code>''</code> <code>runs_prefix</code> <code>str</code> <p>Prefix for runs directory (default: \"runs\")</p> <code>'runs'</code> <code>dataset_prefix</code> <code>str</code> <p>Prefix for datasets directory (default: \"datasets\")</p> <code>'datasets'</code> <code>model_prefix</code> <code>str</code> <p>Prefix for models directory (default: \"models\")</p> <code>'models'</code> Source code in <code>src/amber/store/store.py</code> <pre><code>def __init__(\n        self,\n        base_path: Path | str = \"\",\n        runs_prefix: str = \"runs\",\n        dataset_prefix: str = \"datasets\",\n        model_prefix: str = \"models\",\n):\n    \"\"\"Initialize Store.\n\n    Args:\n        base_path: Base directory path for the store\n        runs_prefix: Prefix for runs directory (default: \"runs\")\n        dataset_prefix: Prefix for datasets directory (default: \"datasets\")\n        model_prefix: Prefix for models directory (default: \"models\")\n    \"\"\"\n    self.runs_prefix = runs_prefix\n    self.dataset_prefix = dataset_prefix\n    self.model_prefix = model_prefix\n    self.base_path = Path(base_path)\n</code></pre>"},{"location":"api/#amber.store.Store.get_detector_metadata","title":"get_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>get_detector_metadata(run_id, batch_index)\n</code></pre> <p>Load detector metadata with separate JSON and tensor store.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>tuple[Dict[str, Any], TensorMetadata]</code> <p>Tuple of (metadata dict, tensor_metadata dict). Returns empty dicts if not found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata format is invalid</p> <code>JSONDecodeError</code> <p>If metadata file exists but contains invalid JSON</p> <code>OSError</code> <p>If tensor files exist but cannot be loaded</p> Source code in <code>src/amber/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_detector_metadata(\n        self,\n        run_id: str,\n        batch_index: int\n) -&gt; tuple[Dict[str, Any], TensorMetadata]:\n    \"\"\"Load detector metadata with separate JSON and tensor store.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index\n\n    Returns:\n        Tuple of (metadata dict, tensor_metadata dict). Returns empty dicts if not found.\n\n    Raises:\n        ValueError: If parameters are invalid or metadata format is invalid\n        json.JSONDecodeError: If metadata file exists but contains invalid JSON\n        OSError: If tensor files exist but cannot be loaded\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#amber.store.Store.get_detector_metadata_by_layer_by_key","title":"get_detector_metadata_by_layer_by_key  <code>abstractmethod</code>","text":"<pre><code>get_detector_metadata_by_layer_by_key(\n    run_id, batch_index, layer, key\n)\n</code></pre> <p>Get a specific tensor from detector metadata by layer and key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index</p> required <code>layer</code> <code>str</code> <p>Layer signature</p> required <code>key</code> <code>str</code> <p>Tensor key (e.g., \"activations\")</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The requested tensor</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>FileNotFoundError</code> <p>If the tensor doesn't exist</p> <code>OSError</code> <p>If tensor file exists but cannot be loaded</p> Source code in <code>src/amber/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_detector_metadata_by_layer_by_key(\n        self,\n        run_id: str,\n        batch_index: int,\n        layer: str,\n        key: str\n) -&gt; torch.Tensor:\n    \"\"\"Get a specific tensor from detector metadata by layer and key.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index\n        layer: Layer signature\n        key: Tensor key (e.g., \"activations\")\n\n    Returns:\n        The requested tensor\n\n    Raises:\n        ValueError: If parameters are invalid\n        FileNotFoundError: If the tensor doesn't exist\n        OSError: If tensor file exists but cannot be loaded\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#amber.store.Store.get_run_metadata","title":"get_run_metadata  <code>abstractmethod</code>","text":"<pre><code>get_run_metadata(run_id)\n</code></pre> <p>Load metadata for a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Metadata dictionary, or empty dict if not found</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_id is invalid</p> <code>JSONDecodeError</code> <p>If metadata file exists but contains invalid JSON</p> Source code in <code>src/amber/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_run_metadata(self, run_id: str) -&gt; Dict[str, Any]:\n    \"\"\"Load metadata for a run.\n\n    Args:\n        run_id: Run identifier\n\n    Returns:\n        Metadata dictionary, or empty dict if not found\n\n    Raises:\n        ValueError: If run_id is invalid\n        json.JSONDecodeError: If metadata file exists but contains invalid JSON\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#amber.store.Store.iter_run_batch_range","title":"iter_run_batch_range","text":"<pre><code>iter_run_batch_range(run_id, *, start=0, stop=None, step=1)\n</code></pre> <p>Iterate run batches for indices in range(start, stop, step).</p> <p>If stop is None, it will be set to max(list_run_batches(run_id)) + 1 (or 0 if none). Raises ValueError if step == 0 or start &lt; 0.</p> Source code in <code>src/amber/store/store.py</code> <pre><code>def iter_run_batch_range(\n        self,\n        run_id: str,\n        *,\n        start: int = 0,\n        stop: int | None = None,\n        step: int = 1,\n) -&gt; Iterator[List[torch.Tensor] | Dict[str, torch.Tensor]]:\n    \"\"\"Iterate run batches for indices in range(start, stop, step).\n\n    If stop is None, it will be set to max(list_run_batches(run_id)) + 1 (or 0 if none).\n    Raises ValueError if step == 0 or start &lt; 0.\n    \"\"\"\n    if step == 0:\n        raise ValueError(\"step must not be 0\")\n    if start &lt; 0:\n        raise ValueError(\"start must be &gt;= 0\")\n    indices = self.list_run_batches(run_id)\n    if not indices:\n        return\n    max_idx = max(indices)\n    if stop is None:\n        stop = max_idx + 1\n    for idx in range(start, stop, step):\n        try:\n            yield self.get_run_batch(run_id, idx)\n        except FileNotFoundError:\n            continue\n</code></pre>"},{"location":"api/#amber.store.Store.put_detector_metadata","title":"put_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>put_detector_metadata(\n    run_id, batch_index, metadata, tensor_metadata\n)\n</code></pre> <p>Save detector metadata with separate JSON and tensor store.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index (must be non-negative)</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>JSON-serializable metadata dictionary (aggregated from all detectors)</p> required <code>tensor_metadata</code> <code>TensorMetadata</code> <p>Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor            (from all detectors)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path key used for store (e.g., \"runs/{run_id}/batch_{batch_index}\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata is not JSON-serializable</p> <code>OSError</code> <p>If file system operations fail</p> Source code in <code>src/amber/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_detector_metadata(\n        self,\n        run_id: str,\n        batch_index: int,\n        metadata: Dict[str, Any],\n        tensor_metadata: TensorMetadata\n) -&gt; str:\n    \"\"\"Save detector metadata with separate JSON and tensor store.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index (must be non-negative)\n        metadata: JSON-serializable metadata dictionary (aggregated from all detectors)\n        tensor_metadata: Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor\n                       (from all detectors)\n\n    Returns:\n        Full path key used for store (e.g., \"runs/{run_id}/batch_{batch_index}\")\n\n    Raises:\n        ValueError: If parameters are invalid or metadata is not JSON-serializable\n        OSError: If file system operations fail\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#amber.store.Store.put_run_detector_metadata","title":"put_run_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>put_run_detector_metadata(\n    run_id, metadata, tensor_metadata\n)\n</code></pre> <p>Save detector metadata for a whole run in a unified location.</p> <p>This differs from <code>put_detector_metadata</code> which organises data per-batch under <code>runs/{run_id}/batch_{batch_index}</code>.</p> <p><code>put_run_detector_metadata</code> instead stores everything under <code>runs/{run_id}/detectors</code>. Implementations are expected to support being called multiple times for the same <code>run_id</code> and append / aggregate new metadata rather than overwrite it.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>JSON-serialisable metadata dictionary aggregated from all detectors for the current chunk / batch.</p> required <code>tensor_metadata</code> <code>TensorMetadata</code> <p>Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor (from all detectors).</p> required <p>Returns:</p> Type Description <code>str</code> <p>String path/key where metadata was stored</p> <code>str</code> <p>(e.g. <code>runs/{run_id}/detectors</code>).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata is not JSON\u2011serialisable.</p> <code>OSError</code> <p>If file system operations fail.</p> Source code in <code>src/amber/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_run_detector_metadata(\n        self,\n        run_id: str,\n        metadata: Dict[str, Any],\n        tensor_metadata: TensorMetadata,\n) -&gt; str:\n    \"\"\"\n    Save detector metadata for a whole run in a unified location.\n\n    This differs from ``put_detector_metadata`` which organises data\n    per-batch under ``runs/{run_id}/batch_{batch_index}``.\n\n    ``put_run_detector_metadata`` instead stores everything under\n    ``runs/{run_id}/detectors``. Implementations are expected to\n    support being called multiple times for the same ``run_id`` and\n    append / aggregate new metadata rather than overwrite it.\n\n    Args:\n        run_id: Run identifier\n        metadata: JSON-serialisable metadata dictionary aggregated\n            from all detectors for the current chunk / batch.\n        tensor_metadata: Dictionary mapping layer_signature to dict\n            of tensor_key -&gt; tensor (from all detectors).\n\n    Returns:\n        String path/key where metadata was stored\n        (e.g. ``runs/{run_id}/detectors``).\n\n    Raises:\n        ValueError: If parameters are invalid or metadata is not\n            JSON\u2011serialisable.\n        OSError: If file system operations fail.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#amber.store.Store.put_run_metadata","title":"put_run_metadata  <code>abstractmethod</code>","text":"<pre><code>put_run_metadata(run_id, meta)\n</code></pre> <p>Persist metadata for a run (e.g., dataset/model identifiers).</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>meta</code> <code>Dict[str, Any]</code> <p>Metadata dictionary to save (must be JSON-serializable)</p> required <p>Returns:</p> Type Description <code>str</code> <p>String path/key where metadata was stored (e.g., \"runs/{run_id}/meta.json\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_id is invalid or meta is not JSON-serializable</p> <code>OSError</code> <p>If file system operations fail</p> Note <p>Implementations should store JSON at a stable location, e.g., runs/{run_id}/meta.json.</p> Source code in <code>src/amber/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_run_metadata(self, run_id: str, meta: Dict[str, Any]) -&gt; str:\n    \"\"\"Persist metadata for a run (e.g., dataset/model identifiers).\n\n    Args:\n        run_id: Run identifier\n        meta: Metadata dictionary to save (must be JSON-serializable)\n\n    Returns:\n        String path/key where metadata was stored (e.g., \"runs/{run_id}/meta.json\")\n\n    Raises:\n        ValueError: If run_id is invalid or meta is not JSON-serializable\n        OSError: If file system operations fail\n\n    Note:\n        Implementations should store JSON at a stable location, e.g., runs/{run_id}/meta.json.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"}]}