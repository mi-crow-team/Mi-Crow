{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"mi_crow","text":"<p>mi_crow is a Python package for explaining and steering LLM behavior (SAE, concepts).</p> <ul> <li>Repo: GitHub</li> </ul>"},{"location":"api/","title":"Api","text":""},{"location":"api/#api-reference","title":"API Reference","text":"<p>mi_crow's public Python API is documented automatically from docstrings</p>"},{"location":"api/#mi_crow","title":"mi_crow","text":"<p>mi_crow: helper package for the Engineer Thesis project.</p> <p>This module is intentionally minimal. It exists to define the top-level package and to enable code coverage to include the package. Importing it should succeed without side effects.</p>"},{"location":"api/#mi_crow.ping","title":"ping","text":"<pre><code>ping()\n</code></pre> <p>Return a simple response to verify the package is wired correctly.</p> Source code in <code>src/mi_crow/__init__.py</code> <pre><code>def ping() -&gt; str:\n    \"\"\"Return a simple response to verify the package is wired correctly.\"\"\"\n    return \"pong\"\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>mi_crow's public Python API is documented automatically from docstrings.</p> <p>The top-level <code>mi_crow</code> package is intentionally minimal (it only exports things like <code>ping</code>). The real functionality lives in subpackages, which are documented in the sections below.</p>"},{"location":"api/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Language Model - Core language model API for loading models, running inference, and managing activations</li> <li>Mechanistic Interpretability (SAE) - Sparse Autoencoders, training, concepts, and related modules</li> <li>Datasets - Dataset loading and management utilities</li> <li>Store - Persistence layer for activations, models, and runs</li> <li>Hooks - Hook system for intercepting model activations</li> </ul>"},{"location":"api/#top-level-package","title":"Top-level Package","text":""},{"location":"api/#mi_crow","title":"mi_crow","text":"<p>mi_crow: helper package for the Engineer Thesis project.</p> <p>This module is intentionally minimal. It exists to define the top-level package and to enable code coverage to include the package. Importing it should succeed without side effects.</p>"},{"location":"api/#mi_crow.ping","title":"ping","text":"<pre><code>ping()\n</code></pre> <p>Return a simple response to verify the package is wired correctly.</p> Source code in <code>src/mi_crow/__init__.py</code> <pre><code>def ping() -&gt; str:\n    \"\"\"Return a simple response to verify the package is wired correctly.\"\"\"\n    return \"pong\"\n</code></pre>"},{"location":"api/datasets/","title":"Datasets API","text":"<p>Dataset loading and management utilities for text and classification datasets.</p>"},{"location":"api/datasets/#mi_crow.datasets","title":"mi_crow.datasets","text":""},{"location":"api/datasets/#mi_crow.datasets.BaseDataset","title":"BaseDataset","text":"<pre><code>BaseDataset(\n    ds, store, loading_strategy=LoadingStrategy.MEMORY\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for datasets with support for multiple sources, loading strategies, and Store integration.</p> <p>Loading Strategies: - MEMORY: Load entire dataset into memory (fastest random access, highest memory usage) - DISK: Save to disk, read dynamically via memory-mapped Arrow files   (supports len/getitem, lower memory usage) - STREAMING: True streaming mode using IterableDataset   (lowest memory, no len/getitem support, no stratification and limit support)</p> <p>Initialize dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance for caching/persistence</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>How to load data (MEMORY, DISK, or STREAMING)</p> <code>MEMORY</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is None, loading_strategy is invalid, or dataset operations fail</p> <code>OSError</code> <p>If file system operations fail</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n):\n    \"\"\"\n    Initialize dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance for caching/persistence\n        loading_strategy: How to load data (MEMORY, DISK, or STREAMING)\n\n    Raises:\n        ValueError: If store is None, loading_strategy is invalid, or dataset operations fail\n        OSError: If file system operations fail\n    \"\"\"\n    self._validate_initialization_params(store, loading_strategy)\n\n    self._store = store\n    self._loading_strategy = loading_strategy\n    self._dataset_dir: Path = Path(store.base_path) / store.dataset_prefix\n\n    is_iterable_input = isinstance(ds, IterableDataset)\n\n    if loading_strategy == LoadingStrategy.MEMORY:\n        # MEMORY: Convert to Dataset if needed, save to disk, load fully into memory\n        self._is_iterable = False\n        if is_iterable_input:\n            ds = Dataset.from_generator(lambda: iter(ds))\n        self._ds = self._save_and_load_dataset(ds, use_memory_mapping=False)\n    elif loading_strategy == LoadingStrategy.DISK:\n        # DISK: Save to disk, use memory-mapped Arrow files (supports len/getitem)\n        self._is_iterable = False\n        if is_iterable_input:\n            ds = Dataset.from_generator(lambda: iter(ds))\n        self._ds = self._save_and_load_dataset(ds, use_memory_mapping=True)\n    elif loading_strategy == LoadingStrategy.STREAMING:\n        # STREAMING: Convert to IterableDataset, don't save to disk (no len/getitem)\n        if not is_iterable_input:\n            ds = ds.to_iterable_dataset()\n        self._is_iterable = True\n        self._ds = ds\n        # Don't save to disk for iterable-only mode\n    else:\n        raise ValueError(\n            f\"Unknown loading strategy: {loading_strategy}. Must be one of: {[s.value for s in LoadingStrategy]}\"\n        )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.is_streaming","title":"is_streaming  <code>property</code>","text":"<pre><code>is_streaming\n</code></pre> <p>Whether this dataset is streaming (DISK or STREAMING).</p>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get item(s) by index.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, idx: IndexLike) -&gt; Any:\n    \"\"\"Get item(s) by index.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the number of items in the dataset.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.extract_texts_from_batch","title":"extract_texts_from_batch  <code>abstractmethod</code>","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Any]</code> <p>A batch as returned by iter_batches()</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of text strings ready for model inference</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef extract_texts_from_batch(self, batch: List[Any]) -&gt; List[str]:\n    \"\"\"Extract text strings from a batch.\n\n    Args:\n        batch: A batch as returned by iter_batches()\n\n    Returns:\n        List of text strings ready for model inference\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    delimiter=\",\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na_columns=None,\n    **kwargs\n)\n</code></pre> <p>Load dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to check for None/empty values</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>ValueError</code> <p>If store is None or source is invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na_columns: Optional[List[str]] = None,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na_columns: Optional list of columns to check for None/empty values\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        ValueError: If store is None or source is invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na_columns) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    ds = cls._load_csv_source(\n        source,\n        delimiter=delimiter,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    if not use_streaming and (stratify_by or drop_na_columns):\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    repo_id,\n    store,\n    *,\n    split=\"train\",\n    loading_strategy=LoadingStrategy.MEMORY,\n    revision=None,\n    streaming=None,\n    filters=None,\n    limit=None,\n    stratify_by=None,\n    stratify_seed=None,\n    **kwargs\n)\n</code></pre> <p>Load dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split (e.g., \"train\", \"validation\")</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy (MEMORY, DISK, or STREAMING)</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision/branch/tag</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming (if None, uses loading_strategy)</p> <code>None</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict of column-&gt;value pairs used for exact-match filtering</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional maximum number of rows to keep (applied after filtering/stratification)</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for deterministic stratification</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If repo_id is empty or store is None</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    streaming: Optional[bool] = None,\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split (e.g., \"train\", \"validation\")\n        loading_strategy: Loading strategy (MEMORY, DISK, or STREAMING)\n        revision: Optional git revision/branch/tag\n        streaming: Optional override for streaming (if None, uses loading_strategy)\n        filters: Optional dict of column-&gt;value pairs used for exact-match filtering\n        limit: Optional maximum number of rows to keep (applied after filtering/stratification)\n        stratify_by: Optional column to use for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for deterministic stratification\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        ValueError: If repo_id is empty or store is None\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if not repo_id or not isinstance(repo_id, str) or not repo_id.strip():\n        raise ValueError(f\"repo_id must be a non-empty string, got: {repo_id!r}\")\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    # Determine if we should use streaming for HuggingFace load_dataset\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.STREAMING)\n\n    if stratify_by and loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"Stratification is not supported for STREAMING datasets.\")\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load dataset from HuggingFace Hub: repo_id={repo_id!r}, \"\n            f\"split={split!r}, revision={revision!r}. Error: {e}\"\n        ) from e\n\n    if use_streaming:\n        if filters or limit or stratify_by:\n            raise NotImplementedError(\n                \"filters, limit, and stratification are not supported when streaming datasets. \"\n                \"Choose MEMORY or DISK loading strategy instead.\"\n            )\n    else:\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            filters=filters,\n            limit=limit,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na_columns=None,\n    **kwargs\n)\n</code></pre> <p>Load dataset from JSON or JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text (for JSON objects)</p> <code>'text'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na_columns</code> <code>Optional[List[str]]</code> <p>Optional list of columns to check for None/empty values</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'BaseDataset'</code> <p>BaseDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>ValueError</code> <p>If store is None or source is invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na_columns: Optional[List[str]] = None,\n    **kwargs: Any,\n) -&gt; \"BaseDataset\":\n    \"\"\"\n    Load dataset from JSON or JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text (for JSON objects)\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na_columns: Optional list of columns to check for None/empty values\n        **kwargs: Additional arguments passed to load_dataset\n\n    Returns:\n        BaseDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        ValueError: If store is None or source is invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    use_streaming = loading_strategy == LoadingStrategy.STREAMING\n    if (stratify_by or drop_na_columns) and use_streaming:\n        raise NotImplementedError(\"Stratification and drop_na are not supported for STREAMING datasets.\")\n\n    ds = cls._load_json_source(\n        source,\n        streaming=use_streaming,\n        **kwargs,\n    )\n\n    if not use_streaming and (stratify_by or drop_na_columns):\n        ds = cls._postprocess_non_streaming_dataset(\n            ds,\n            stratify_by=stratify_by,\n            stratify_seed=stratify_seed,\n            drop_na_columns=drop_na_columns,\n        )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.get_all_texts","title":"get_all_texts  <code>abstractmethod</code>","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of all text strings in the dataset</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not supported for streaming datasets</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef get_all_texts(self) -&gt; List[str]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings in the dataset\n\n    Raises:\n        NotImplementedError: If not supported for streaming datasets\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.get_batch","title":"get_batch","text":"<pre><code>get_batch(start, batch_size)\n</code></pre> <p>Get a contiguous batch of items.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Starting index</p> required <code>batch_size</code> <code>int</code> <p>Number of items to retrieve</p> required <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def get_batch(self, start: int, batch_size: int) -&gt; List[Any]:\n    \"\"\"\n    Get a contiguous batch of items.\n\n    Args:\n        start: Starting index\n        batch_size: Number of items to retrieve\n\n    Returns:\n        List of items\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"get_batch not supported for STREAMING datasets. Use iter_batches instead.\")\n    if batch_size &lt;= 0:\n        return []\n    end = min(start + batch_size, len(self))\n    if start &gt;= end:\n        return []\n    return self[start:end]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.head","title":"head","text":"<pre><code>head(n=5)\n</code></pre> <p>Get first n items.</p> <p>Works for all loading strategies.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to retrieve (default: 5)</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of first n items</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def head(self, n: int = 5) -&gt; List[Any]:\n    \"\"\"\n    Get first n items.\n\n    Works for all loading strategies.\n\n    Args:\n        n: Number of items to retrieve (default: 5)\n\n    Returns:\n        List of first n items\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        items = []\n        for i, item in enumerate(self.iter_items()):\n            if i &gt;= n:\n                break\n            items.append(item)\n        return items\n    return self[:n]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.iter_batches","title":"iter_batches  <code>abstractmethod</code>","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over items in batches.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef iter_batches(self, batch_size: int) -&gt; Iterator[List[Any]]:\n    \"\"\"Iterate over items in batches.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.iter_items","title":"iter_items  <code>abstractmethod</code>","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over items one by one.</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>@abstractmethod\ndef iter_items(self) -&gt; Iterator[Any]:\n    \"\"\"Iterate over items one by one.\"\"\"\n    pass\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.BaseDataset.sample","title":"sample","text":"<pre><code>sample(n=5)\n</code></pre> <p>Get n random items from the dataset.</p> <p>Works for MEMORY and DISK strategies only.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of items to sample</p> <code>5</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of n randomly sampled items</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/base_dataset.py</code> <pre><code>def sample(self, n: int = 5) -&gt; List[Any]:\n    \"\"\"\n    Get n random items from the dataset.\n\n    Works for MEMORY and DISK strategies only.\n\n    Args:\n        n: Number of items to sample\n\n    Returns:\n        List of n randomly sampled items\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\n            \"sample() not supported for STREAMING datasets. Use iter_items() and sample manually.\"\n        )\n\n    dataset_len = len(self)\n    if n &lt;= 0:\n        return []\n    if n &gt;= dataset_len:\n        # Return all items in random order\n        indices = list(range(dataset_len))\n        random.shuffle(indices)\n        return [self[i] for i in indices]\n\n    # Sample n random indices\n    indices = random.sample(range(dataset_len), n)\n    # Use __getitem__ with list of indices\n    return self[indices]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset","title":"ClassificationDataset","text":"<pre><code>ClassificationDataset(\n    ds,\n    store,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    category_field=\"category\",\n)\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Classification dataset with text and category/label columns. Each item is a dict with 'text' and label column(s) as keys. Supports single or multiple label columns.</p> <p>Initialize classification dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label.           Can be a single string or a list of strings for multiple labels.</p> <code>'category'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_field or category_field is empty, or fields not found in dataset</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n):\n    \"\"\"\n    Initialize classification dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label.\n                      Can be a single string or a list of strings for multiple labels.\n\n    Raises:\n        ValueError: If text_field or category_field is empty, or fields not found in dataset\n    \"\"\"\n    self._validate_text_field(text_field)\n\n    # Normalize category_field to list\n    if isinstance(category_field, str):\n        self._category_fields = [category_field]\n    else:\n        self._category_fields = list(category_field)\n\n    self._validate_category_fields(self._category_fields)\n\n    # Validate dataset\n    is_iterable = isinstance(ds, IterableDataset)\n    if not is_iterable:\n        if text_field not in ds.column_names:\n            raise ValueError(f\"Dataset must have a '{text_field}' column; got columns: {ds.column_names}\")\n        for cat_field in self._category_fields:\n            if cat_field not in ds.column_names:\n                raise ValueError(f\"Dataset must have a '{cat_field}' column; got columns: {ds.column_names}\")\n        # Set format with all required columns\n        format_columns = [text_field] + self._category_fields\n        ds.set_format(\"python\", columns=format_columns)\n\n    self._text_field = text_field\n    self._category_field = category_field  # Keep original for backward compatibility\n    super().__init__(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get item(s) by index. Returns dict with 'text' and label column(s) as keys.</p> <p>For single label: {\"text\": \"...\", \"category\": \"...\"} For multiple labels: {\"text\": \"...\", \"label1\": \"...\", \"label2\": \"...\"}</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>IndexLike</code> <p>Index (int), slice, or sequence of indices</p> required <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>Single item dict or list of item dicts</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>IndexError</code> <p>If index is out of bounds</p> <code>ValueError</code> <p>If dataset is empty</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def __getitem__(self, idx: IndexLike) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Get item(s) by index. Returns dict with 'text' and label column(s) as keys.\n\n    For single label: {\"text\": \"...\", \"category\": \"...\"}\n    For multiple labels: {\"text\": \"...\", \"label1\": \"...\", \"label2\": \"...\"}\n\n    Args:\n        idx: Index (int), slice, or sequence of indices\n\n    Returns:\n        Single item dict or list of item dicts\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        IndexError: If index is out of bounds\n        ValueError: If dataset is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\n            \"Indexing not supported for STREAMING datasets. Use iter_items or iter_batches.\"\n        )\n\n    dataset_len = len(self)\n    if dataset_len == 0:\n        raise ValueError(\"Cannot index into empty dataset\")\n\n    if isinstance(idx, int):\n        if idx &lt; 0:\n            idx = dataset_len + idx\n        if idx &lt; 0 or idx &gt;= dataset_len:\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {dataset_len}\")\n        row = self._ds[idx]\n        return self._extract_item_from_row(row)\n\n    if isinstance(idx, slice):\n        start, stop, step = idx.indices(dataset_len)\n        if step != 1:\n            indices = list(range(start, stop, step))\n            selected = self._ds.select(indices)\n        else:\n            selected = self._ds.select(range(start, stop))\n        return [self._extract_item_from_row(row) for row in selected]\n\n    if isinstance(idx, Sequence):\n        # Validate all indices are in bounds\n        invalid_indices = [i for i in idx if not (0 &lt;= i &lt; dataset_len)]\n        if invalid_indices:\n            raise IndexError(f\"Indices out of bounds: {invalid_indices} (dataset length: {dataset_len})\")\n        selected = self._ds.select(list(idx))\n        return [self._extract_item_from_row(row) for row in selected]\n\n    raise TypeError(f\"Invalid index type: {type(idx)}\")\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of items in the dataset.\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"len() not supported for STREAMING datasets\")\n    return self._ds.num_rows\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.extract_texts_from_batch","title":"extract_texts_from_batch","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch of classification items.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Dict[str, Any]]</code> <p>List of dicts with 'text' and category fields</p> required <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of text strings from the batch</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'text' key is not found in any batch item</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def extract_texts_from_batch(self, batch: List[Dict[str, Any]]) -&gt; List[Optional[str]]:\n    \"\"\"Extract text strings from a batch of classification items.\n\n    Args:\n        batch: List of dicts with 'text' and category fields\n\n    Returns:\n        List of text strings from the batch\n\n    Raises:\n        ValueError: If 'text' key is not found in any batch item\n    \"\"\"\n    texts = []\n    for item in batch:\n        if \"text\" not in item:\n            raise ValueError(f\"'text' key not found in batch item. Available keys: {list(item.keys())}\")\n        texts.append(item[\"text\"])\n    return texts\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    category_field=\"category\",\n    delimiter=\",\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load classification dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column used for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = None\n    if drop_na:\n        cat_fields = [category_field] if isinstance(category_field, str) else category_field\n        drop_na_columns = [text_field] + list(cat_fields)\n\n    dataset = super().from_csv(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        delimiter=delimiter,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    repo_id,\n    store,\n    *,\n    split=\"train\",\n    loading_strategy=LoadingStrategy.MEMORY,\n    revision=None,\n    text_field=\"text\",\n    category_field=\"category\",\n    filters=None,\n    limit=None,\n    stratify_by=None,\n    stratify_seed=None,\n    streaming=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load classification dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision</p> <code>None</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the column(s) containing category/label</p> <code>'category'</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional filters to apply (dict of column: value)</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of rows</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    streaming: Optional[bool] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split\n        loading_strategy: Loading strategy\n        revision: Optional git revision\n        text_field: Name of the column containing text\n        category_field: Name(s) of the column(s) containing category/label\n        filters: Optional filters to apply (dict of column: value)\n        limit: Optional limit on number of rows\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for stratified sampling\n        streaming: Optional override for streaming\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        ValueError: If parameters are invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.STREAMING)\n\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\n            \"Stratification and drop_na are not supported for streaming datasets. Use MEMORY or DISK.\"\n        )\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n\n        if use_streaming:\n            if filters or limit:\n                raise NotImplementedError(\n                    \"filters and limit are not supported when streaming datasets. Choose MEMORY or DISK.\"\n                )\n        else:\n            drop_na_columns = None\n            if drop_na:\n                cat_fields = [category_field] if isinstance(category_field, str) else category_field\n                drop_na_columns = [text_field] + list(cat_fields)\n\n            ds = cls._postprocess_non_streaming_dataset(\n                ds,\n                filters=filters,\n                limit=limit,\n                stratify_by=stratify_by,\n                stratify_seed=stratify_seed,\n                drop_na_columns=drop_na_columns,\n            )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load classification dataset from HuggingFace Hub: \"\n            f\"repo_id={repo_id!r}, split={split!r}, text_field={text_field!r}, \"\n            f\"category_field={category_field!r}. Error: {e}\"\n        ) from e\n\n    return cls(\n        ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    category_field=\"category\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load classification dataset from JSON/JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text</p> <code>'text'</code> <code>category_field</code> <code>Union[str, List[str]]</code> <p>Name(s) of the field(s) containing category/label</p> <code>'category'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text or categories</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'ClassificationDataset'</code> <p>ClassificationDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    category_field: Union[str, List[str]] = \"category\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"ClassificationDataset\":\n    \"\"\"\n    Load classification dataset from JSON/JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text\n        category_field: Name(s) of the field(s) containing category/label\n        stratify_by: Optional column used for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text or categories\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        ClassificationDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = None\n    if drop_na:\n        cat_fields = [category_field] if isinstance(category_field, str) else category_field\n        drop_na_columns = [text_field] + list(cat_fields)\n\n    dataset = super().from_json(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        category_field=category_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.get_all_texts","title":"get_all_texts","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of all text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING and dataset is very large</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def get_all_texts(self) -&gt; List[Optional[str]]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING and dataset is very large\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        return [item[\"text\"] for item in self.iter_items()]\n    return list(self._ds[self._text_field])\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.get_categories","title":"get_categories","text":"<pre><code>get_categories()\n</code></pre> <p>Get unique categories in the dataset, excluding None values.</p> <p>Returns:</p> Type Description <code>Union[List[Any], Dict[str, List[Any]]]</code> <ul> <li>For single label column: List of unique category values</li> </ul> <code>Union[List[Any], Dict[str, List[Any]]]</code> <ul> <li>For multiple label columns: Dict mapping column name to list of unique categories</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING and dataset is large</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def get_categories(self) -&gt; Union[List[Any], Dict[str, List[Any]]]:  # noqa: C901\n    \"\"\"\n    Get unique categories in the dataset, excluding None values.\n\n    Returns:\n        - For single label column: List of unique category values\n        - For multiple label columns: Dict mapping column name to list of unique categories\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING and dataset is large\n    \"\"\"\n    if len(self._category_fields) == 1:\n        # Single label: return list for backward compatibility\n        cat_field = self._category_fields[0]\n        if self._loading_strategy == LoadingStrategy.STREAMING:\n            categories = set()\n            for item in self.iter_items():\n                cat = item[cat_field]\n                if cat is not None:\n                    categories.add(cat)\n            return sorted(list(categories))  # noqa: C414\n        categories = [cat for cat in set(self._ds[cat_field]) if cat is not None]\n        return sorted(categories)\n    else:\n        # Multiple labels: return dict\n        result = {}\n        if self._loading_strategy == LoadingStrategy.STREAMING:\n            # Collect categories from all items\n            category_sets = {field: set() for field in self._category_fields}\n            for item in self.iter_items():\n                for field in self._category_fields:\n                    cat = item[field]\n                    if cat is not None:\n                        category_sets[field].add(cat)\n            for field in self._category_fields:\n                result[field] = sorted(list(category_sets[field]))  # noqa: C414\n        else:\n            # Use direct column access\n            for field in self._category_fields:\n                categories = [cat for cat in set(self._ds[field]) if cat is not None]\n                result[field] = sorted(categories)\n        return result\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.get_categories_for_texts","title":"get_categories_for_texts","text":"<pre><code>get_categories_for_texts(texts)\n</code></pre> <p>Get categories for given texts (if texts match dataset texts).</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[Optional[str]]</code> <p>List of text strings to look up</p> required <p>Returns:</p> Type Description <code>Union[List[Any], List[Dict[str, Any]]]</code> <ul> <li>For single label column: List of category values (one per text)</li> </ul> <code>Union[List[Any], List[Dict[str, Any]]]</code> <ul> <li>For multiple label columns: List of dicts with label columns as keys</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>ValueError</code> <p>If texts list is empty</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def get_categories_for_texts(self, texts: List[Optional[str]]) -&gt; Union[List[Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Get categories for given texts (if texts match dataset texts).\n\n    Args:\n        texts: List of text strings to look up\n\n    Returns:\n        - For single label column: List of category values (one per text)\n        - For multiple label columns: List of dicts with label columns as keys\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        ValueError: If texts list is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"get_categories_for_texts not supported for STREAMING datasets\")\n\n    if not texts:\n        raise ValueError(\"texts list cannot be empty\")\n\n    if len(self._category_fields) == 1:\n        # Single label: return list for backward compatibility\n        cat_field = self._category_fields[0]\n        text_to_category = {row[self._text_field]: row[cat_field] for row in self._ds}\n        return [text_to_category.get(text) for text in texts]\n    else:\n        # Multiple labels: return list of dicts\n        text_to_categories = {\n            row[self._text_field]: {field: row[field] for field in self._category_fields} for row in self._ds\n        }\n        return [text_to_categories.get(text) for text in texts]\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over items in batches. Each batch is a list of dicts with 'text' and label column(s) as keys.</p> <p>For single label: [{\"text\": \"...\", \"category_column_1\": \"...\"}, ...] For multiple labels: [{\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}, ...]</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items per batch</p> required <p>Yields:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Lists of item dictionaries (batches)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt;= 0 or required fields are not found in any row</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Iterator[List[Dict[str, Any]]]:\n    \"\"\"\n    Iterate over items in batches. Each batch is a list of dicts with 'text' and label column(s) as keys.\n\n    For single label: [{\"text\": \"...\", \"category_column_1\": \"...\"}, ...]\n    For multiple labels: [{\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}, ...]\n\n    Args:\n        batch_size: Number of items per batch\n\n    Yields:\n        Lists of item dictionaries (batches)\n\n    Raises:\n        ValueError: If batch_size &lt;= 0 or required fields are not found in any row\n    \"\"\"\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be &gt; 0, got: {batch_size}\")\n\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        batch = []\n        for row in self._ds:\n            batch.append(self._extract_item_from_row(row))\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n    else:\n        # Use select to get batches with proper format\n        for i in range(0, len(self), batch_size):\n            end = min(i + batch_size, len(self))\n            batch_list = self[i:end]\n            yield batch_list\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.ClassificationDataset.iter_items","title":"iter_items","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over items one by one. Yields dict with 'text' and label column(s) as keys.</p> <p>For single label: {\"text\": \"...\", \"category_column_1\": \"...\"} For multiple labels: {\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Item dictionaries with text and category fields</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required fields are not found in any row</p> Source code in <code>src/mi_crow/datasets/classification_dataset.py</code> <pre><code>def iter_items(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Iterate over items one by one. Yields dict with 'text' and label column(s) as keys.\n\n    For single label: {\"text\": \"...\", \"category_column_1\": \"...\"}\n    For multiple labels: {\"text\": \"...\", \"category_column_1\": \"...\", \"category_column_2\": \"...\"}\n\n    Yields:\n        Item dictionaries with text and category fields\n\n    Raises:\n        ValueError: If required fields are not found in any row\n    \"\"\"\n    for row in self._ds:\n        yield self._extract_item_from_row(row)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.LoadingStrategy","title":"LoadingStrategy","text":"<p>               Bases: <code>Enum</code></p> <p>Strategy for loading dataset data.</p> <p>Choose the best strategy for your use case:</p> <ul> <li> <p>MEMORY: Load entire dataset into memory (fastest random access, highest memory usage)   Best for: Small datasets that fit in memory, when you need fast random access</p> </li> <li> <p>DISK: Save to disk, read dynamically via memory-mapped Arrow files   (supports len/getitem, lower memory usage)   Best for: Large datasets that don't fit in memory, when you need random access</p> </li> <li> <p>STREAMING: True streaming mode using IterableDataset (lowest memory, no len/getitem support)   Best for: Very large datasets, when you only need sequential iteration</p> </li> </ul>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset","title":"TextDataset","text":"<pre><code>TextDataset(\n    ds,\n    store,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n)\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Text-only dataset with support for multiple sources and loading strategies. Each item is a string (text snippet).</p> <p>Initialize text dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset | IterableDataset</code> <p>HuggingFace Dataset or IterableDataset</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text_field is empty or not found in dataset</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def __init__(\n    self,\n    ds: Dataset | IterableDataset,\n    store: Store,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n):\n    \"\"\"\n    Initialize text dataset.\n\n    Args:\n        ds: HuggingFace Dataset or IterableDataset\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n\n    Raises:\n        ValueError: If text_field is empty or not found in dataset\n    \"\"\"\n    self._validate_text_field(text_field)\n\n    # Validate and prepare dataset\n    is_iterable = isinstance(ds, IterableDataset)\n    if not is_iterable:\n        if text_field not in ds.column_names:\n            raise ValueError(f\"Dataset must have a '{text_field}' column; got columns: {ds.column_names}\")\n        # Keep only text column for memory efficiency\n        columns_to_remove = [c for c in ds.column_names if c != text_field]\n        if columns_to_remove:\n            ds = ds.remove_columns(columns_to_remove)\n        if text_field != \"text\":\n            ds = ds.rename_column(text_field, \"text\")\n        ds.set_format(\"python\", columns=[\"text\"])\n\n    self._text_field = text_field\n    super().__init__(ds, store=store, loading_strategy=loading_strategy)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> <p>Get text item(s) by index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>IndexLike</code> <p>Index (int), slice, or sequence of indices</p> required <p>Returns:</p> Type Description <code>Union[Optional[str], List[Optional[str]]]</code> <p>Single text string or list of text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> <code>IndexError</code> <p>If index is out of bounds</p> <code>ValueError</code> <p>If dataset is empty</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def __getitem__(self, idx: IndexLike) -&gt; Union[Optional[str], List[Optional[str]]]:\n    \"\"\"\n    Get text item(s) by index.\n\n    Args:\n        idx: Index (int), slice, or sequence of indices\n\n    Returns:\n        Single text string or list of text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n        IndexError: If index is out of bounds\n        ValueError: If dataset is empty\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\n            \"Indexing not supported for STREAMING datasets. Use iter_items or iter_batches.\"\n        )\n\n    dataset_len = len(self)\n    if dataset_len == 0:\n        raise ValueError(\"Cannot index into empty dataset\")\n\n    if isinstance(idx, int):\n        if idx &lt; 0:\n            idx = dataset_len + idx\n        if idx &lt; 0 or idx &gt;= dataset_len:\n            raise IndexError(f\"Index {idx} out of bounds for dataset of length {dataset_len}\")\n        return self._ds[idx][\"text\"]\n\n    if isinstance(idx, slice):\n        start, stop, step = idx.indices(dataset_len)\n        if step != 1:\n            indices = list(range(start, stop, step))\n            out = self._ds.select(indices)[\"text\"]\n        else:\n            out = self._ds.select(range(start, stop))[\"text\"]\n        return list(out)\n\n    if isinstance(idx, Sequence):\n        # Validate all indices are in bounds\n        invalid_indices = [i for i in idx if not (0 &lt;= i &lt; dataset_len)]\n        if invalid_indices:\n            raise IndexError(f\"Indices out of bounds: {invalid_indices} (dataset length: {dataset_len})\")\n        out = self._ds.select(list(idx))[\"text\"]\n        return list(out)\n\n    raise TypeError(f\"Invalid index type: {type(idx)}\")\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of items in the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of items in the dataset.\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        raise NotImplementedError(\"len() not supported for STREAMING datasets\")\n    return self._ds.num_rows\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.extract_texts_from_batch","title":"extract_texts_from_batch","text":"<pre><code>extract_texts_from_batch(batch)\n</code></pre> <p>Extract text strings from a batch.</p> <p>For TextDataset, batch items are already strings, so return as-is.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Optional[str]]</code> <p>List of text strings</p> required <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of text strings (same as input)</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def extract_texts_from_batch(self, batch: List[Optional[str]]) -&gt; List[Optional[str]]:\n    \"\"\"Extract text strings from a batch.\n\n    For TextDataset, batch items are already strings, so return as-is.\n\n    Args:\n        batch: List of text strings\n\n    Returns:\n        List of text strings (same as input)\n    \"\"\"\n    return batch\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    delimiter=\",\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load text dataset from CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to CSV file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If CSV file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    delimiter: str = \",\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from CSV file.\n\n    Args:\n        source: Path to CSV file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column containing text\n        delimiter: CSV delimiter (default: comma)\n        stratify_by: Optional column to use for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If CSV file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = [text_field] if drop_na else None\n    dataset = super().from_csv(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        delimiter=delimiter,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    repo_id,\n    store,\n    *,\n    split=\"train\",\n    loading_strategy=LoadingStrategy.MEMORY,\n    revision=None,\n    text_field=\"text\",\n    filters=None,\n    limit=None,\n    stratify_by=None,\n    stratify_seed=None,\n    streaming=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load text dataset from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>HuggingFace dataset repository ID</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>split</code> <code>str</code> <p>Dataset split</p> <code>'train'</code> <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>revision</code> <code>Optional[str]</code> <p>Optional git revision</p> <code>None</code> <code>text_field</code> <code>str</code> <p>Name of the column containing text</p> <code>'text'</code> <code>filters</code> <code>Optional[Dict[str, Any]]</code> <p>Optional filters to apply (dict of column: value)</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Optional limit on number of rows</p> <code>None</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column used for stratified sampling (non-streaming only)</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for deterministic stratification</p> <code>None</code> <code>streaming</code> <code>Optional[bool]</code> <p>Optional override for streaming</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_huggingface(\n    cls,\n    repo_id: str,\n    store: Store,\n    *,\n    split: str = \"train\",\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    revision: Optional[str] = None,\n    text_field: str = \"text\",\n    filters: Optional[Dict[str, Any]] = None,\n    limit: Optional[int] = None,\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    streaming: Optional[bool] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from HuggingFace Hub.\n\n    Args:\n        repo_id: HuggingFace dataset repository ID\n        store: Store instance\n        split: Dataset split\n        loading_strategy: Loading strategy\n        revision: Optional git revision\n        text_field: Name of the column containing text\n        filters: Optional filters to apply (dict of column: value)\n        limit: Optional limit on number of rows\n        stratify_by: Optional column used for stratified sampling (non-streaming only)\n        stratify_seed: Optional RNG seed for deterministic stratification\n        streaming: Optional override for streaming\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        ValueError: If parameters are invalid\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    use_streaming = streaming if streaming is not None else (loading_strategy == LoadingStrategy.STREAMING)\n\n    if (stratify_by or drop_na) and use_streaming:\n        raise NotImplementedError(\n            \"Stratification and drop_na are not supported for streaming datasets. Use MEMORY or DISK.\"\n        )\n\n    try:\n        ds = load_dataset(\n            path=repo_id,\n            split=split,\n            revision=revision,\n            streaming=use_streaming,\n            **kwargs,\n        )\n\n        if use_streaming:\n            if filters or limit:\n                raise NotImplementedError(\n                    \"filters and limit are not supported when streaming datasets. Choose MEMORY or DISK.\"\n                )\n        else:\n            drop_na_columns = [text_field] if drop_na else None\n            ds = cls._postprocess_non_streaming_dataset(\n                ds,\n                filters=filters,\n                limit=limit,\n                stratify_by=stratify_by,\n                stratify_seed=stratify_seed,\n                drop_na_columns=drop_na_columns,\n            )\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load text dataset from HuggingFace Hub: \"\n            f\"repo_id={repo_id!r}, split={split!r}, text_field={text_field!r}. \"\n            f\"Error: {e}\"\n        ) from e\n\n    return cls(ds, store=store, loading_strategy=loading_strategy, text_field=text_field)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    stratify_by=None,\n    stratify_seed=None,\n    drop_na=False,\n    **kwargs\n)\n</code></pre> <p>Load text dataset from JSON/JSONL file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to JSON or JSONL file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the field containing text</p> <code>'text'</code> <code>stratify_by</code> <code>Optional[str]</code> <p>Optional column to use for stratified sampling</p> <code>None</code> <code>stratify_seed</code> <code>Optional[int]</code> <p>Optional RNG seed for stratified sampling</p> <code>None</code> <code>drop_na</code> <code>bool</code> <p>Whether to drop rows with None/empty text</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for load_dataset</p> <code>{}</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If JSON file doesn't exist</p> <code>RuntimeError</code> <p>If dataset loading fails</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    stratify_by: Optional[str] = None,\n    stratify_seed: Optional[int] = None,\n    drop_na: bool = False,\n    **kwargs: Any,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load text dataset from JSON/JSONL file.\n\n    Args:\n        source: Path to JSON or JSONL file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the field containing text\n        stratify_by: Optional column to use for stratified sampling\n        stratify_seed: Optional RNG seed for stratified sampling\n        drop_na: Whether to drop rows with None/empty text\n        **kwargs: Additional arguments for load_dataset\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If JSON file doesn't exist\n        RuntimeError: If dataset loading fails\n    \"\"\"\n    drop_na_columns = [text_field] if drop_na else None\n    dataset = super().from_json(\n        source,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n        stratify_by=stratify_by,\n        stratify_seed=stratify_seed,\n        drop_na_columns=drop_na_columns,\n        **kwargs,\n    )\n    # Re-initialize with text_field\n    return cls(\n        dataset._ds,\n        store=store,\n        loading_strategy=loading_strategy,\n        text_field=text_field,\n    )\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(\n    source,\n    store,\n    *,\n    loading_strategy=LoadingStrategy.MEMORY,\n    text_field=\"text\",\n    recursive=True\n)\n</code></pre> <p>Load from a local directory or file(s).</p> Supported <ul> <li>Directory of .txt files (each file becomes one example)</li> <li>JSONL/JSON/CSV/TSV files with a text column</li> </ul> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Path]</code> <p>Path to directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance</p> required <code>loading_strategy</code> <code>LoadingStrategy</code> <p>Loading strategy</p> <code>MEMORY</code> <code>text_field</code> <code>str</code> <p>Name of the column/field containing text</p> <code>'text'</code> <code>recursive</code> <code>bool</code> <p>Whether to recursively search directories for .txt files</p> <code>True</code> <p>Returns:</p> Type Description <code>'TextDataset'</code> <p>TextDataset instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If source path doesn't exist</p> <code>ValueError</code> <p>If source is invalid or unsupported file type</p> <code>RuntimeError</code> <p>If file operations fail</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>@classmethod\ndef from_local(\n    cls,\n    source: Union[str, Path],\n    store: Store,\n    *,\n    loading_strategy: LoadingStrategy = LoadingStrategy.MEMORY,\n    text_field: str = \"text\",\n    recursive: bool = True,\n) -&gt; \"TextDataset\":\n    \"\"\"\n    Load from a local directory or file(s).\n\n    Supported:\n      - Directory of .txt files (each file becomes one example)\n      - JSONL/JSON/CSV/TSV files with a text column\n\n    Args:\n        source: Path to directory or file\n        store: Store instance\n        loading_strategy: Loading strategy\n        text_field: Name of the column/field containing text\n        recursive: Whether to recursively search directories for .txt files\n\n    Returns:\n        TextDataset instance\n\n    Raises:\n        FileNotFoundError: If source path doesn't exist\n        ValueError: If source is invalid or unsupported file type\n        RuntimeError: If file operations fail\n    \"\"\"\n    p = Path(source)\n    if not p.exists():\n        raise FileNotFoundError(f\"Source path does not exist: {source}\")\n\n    if p.is_dir():\n        txts: List[str] = []\n        pattern = \"**/*.txt\" if recursive else \"*.txt\"\n        try:\n            for fp in sorted(p.glob(pattern)):\n                txts.append(fp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n        except OSError as e:\n            raise RuntimeError(f\"Failed to read text files from directory {source}. Error: {e}\") from e\n\n        if not txts:\n            raise ValueError(f\"No .txt files found in directory: {source} (recursive={recursive})\")\n\n        ds = Dataset.from_dict({\"text\": txts})\n    else:\n        suffix = p.suffix.lower()\n        if suffix in {\".jsonl\", \".json\"}:\n            return cls.from_json(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n            )\n        elif suffix in {\".csv\"}:\n            return cls.from_csv(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n            )\n        elif suffix in {\".tsv\"}:\n            return cls.from_csv(\n                source,\n                store=store,\n                loading_strategy=loading_strategy,\n                text_field=text_field,\n                delimiter=\"\\t\",\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported file type: {suffix} for source: {source}. \"\n                f\"Use directory of .txt, or JSON/JSONL/CSV/TSV.\"\n            )\n\n    return cls(ds, store=store, loading_strategy=loading_strategy, text_field=text_field)\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.get_all_texts","title":"get_all_texts","text":"<pre><code>get_all_texts()\n</code></pre> <p>Get all texts from the dataset.</p> <p>Returns:</p> Type Description <code>List[Optional[str]]</code> <p>List of all text strings</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If loading_strategy is STREAMING</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def get_all_texts(self) -&gt; List[Optional[str]]:\n    \"\"\"Get all texts from the dataset.\n\n    Returns:\n        List of all text strings\n\n    Raises:\n        NotImplementedError: If loading_strategy is STREAMING\n    \"\"\"\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        return list(self.iter_items())\n    return list(self._ds[\"text\"])\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.iter_batches","title":"iter_batches","text":"<pre><code>iter_batches(batch_size)\n</code></pre> <p>Iterate over text items in batches.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of items per batch</p> required <p>Yields:</p> Type Description <code>List[Optional[str]]</code> <p>Lists of text strings (batches)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batch_size &lt;= 0 or text field is not found in any row</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def iter_batches(self, batch_size: int) -&gt; Iterator[List[Optional[str]]]:\n    \"\"\"\n    Iterate over text items in batches.\n\n    Args:\n        batch_size: Number of items per batch\n\n    Yields:\n        Lists of text strings (batches)\n\n    Raises:\n        ValueError: If batch_size &lt;= 0 or text field is not found in any row\n    \"\"\"\n    if batch_size &lt;= 0:\n        raise ValueError(f\"batch_size must be &gt; 0, got: {batch_size}\")\n\n    if self._loading_strategy == LoadingStrategy.STREAMING:\n        batch = []\n        for row in self._ds:\n            batch.append(self._extract_text_from_row(row))\n            if len(batch) &gt;= batch_size:\n                yield batch\n                batch = []\n        if batch:\n            yield batch\n    else:\n        for batch in self._ds.iter(batch_size=batch_size):\n            yield list(batch[\"text\"])\n</code></pre>"},{"location":"api/datasets/#mi_crow.datasets.TextDataset.iter_items","title":"iter_items","text":"<pre><code>iter_items()\n</code></pre> <p>Iterate over text items one by one.</p> <p>Yields:</p> Type Description <code>Optional[str]</code> <p>Text strings from the dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If text field is not found in any row</p> Source code in <code>src/mi_crow/datasets/text_dataset.py</code> <pre><code>def iter_items(self) -&gt; Iterator[Optional[str]]:\n    \"\"\"\n    Iterate over text items one by one.\n\n    Yields:\n        Text strings from the dataset\n\n    Raises:\n        ValueError: If text field is not found in any row\n    \"\"\"\n    for row in self._ds:\n        yield self._extract_text_from_row(row)\n</code></pre>"},{"location":"api/hooks/","title":"Hooks API","text":"<p>Hook system for intercepting and managing model activations during inference.</p>"},{"location":"api/hooks/#core-hook-classes","title":"Core Hook Classes","text":""},{"location":"api/hooks/#mi_crow.hooks.hook.Hook","title":"mi_crow.hooks.hook.Hook","text":"<pre><code>Hook(\n    layer_signature=None,\n    hook_type=HookType.FORWARD,\n    hook_id=None,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for hooks that can be registered on language model layers.</p> <p>Hooks provide a way to intercept and process activations during model inference. They expose PyTorch-compatible callables via get_torch_hook() while providing additional functionality like enable/disable and unique identification.</p> <p>Initialize a hook.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Layer name or index to attach hook to</p> <code>None</code> <code>hook_type</code> <code>HookType | str</code> <p>Type of hook - HookType.FORWARD or HookType.PRE_FORWARD</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier (auto-generated if not provided)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hook_type string is invalid</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def __init__(\n        self,\n        layer_signature: str | int | None = None,\n        hook_type: HookType | str = HookType.FORWARD,\n        hook_id: str | None = None\n):\n    \"\"\"\n    Initialize a hook.\n\n    Args:\n        layer_signature: Layer name or index to attach hook to\n        hook_type: Type of hook - HookType.FORWARD or HookType.PRE_FORWARD\n        hook_id: Unique identifier (auto-generated if not provided)\n\n    Raises:\n        ValueError: If hook_type string is invalid\n    \"\"\"\n    self.layer_signature = layer_signature\n    self.hook_type = self._normalize_hook_type(hook_type)\n    self.id = hook_id if hook_id is not None else str(uuid.uuid4())\n    self._enabled = True\n    self._torch_hook_handle = None\n    self._context: Optional[\"LanguageModelContext\"] = None\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.context","title":"context  <code>property</code>","text":"<pre><code>context\n</code></pre> <p>Get the LanguageModelContext associated with this hook.</p>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.enabled","title":"enabled  <code>property</code>","text":"<pre><code>enabled\n</code></pre> <p>Whether this hook is currently enabled.</p>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.disable","title":"disable","text":"<pre><code>disable()\n</code></pre> <p>Disable this hook.</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def disable(self) -&gt; None:\n    \"\"\"Disable this hook.\"\"\"\n    self._enabled = False\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.enable","title":"enable","text":"<pre><code>enable()\n</code></pre> <p>Enable this hook.</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def enable(self) -&gt; None:\n    \"\"\"Enable this hook.\"\"\"\n    self._enabled = True\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.get_torch_hook","title":"get_torch_hook","text":"<pre><code>get_torch_hook()\n</code></pre> <p>Return a PyTorch-compatible hook function.</p> <p>The returned callable will check the enabled flag before executing and call the abstract _hook_fn method.</p> <p>Returns:</p> Type Description <code>Callable</code> <p>A callable compatible with PyTorch's register_forward_hook or</p> <code>Callable</code> <p>register_forward_pre_hook APIs.</p> Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def get_torch_hook(self) -&gt; Callable:\n    \"\"\"\n    Return a PyTorch-compatible hook function.\n\n    The returned callable will check the enabled flag before executing\n    and call the abstract _hook_fn method.\n\n    Returns:\n        A callable compatible with PyTorch's register_forward_hook or\n        register_forward_pre_hook APIs.\n    \"\"\"\n    if self.hook_type == HookType.PRE_FORWARD:\n        return self._create_pre_forward_wrapper()\n    else:\n        return self._create_forward_wrapper()\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.hook.Hook.set_context","title":"set_context","text":"<pre><code>set_context(context)\n</code></pre> <p>Set the LanguageModelContext for this hook.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'LanguageModelContext'</code> <p>The LanguageModelContext instance</p> required Source code in <code>src/mi_crow/hooks/hook.py</code> <pre><code>def set_context(self, context: \"LanguageModelContext\") -&gt; None:\n    \"\"\"Set the LanguageModelContext for this hook.\n\n    Args:\n        context: The LanguageModelContext instance\n    \"\"\"\n    self._context = context\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.detector.Detector","title":"mi_crow.hooks.detector.Detector","text":"<pre><code>Detector(\n    hook_type=HookType.FORWARD,\n    hook_id=None,\n    store=None,\n    layer_signature=None,\n)\n</code></pre> <p>               Bases: <code>Hook</code></p> <p>Abstract base class for detector hooks that collect metadata during inference.</p> <p>Detectors can accumulate data across batches and optionally save it to a Store. They are designed to observe and record information without modifying activations.</p> <p>Initialize a detector hook.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HookType | str</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier</p> <code>None</code> <code>store</code> <code>Store | None</code> <p>Optional Store for saving metadata</p> <code>None</code> <code>layer_signature</code> <code>str | int | None</code> <p>Layer to attach to (optional, for compatibility)</p> <code>None</code> Source code in <code>src/mi_crow/hooks/detector.py</code> <pre><code>def __init__(\n        self,\n        hook_type: HookType | str = HookType.FORWARD,\n        hook_id: str | None = None,\n        store: Store | None = None,\n        layer_signature: str | int | None = None\n):\n    \"\"\"\n    Initialize a detector hook.\n\n    Args:\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)\n        hook_id: Unique identifier\n        store: Optional Store for saving metadata\n        layer_signature: Layer to attach to (optional, for compatibility)\n    \"\"\"\n    super().__init__(layer_signature=layer_signature, hook_type=hook_type, hook_id=hook_id)\n    self.store = store\n    self.metadata: Dict[str, Any] = {}\n    self.tensor_metadata: Dict[str, torch.Tensor] = {}\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.detector.Detector.process_activations","title":"process_activations  <code>abstractmethod</code>","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Process activations from the hooked layer.</p> <p>This is where detector-specific logic goes (e.g., tracking top activations, computing statistics, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>Subclasses may raise exceptions for invalid inputs or processing errors</p> Source code in <code>src/mi_crow/hooks/detector.py</code> <pre><code>@abc.abstractmethod\ndef process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Process activations from the hooked layer.\n\n    This is where detector-specific logic goes (e.g., tracking top activations,\n    computing statistics, etc.).\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n\n    Raises:\n        Exception: Subclasses may raise exceptions for invalid inputs or processing errors\n    \"\"\"\n    raise NotImplementedError(\"process_activations must be implemented by subclasses\")\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.controller.Controller","title":"mi_crow.hooks.controller.Controller","text":"<pre><code>Controller(\n    hook_type=HookType.FORWARD,\n    hook_id=None,\n    layer_signature=None,\n)\n</code></pre> <p>               Bases: <code>Hook</code></p> <p>Abstract base class for controller hooks that modify activations during inference.</p> <p>Controllers can modify inputs (pre_forward) or outputs (forward) of layers. They are designed to actively change the behavior of the model during inference.</p> <p>Initialize a controller hook.</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>HookType | str</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier</p> <code>None</code> <code>layer_signature</code> <code>str | int | None</code> <p>Layer to attach to (optional, for compatibility)</p> <code>None</code> Source code in <code>src/mi_crow/hooks/controller.py</code> <pre><code>def __init__(\n        self,\n        hook_type: HookType | str = HookType.FORWARD,\n        hook_id: str | None = None,\n        layer_signature: str | int | None = None\n):\n    \"\"\"\n    Initialize a controller hook.\n\n    Args:\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)\n        hook_id: Unique identifier\n        layer_signature: Layer to attach to (optional, for compatibility)\n    \"\"\"\n    super().__init__(layer_signature=layer_signature, hook_type=hook_type, hook_id=hook_id)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.controller.Controller.modify_activations","title":"modify_activations  <code>abstractmethod</code>","text":"<pre><code>modify_activations(module, inputs, output)\n</code></pre> <p>Modify activations from the hooked layer.</p> <p>For pre_forward hooks: receives input tensor, should return modified input tensor. For forward hooks: receives input and output tensors, should return modified output tensor.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>inputs</code> <code>Tensor | None</code> <p>Input tensor (None for forward hooks if not available)</p> required <code>output</code> <code>Tensor | None</code> <p>Output tensor (None for pre_forward hooks)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>Modified input tensor (for pre_forward) or modified output tensor (for forward).</p> <code>Tensor | None</code> <p>Return None to keep original tensor unchanged.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Subclasses may raise exceptions for invalid inputs or modification errors</p> Source code in <code>src/mi_crow/hooks/controller.py</code> <pre><code>@abc.abstractmethod\ndef modify_activations(\n        self,\n        module: nn.Module,\n        inputs: torch.Tensor | None,\n        output: torch.Tensor | None\n) -&gt; torch.Tensor | None:\n    \"\"\"\n    Modify activations from the hooked layer.\n\n    For pre_forward hooks: receives input tensor, should return modified input tensor.\n    For forward hooks: receives input and output tensors, should return modified output tensor.\n\n    Args:\n        module: The PyTorch module being hooked\n        inputs: Input tensor (None for forward hooks if not available)\n        output: Output tensor (None for pre_forward hooks)\n\n    Returns:\n        Modified input tensor (for pre_forward) or modified output tensor (for forward).\n        Return None to keep original tensor unchanged.\n\n    Raises:\n        Exception: Subclasses may raise exceptions for invalid inputs or modification errors\n    \"\"\"\n    raise NotImplementedError(\"modify_activations must be implemented by subclasses\")\n</code></pre>"},{"location":"api/hooks/#implementations","title":"Implementations","text":""},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector","title":"mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector","text":"<pre><code>LayerActivationDetector(layer_signature, hook_id=None)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>Detector hook that captures and saves activations during inference.</p> <p>This detector extracts activations from layer outputs and stores them for later use (e.g., saving to disk, further analysis).</p> <p>Initialize the activation saver detector.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer to capture activations from</p> required <code>hook_id</code> <code>str | None</code> <p>Unique identifier for this hook</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If layer_signature is None</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def __init__(\n        self,\n        layer_signature: str | int,\n        hook_id: str | None = None\n):\n    \"\"\"\n    Initialize the activation saver detector.\n\n    Args:\n        layer_signature: Layer to capture activations from\n        hook_id: Unique identifier for this hook\n\n    Raises:\n        ValueError: If layer_signature is None\n    \"\"\"\n    if layer_signature is None:\n        raise ValueError(\"layer_signature cannot be None for LayerActivationDetector\")\n\n    super().__init__(\n        hook_type=HookType.FORWARD,\n        hook_id=hook_id,\n        store=None,\n        layer_signature=layer_signature\n    )\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector.clear_captured","title":"clear_captured","text":"<pre><code>clear_captured()\n</code></pre> <p>Clear captured activations for current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def clear_captured(self) -&gt; None:\n    \"\"\"Clear captured activations for current batch.\"\"\"\n    self.tensor_metadata.pop('activations', None)\n    self.metadata.pop('activations_shape', None)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector.get_captured","title":"get_captured","text":"<pre><code>get_captured()\n</code></pre> <p>Get the captured activations from the current batch.</p> <p>Returns:</p> Type Description <code>Tensor | None</code> <p>The captured activation tensor from the current batch or None if no activations captured yet</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def get_captured(self) -&gt; torch.Tensor | None:\n    \"\"\"\n    Get the captured activations from the current batch.\n\n    Returns:\n        The captured activation tensor from the current batch or None if no activations captured yet\n    \"\"\"\n    return self.tensor_metadata.get('activations')\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.layer_activation_detector.LayerActivationDetector.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Extract and store activations from output.</p> <p>Handles various output types: - Plain tensors - Tuples/lists of tensors (takes first tensor) - Objects with last_hidden_state attribute (e.g., HuggingFace outputs)</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/layer_activation_detector.py</code> <pre><code>def process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Extract and store activations from output.\n\n    Handles various output types:\n    - Plain tensors\n    - Tuples/lists of tensors (takes first tensor)\n    - Objects with last_hidden_state attribute (e.g., HuggingFace outputs)\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        tensor = extract_tensor_from_output(output)\n\n        if tensor is not None:\n            tensor_cpu = tensor.detach().to(\"cpu\")\n            # Store current batch's tensor (overwrites previous)\n            self.tensor_metadata['activations'] = tensor_cpu\n            # Store activations shape to metadata\n            self.metadata['activations_shape'] = tuple(tensor_cpu.shape)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error extracting activations in LayerActivationDetector {self.id}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector","title":"mi_crow.hooks.implementations.model_input_detector.ModelInputDetector","text":"<pre><code>ModelInputDetector(\n    layer_signature=None,\n    hook_id=None,\n    save_input_ids=True,\n    save_attention_mask=False,\n    special_token_ids=None,\n)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>Detector hook that captures and saves tokenized inputs from model forward pass.</p> <p>This detector is designed to be attached to the root model module and captures: - Tokenized inputs (input_ids) from the model's forward pass - Attention masks (optional) that exclude both padding and special tokens</p> <p>Uses PRE_FORWARD hook to capture inputs before they are processed. Useful for saving tokenized inputs for analysis or training.</p> <p>Initialize the model input detector.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Layer to capture from (typically the root model, can be None)</p> <code>None</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier for this hook</p> <code>None</code> <code>save_input_ids</code> <code>bool</code> <p>Whether to save input_ids tensor</p> <code>True</code> <code>save_attention_mask</code> <code>bool</code> <p>Whether to save attention_mask tensor (excludes padding and special tokens)</p> <code>False</code> <code>special_token_ids</code> <code>Optional[List[int] | Set[int]]</code> <p>Optional list/set of special token IDs. If None, will extract from LanguageModel context.</p> <code>None</code> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def __init__(\n        self,\n        layer_signature: str | int | None = None,\n        hook_id: str | None = None,\n        save_input_ids: bool = True,\n        save_attention_mask: bool = False,\n        special_token_ids: Optional[List[int] | Set[int]] = None\n):\n    \"\"\"\n    Initialize the model input detector.\n\n    Args:\n        layer_signature: Layer to capture from (typically the root model, can be None)\n        hook_id: Unique identifier for this hook\n        save_input_ids: Whether to save input_ids tensor\n        save_attention_mask: Whether to save attention_mask tensor (excludes padding and special tokens)\n        special_token_ids: Optional list/set of special token IDs. If None, will extract from LanguageModel context.\n    \"\"\"\n    super().__init__(\n        hook_type=HookType.PRE_FORWARD,\n        hook_id=hook_id,\n        store=None,\n        layer_signature=layer_signature\n    )\n    self.save_input_ids = save_input_ids\n    self.save_attention_mask = save_attention_mask\n    self.special_token_ids = set(special_token_ids) if special_token_ids is not None else None\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.clear_captured","title":"clear_captured","text":"<pre><code>clear_captured()\n</code></pre> <p>Clear all captured inputs for current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def clear_captured(self) -&gt; None:\n    \"\"\"Clear all captured inputs for current batch.\"\"\"\n    keys_to_remove = ['input_ids', 'attention_mask']\n    for key in keys_to_remove:\n        self.tensor_metadata.pop(key, None)\n        self.metadata.pop(f'{key}_shape', None)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.get_captured_attention_mask","title":"get_captured_attention_mask","text":"<pre><code>get_captured_attention_mask()\n</code></pre> <p>Get the captured attention_mask from the current batch (excludes padding and special tokens).</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def get_captured_attention_mask(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured attention_mask from the current batch (excludes padding and special tokens).\"\"\"\n    return self.tensor_metadata.get('attention_mask')\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.get_captured_input_ids","title":"get_captured_input_ids","text":"<pre><code>get_captured_input_ids()\n</code></pre> <p>Get the captured input_ids from the current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def get_captured_input_ids(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured input_ids from the current batch.\"\"\"\n    return self.tensor_metadata.get('input_ids')\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Extract and store tokenized inputs.</p> <p>Note: For HuggingFace models called with **kwargs, the input tuple may be empty. In such cases, use set_inputs_from_encodings() to manually set inputs from the encodings dictionary returned by lm.forwards().</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked (typically the root model)</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors/dicts to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output from the module (None for PRE_FORWARD hooks)</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Extract and store tokenized inputs.\n\n    Note: For HuggingFace models called with **kwargs, the input tuple may be empty.\n    In such cases, use set_inputs_from_encodings() to manually set inputs from\n    the encodings dictionary returned by lm.forwards().\n\n    Args:\n        module: The PyTorch module being hooked (typically the root model)\n        input: Tuple of input tensors/dicts to the module\n        output: Output from the module (None for PRE_FORWARD hooks)\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        if self.save_input_ids:\n            input_ids = self._extract_input_ids(input)\n            if input_ids is not None:\n                self.tensor_metadata['input_ids'] = input_ids.detach().to(\"cpu\")\n                self.metadata['input_ids_shape'] = tuple(input_ids.shape)\n\n        if self.save_attention_mask:\n            input_ids = self._extract_input_ids(input)\n            if input_ids is not None:\n                original_attention_mask = self._extract_attention_mask(input)\n                combined_mask = self._create_combined_attention_mask(input_ids, original_attention_mask, module)\n                self.tensor_metadata['attention_mask'] = combined_mask.detach().to(\"cpu\")\n                self.metadata['attention_mask_shape'] = tuple(combined_mask.shape)\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error extracting inputs in ModelInputDetector {self.id}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_input_detector.ModelInputDetector.set_inputs_from_encodings","title":"set_inputs_from_encodings","text":"<pre><code>set_inputs_from_encodings(encodings, module=None)\n</code></pre> <p>Manually set inputs from encodings dictionary.</p> <p>This is useful when the model is called with keyword arguments, as PyTorch's pre_forward hook doesn't receive kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>encodings</code> <code>Dict[str, Tensor]</code> <p>Dictionary of encoded inputs (e.g., from lm.forwards() or lm.tokenize())</p> required <code>module</code> <code>Optional[Module]</code> <p>Optional module for extracting special token IDs. If None, will use DummyModule.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/model_input_detector.py</code> <pre><code>def set_inputs_from_encodings(self, encodings: Dict[str, torch.Tensor], module: Optional[torch.nn.Module] = None) -&gt; None:\n    \"\"\"\n    Manually set inputs from encodings dictionary.\n\n    This is useful when the model is called with keyword arguments,\n    as PyTorch's pre_forward hook doesn't receive kwargs.\n\n    Args:\n        encodings: Dictionary of encoded inputs (e.g., from lm.forwards() or lm.tokenize())\n        module: Optional module for extracting special token IDs. If None, will use DummyModule.\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        if self.save_input_ids and 'input_ids' in encodings:\n            input_ids = encodings['input_ids']\n            self.tensor_metadata['input_ids'] = input_ids.detach().to(\"cpu\")\n            self.metadata['input_ids_shape'] = tuple(input_ids.shape)\n\n        if self.save_attention_mask and 'input_ids' in encodings:\n            input_ids = encodings['input_ids']\n            if module is None:\n                class DummyModule:\n                    pass\n                module = DummyModule()\n\n            original_attention_mask = encodings.get('attention_mask')\n            combined_mask = self._create_combined_attention_mask(input_ids, original_attention_mask, module)\n            self.tensor_metadata['attention_mask'] = combined_mask.detach().to(\"cpu\")\n            self.metadata['attention_mask_shape'] = tuple(combined_mask.shape)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error setting inputs from encodings in ModelInputDetector {self.id}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector","title":"mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector","text":"<pre><code>ModelOutputDetector(\n    layer_signature=None,\n    hook_id=None,\n    save_output_logits=True,\n    save_output_hidden_state=False,\n)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>Detector hook that captures and saves model outputs.</p> <p>This detector is designed to be attached to the root model module and captures: - Model outputs (logits) from the model's forward pass - Hidden states (optional) from the model's forward pass</p> <p>Uses FORWARD hook to capture outputs after they are computed. Useful for saving model outputs for analysis or training.</p> <p>Initialize the model output detector.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Layer to capture from (typically the root model, can be None)</p> <code>None</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier for this hook</p> <code>None</code> <code>save_output_logits</code> <code>bool</code> <p>Whether to save output logits (if available)</p> <code>True</code> <code>save_output_hidden_state</code> <code>bool</code> <p>Whether to save last_hidden_state (if available)</p> <code>False</code> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def __init__(\n        self,\n        layer_signature: str | int | None = None,\n        hook_id: str | None = None,\n        save_output_logits: bool = True,\n        save_output_hidden_state: bool = False\n):\n    \"\"\"\n    Initialize the model output detector.\n\n    Args:\n        layer_signature: Layer to capture from (typically the root model, can be None)\n        hook_id: Unique identifier for this hook\n        save_output_logits: Whether to save output logits (if available)\n        save_output_hidden_state: Whether to save last_hidden_state (if available)\n    \"\"\"\n    super().__init__(\n        hook_type=HookType.FORWARD,\n        hook_id=hook_id,\n        store=None,\n        layer_signature=layer_signature\n    )\n    self.save_output_logits = save_output_logits\n    self.save_output_hidden_state = save_output_hidden_state\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.clear_captured","title":"clear_captured","text":"<pre><code>clear_captured()\n</code></pre> <p>Clear all captured outputs for current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def clear_captured(self) -&gt; None:\n    \"\"\"Clear all captured outputs for current batch.\"\"\"\n    keys_to_remove = ['output_logits', 'output_hidden_state']\n    for key in keys_to_remove:\n        self.tensor_metadata.pop(key, None)\n        self.metadata.pop(f'{key}_shape', None)\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.get_captured_output_hidden_state","title":"get_captured_output_hidden_state","text":"<pre><code>get_captured_output_hidden_state()\n</code></pre> <p>Get the captured output hidden state from the current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def get_captured_output_hidden_state(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured output hidden state from the current batch.\"\"\"\n    return self.tensor_metadata.get('output_hidden_state')\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.get_captured_output_logits","title":"get_captured_output_logits","text":"<pre><code>get_captured_output_logits()\n</code></pre> <p>Get the captured output logits from the current batch.</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def get_captured_output_logits(self) -&gt; torch.Tensor | None:\n    \"\"\"Get the captured output logits from the current batch.\"\"\"\n    return self.tensor_metadata.get('output_logits')\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.model_output_detector.ModelOutputDetector.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Extract and store model outputs.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked (typically the root model)</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors/dicts to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output from the module</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tensor extraction or storage fails</p> Source code in <code>src/mi_crow/hooks/implementations/model_output_detector.py</code> <pre><code>def process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Extract and store model outputs.\n\n    Args:\n        module: The PyTorch module being hooked (typically the root model)\n        input: Tuple of input tensors/dicts to the module\n        output: Output from the module\n\n    Raises:\n        RuntimeError: If tensor extraction or storage fails\n    \"\"\"\n    try:\n        # Extract and save outputs\n        logits, hidden_state = self._extract_output_tensor(output)\n\n        if self.save_output_logits and logits is not None:\n            self.tensor_metadata['output_logits'] = logits.detach().to(\"cpu\")\n            self.metadata['output_logits_shape'] = tuple(logits.shape)\n\n        if self.save_output_hidden_state and hidden_state is not None:\n            self.tensor_metadata['output_hidden_state'] = hidden_state.detach().to(\"cpu\")\n            self.metadata['output_hidden_state_shape'] = tuple(hidden_state.shape)\n\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error extracting outputs in ModelOutputDetector {self.id}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.function_controller.FunctionController","title":"mi_crow.hooks.implementations.function_controller.FunctionController","text":"<pre><code>FunctionController(\n    layer_signature,\n    function,\n    hook_type=HookType.FORWARD,\n    hook_id=None,\n)\n</code></pre> <p>               Bases: <code>Controller</code></p> <p>A controller that applies a user-provided function to tensors during inference.</p> <p>This controller allows users to pass any function and apply it to activations. The function will be applied to: - Single tensors directly - All tensors in tuples/lists (default behavior)</p> Example <p>Initialize a function controller.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer to attach to</p> required <code>function</code> <code>Callable[[Tensor], Tensor]</code> <p>Function to apply to tensors. Must take a torch.Tensor and return a torch.Tensor</p> required <code>hook_type</code> <code>HookType | str</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>FORWARD</code> <code>hook_id</code> <code>str | None</code> <p>Unique identifier</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If function is None or not callable</p> Source code in <code>src/mi_crow/hooks/implementations/function_controller.py</code> <pre><code>def __init__(\n    self,\n    layer_signature: str | int,\n    function: Callable[[torch.Tensor], torch.Tensor],\n    hook_type: HookType | str = HookType.FORWARD,\n    hook_id: str | None = None,\n):\n    \"\"\"\n    Initialize a function controller.\n\n    Args:\n        layer_signature: Layer to attach to\n        function: Function to apply to tensors. Must take a torch.Tensor and return a torch.Tensor\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD)\n        hook_id: Unique identifier\n\n    Raises:\n        ValueError: If function is None or not callable\n    \"\"\"\n    if function is None:\n        raise ValueError(\"function cannot be None\")\n\n    if not callable(function):\n        raise ValueError(f\"function must be callable, got: {type(function)}\")\n\n    super().__init__(hook_type=hook_type, hook_id=hook_id, layer_signature=layer_signature)\n    self.function = function\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.implementations.function_controller.FunctionController--scale-activations-by-2","title":"Scale activations by 2","text":"<p>controller = FunctionController( ...     layer_signature=\"layer_0\", ...     function=lambda x: x * 2.0 ... )</p>"},{"location":"api/hooks/#mi_crow.hooks.implementations.function_controller.FunctionController.modify_activations","title":"modify_activations","text":"<pre><code>modify_activations(module, inputs, output)\n</code></pre> <p>Apply the user-provided function to activations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>'nn.Module'</code> <p>The PyTorch module being hooked</p> required <code>inputs</code> <code>Tensor | None</code> <p>Input tensor (None for forward hooks)</p> required <code>output</code> <code>Tensor | None</code> <p>Output tensor (None for pre_forward hooks)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>Modified tensor with function applied, or None if target tensor is None</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If function raises an exception when applied to tensor</p> Source code in <code>src/mi_crow/hooks/implementations/function_controller.py</code> <pre><code>def modify_activations(\n    self,\n    module: \"nn.Module\",\n    inputs: torch.Tensor | None,\n    output: torch.Tensor | None\n) -&gt; torch.Tensor | None:\n    \"\"\"\n    Apply the user-provided function to activations.\n\n    Args:\n        module: The PyTorch module being hooked\n        inputs: Input tensor (None for forward hooks)\n        output: Output tensor (None for pre_forward hooks)\n\n    Returns:\n        Modified tensor with function applied, or None if target tensor is None\n\n    Raises:\n        RuntimeError: If function raises an exception when applied to tensor\n    \"\"\"\n    target = output if self.hook_type == HookType.FORWARD else inputs\n\n    if target is None or not isinstance(target, torch.Tensor):\n        return target\n\n    try:\n        result = self.function(target)\n        if not isinstance(result, torch.Tensor):\n            raise TypeError(\n                f\"Function must return a torch.Tensor, got: {type(result)}\"\n            )\n        return result\n    except Exception as e:\n        raise RuntimeError(\n            f\"Error applying function in FunctionController {self.id}: {e}\"\n        ) from e\n</code></pre>"},{"location":"api/hooks/#utilities","title":"Utilities","text":""},{"location":"api/hooks/#mi_crow.hooks.utils","title":"mi_crow.hooks.utils","text":"<p>Utility functions for hook implementations.</p>"},{"location":"api/hooks/#mi_crow.hooks.utils.extract_tensor_from_input","title":"extract_tensor_from_input","text":"<pre><code>extract_tensor_from_input(input)\n</code></pre> <p>Extract the first tensor from input sequence.</p> <p>Handles various input formats: - Direct tensor in first position - Tuple/list of tensors in first position - Empty or None inputs</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Input sequence (tuple/list of tensors)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>First tensor found, or None if no tensor found</p> Source code in <code>src/mi_crow/hooks/utils.py</code> <pre><code>def extract_tensor_from_input(input: HOOK_FUNCTION_INPUT) -&gt; torch.Tensor | None:\n    \"\"\"\n    Extract the first tensor from input sequence.\n\n    Handles various input formats:\n    - Direct tensor in first position\n    - Tuple/list of tensors in first position\n    - Empty or None inputs\n\n    Args:\n        input: Input sequence (tuple/list of tensors)\n\n    Returns:\n        First tensor found, or None if no tensor found\n    \"\"\"\n    if not input or len(input) == 0:\n        return None\n\n    first_item = input[0]\n    if isinstance(first_item, torch.Tensor):\n        return first_item\n\n    if isinstance(first_item, (tuple, list)):\n        for item in first_item:\n            if isinstance(item, torch.Tensor):\n                return item\n\n    return None\n</code></pre>"},{"location":"api/hooks/#mi_crow.hooks.utils.extract_tensor_from_output","title":"extract_tensor_from_output","text":"<pre><code>extract_tensor_from_output(output)\n</code></pre> <p>Extract tensor from output (handles various output types).</p> <p>Handles various output formats: - Plain tensors - Tuples/lists of tensors (takes first tensor) - Objects with last_hidden_state attribute (e.g., HuggingFace outputs) - None outputs</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output from module (tensor, tuple, or object with attributes)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>First tensor found, or None if no tensor found</p> Source code in <code>src/mi_crow/hooks/utils.py</code> <pre><code>def extract_tensor_from_output(output: HOOK_FUNCTION_OUTPUT) -&gt; torch.Tensor | None:\n    \"\"\"\n    Extract tensor from output (handles various output types).\n\n    Handles various output formats:\n    - Plain tensors\n    - Tuples/lists of tensors (takes first tensor)\n    - Objects with last_hidden_state attribute (e.g., HuggingFace outputs)\n    - None outputs\n\n    Args:\n        output: Output from module (tensor, tuple, or object with attributes)\n\n    Returns:\n        First tensor found, or None if no tensor found\n    \"\"\"\n    if output is None:\n        return None\n\n    if isinstance(output, torch.Tensor):\n        return output\n\n    if isinstance(output, (tuple, list)):\n        for item in output:\n            if isinstance(item, torch.Tensor):\n                return item\n\n    # Try common HuggingFace output objects\n    if hasattr(output, \"last_hidden_state\"):\n        maybe = getattr(output, \"last_hidden_state\")\n        if isinstance(maybe, torch.Tensor):\n            return maybe\n\n    return None\n</code></pre>"},{"location":"api/language_model/","title":"Language Model API","text":"<p>Core language model functionality for loading models, running inference, and managing activations.</p>"},{"location":"api/language_model/#main-classes","title":"Main Classes","text":""},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel","title":"mi_crow.language_model.language_model.LanguageModel","text":"<pre><code>LanguageModel(model, tokenizer, store, model_id=None)\n</code></pre> <p>Fence-style language model wrapper.</p> <p>Provides a unified interface for working with language models, including: - Model initialization and configuration - Inference operations - Hook management (detectors and controllers) - Model persistence - Activation tracking</p> <p>Initialize LanguageModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model module</p> required <code>tokenizer</code> <code>PreTrainedTokenizerBase</code> <p>HuggingFace tokenizer</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>model_id</code> <code>str | None</code> <p>Optional model identifier (auto-extracted if not provided)</p> <code>None</code> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def __init__(\n        self,\n        model: nn.Module,\n        tokenizer: PreTrainedTokenizerBase,\n        store: Store,\n        model_id: str | None = None,\n):\n    \"\"\"\n    Initialize LanguageModel.\n\n    Args:\n        model: PyTorch model module\n        tokenizer: HuggingFace tokenizer\n        store: Store instance for persistence\n        model_id: Optional model identifier (auto-extracted if not provided)\n    \"\"\"\n    self.context = LanguageModelContext(self)\n    self.context.model = model\n    self.context.tokenizer = tokenizer\n    self.context.model_id = initialize_model_id(model, model_id)\n    self.context.store = store\n    self.context.special_token_ids = _extract_special_token_ids(tokenizer)\n\n    self.layers = LanguageModelLayers(self.context)\n    self.lm_tokenizer = LanguageModelTokenizer(self.context)\n    self.activations = LanguageModelActivations(self.context)\n    self.inference = InferenceEngine(self)\n    self._inference_engine = self.inference\n\n    self._input_tracker: \"InputTracker | None\" = None\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.model","title":"model  <code>property</code>","text":"<pre><code>model\n</code></pre> <p>Get the underlying PyTorch model.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.model_id","title":"model_id  <code>property</code>","text":"<pre><code>model_id\n</code></pre> <p>Get the model identifier.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.store","title":"store  <code>property</code> <code>writable</code>","text":"<pre><code>store\n</code></pre> <p>Get the store instance.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.tokenizer","title":"tokenizer  <code>property</code>","text":"<pre><code>tokenizer\n</code></pre> <p>Get the tokenizer.</p>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.clear_detectors","title":"clear_detectors","text":"<pre><code>clear_detectors()\n</code></pre> <p>Clear all accumulated metadata for registered detectors.</p> <p>This is useful when running multiple independent inference runs (e.g. separate <code>infer_texts</code> / <code>infer_dataset</code> calls) and you want to ensure that detector state does not leak between runs.</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def clear_detectors(self) -&gt; None:\n    \"\"\"\n    Clear all accumulated metadata for registered detectors.\n\n    This is useful when running multiple independent inference runs\n    (e.g. separate `infer_texts` / `infer_dataset` calls) and you want\n    to ensure that detector state does not leak between runs.\n    \"\"\"\n    detectors = self.layers.get_detectors()\n    for detector in detectors:\n        # Clear generic accumulated metadata\n        detector.metadata.clear()\n        detector.tensor_metadata.clear()\n\n        # Allow detector implementations to provide more specialized\n        # clearing logic (e.g. ModelInputDetector, ModelOutputDetector).\n        clear_captured = getattr(detector, \"clear_captured\", None)\n        if callable(clear_captured):\n            clear_captured()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.forwards","title":"forwards","text":"<pre><code>forwards(\n    texts,\n    tok_kwargs=None,\n    autocast=True,\n    autocast_dtype=None,\n    with_controllers=True,\n)\n</code></pre> <p>Run forward pass on texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Input texts to process</p> required <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[Any, Any]</code> <p>Tuple of (model_output, encodings)</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def forwards(\n        self,\n        texts: Sequence[str],\n        tok_kwargs: Dict | None = None,\n        autocast: bool = True,\n        autocast_dtype: torch.dtype | None = None,\n        with_controllers: bool = True,\n) -&gt; Tuple[Any, Any]:\n    \"\"\"\n    Run forward pass on texts.\n\n    Args:\n        texts: Input texts to process\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n\n    Returns:\n        Tuple of (model_output, encodings)\n    \"\"\"\n    return self._inference_engine.execute_inference(\n        texts,\n        tok_kwargs=tok_kwargs,\n        autocast=autocast,\n        autocast_dtype=autocast_dtype,\n        with_controllers=with_controllers\n    )\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.from_huggingface","title":"from_huggingface  <code>classmethod</code>","text":"<pre><code>from_huggingface(\n    model_name,\n    store,\n    tokenizer_params=None,\n    model_params=None,\n)\n</code></pre> <p>Load a language model from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>HuggingFace model identifier</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>tokenizer_params</code> <code>dict</code> <p>Optional tokenizer parameters</p> <code>None</code> <code>model_params</code> <code>dict</code> <p>Optional model parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>@classmethod\ndef from_huggingface(\n        cls,\n        model_name: str,\n        store: Store,\n        tokenizer_params: dict = None,\n        model_params: dict = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from HuggingFace Hub.\n\n    Args:\n        model_name: HuggingFace model identifier\n        store: Store instance for persistence\n        tokenizer_params: Optional tokenizer parameters\n        model_params: Optional model parameters\n\n    Returns:\n        LanguageModel instance\n    \"\"\"\n    return create_from_huggingface(cls, model_name, store, tokenizer_params, model_params)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.from_local","title":"from_local  <code>classmethod</code>","text":"<pre><code>from_local(saved_path, store, model_id=None)\n</code></pre> <p>Load a language model from a saved file (created by save_model).</p> <p>Parameters:</p> Name Type Description Default <code>saved_path</code> <code>Path | str</code> <p>Path to the saved model file (.pt file)</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>model_id</code> <code>str | None</code> <p>Optional model identifier. If not provided, will use the model_id from saved metadata.      If provided, will be used to load the model architecture from HuggingFace.</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the saved file doesn't exist</p> <code>ValueError</code> <p>If the saved file format is invalid or model_id is required but not provided</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>@classmethod\ndef from_local(cls, saved_path: Path | str, store: Store, model_id: str | None = None) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from a saved file (created by save_model).\n\n    Args:\n        saved_path: Path to the saved model file (.pt file)\n        store: Store instance for persistence\n        model_id: Optional model identifier. If not provided, will use the model_id from saved metadata.\n                 If provided, will be used to load the model architecture from HuggingFace.\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        FileNotFoundError: If the saved file doesn't exist\n        ValueError: If the saved file format is invalid or model_id is required but not provided\n    \"\"\"\n    return load_model_from_saved_file(cls, saved_path, store, model_id)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.from_local_torch","title":"from_local_torch  <code>classmethod</code>","text":"<pre><code>from_local_torch(model_path, tokenizer_path, store)\n</code></pre> <p>Load a language model from local HuggingFace paths.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model directory or file</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to the tokenizer directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>@classmethod\ndef from_local_torch(cls, model_path: str, tokenizer_path: str, store: Store) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from local HuggingFace paths.\n\n    Args:\n        model_path: Path to the model directory or file\n        tokenizer_path: Path to the tokenizer directory or file\n        store: Store instance for persistence\n\n    Returns:\n        LanguageModel instance\n    \"\"\"\n    return create_from_local_torch(cls, model_path, tokenizer_path, store)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.generate","title":"generate","text":"<pre><code>generate(\n    texts,\n    tok_kwargs=None,\n    autocast=True,\n    autocast_dtype=None,\n    with_controllers=True,\n    skip_special_tokens=True,\n)\n</code></pre> <p>Run inference and automatically decode the output with the tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Input texts to process</p> required <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>skip_special_tokens</code> <code>bool</code> <p>Whether to skip special tokens when decoding</p> <code>True</code> <p>Returns:</p> Type Description <code>Sequence[str]</code> <p>Sequence of decoded text strings</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If texts is empty or tokenizer is None</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def generate(\n        self,\n        texts: Sequence[str],\n        tok_kwargs: Dict | None = None,\n        autocast: bool = True,\n        autocast_dtype: torch.dtype | None = None,\n        with_controllers: bool = True,\n        skip_special_tokens: bool = True,\n) -&gt; Sequence[str]:\n    \"\"\"\n    Run inference and automatically decode the output with the tokenizer.\n\n    Args:\n        texts: Input texts to process\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        skip_special_tokens: Whether to skip special tokens when decoding\n\n    Returns:\n        Sequence of decoded text strings\n\n    Raises:\n        ValueError: If texts is empty or tokenizer is None\n    \"\"\"\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    if self.tokenizer is None:\n        raise ValueError(\"Tokenizer is required for decoding but is None\")\n\n    output, enc = self._inference_engine.execute_inference(\n        texts,\n        tok_kwargs=tok_kwargs,\n        autocast=autocast,\n        autocast_dtype=autocast_dtype,\n        with_controllers=with_controllers\n    )\n\n    logits = self._inference_engine.extract_logits(output)\n    predicted_token_ids = logits.argmax(dim=-1)\n\n    decoded_texts = []\n    for i in range(predicted_token_ids.shape[0]):\n        token_ids = predicted_token_ids[i].cpu().tolist()\n        decoded_text = self.tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n        decoded_texts.append(decoded_text)\n\n    return decoded_texts\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.get_all_detector_metadata","title":"get_all_detector_metadata","text":"<pre><code>get_all_detector_metadata()\n</code></pre> <p>Get metadata from all registered detectors.</p> <p>Returns:</p> Type Description <code>tuple[dict[str, dict[str, Any]], dict[str, dict[str, Tensor]]]</code> <p>Tuple of (detectors_metadata, detectors_tensor_metadata)</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def get_all_detector_metadata(self) -&gt; tuple[dict[str, dict[str, Any]], dict[str, dict[str, Tensor]]]:\n    \"\"\"\n    Get metadata from all registered detectors.\n\n    Returns:\n        Tuple of (detectors_metadata, detectors_tensor_metadata)\n    \"\"\"\n    detectors = self.layers.get_detectors()\n    detectors_metadata: Dict[str, Dict[str, Any]] = defaultdict(dict)\n    detectors_tensor_metadata: Dict[str, Dict[str, torch.Tensor]] = defaultdict(dict)\n\n    for detector in detectors:\n        detectors_metadata[detector.layer_signature] = detector.metadata\n        detectors_tensor_metadata[detector.layer_signature] = detector.tensor_metadata\n\n    return detectors_metadata, detectors_tensor_metadata\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.get_input_tracker","title":"get_input_tracker","text":"<pre><code>get_input_tracker()\n</code></pre> <p>Get the input tracker instance if it exists.</p> <p>Returns:</p> Type Description <code>'InputTracker | None'</code> <p>InputTracker instance or None</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def get_input_tracker(self) -&gt; \"InputTracker | None\":\n    \"\"\"\n    Get the input tracker instance if it exists.\n\n    Returns:\n        InputTracker instance or None\n    \"\"\"\n    return self._input_tracker\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.save_detector_metadata","title":"save_detector_metadata","text":"<pre><code>save_detector_metadata(run_name, batch_idx, unified=False)\n</code></pre> <p>Save detector metadata to store.</p> <p>Parameters:</p> Name Type Description Default <code>run_name</code> <code>str</code> <p>Name of the run</p> required <code>batch_idx</code> <code>int | None</code> <p>Batch index. Ignored when <code>unified</code> is True.</p> required <code>unified</code> <code>bool</code> <p>If True, save metadata in a single detectors directory for the whole run instead of per\u2011batch directories.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Path where metadata was saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is not set</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def save_detector_metadata(self, run_name: str, batch_idx: int | None, unified: bool = False) -&gt; str:\n    \"\"\"\n    Save detector metadata to store.\n\n    Args:\n        run_name: Name of the run\n        batch_idx: Batch index. Ignored when ``unified`` is True.\n        unified: If True, save metadata in a single detectors directory\n            for the whole run instead of per\u2011batch directories.\n\n    Returns:\n        Path where metadata was saved\n\n    Raises:\n        ValueError: If store is not set\n    \"\"\"\n    if self.store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n    detectors_metadata, detectors_tensor_metadata = self.get_all_detector_metadata()\n    if unified:\n        return self.store.put_run_detector_metadata(run_name, detectors_metadata, detectors_tensor_metadata)\n    if batch_idx is None:\n        raise ValueError(\"batch_idx must be provided when unified is False\")\n    return self.store.put_detector_metadata(run_name, batch_idx, detectors_metadata, detectors_tensor_metadata)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.save_model","title":"save_model","text":"<pre><code>save_model(path=None)\n</code></pre> <p>Save the model and its metadata to the store.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str | None</code> <p>Optional path to save the model. If None, defaults to {model_id}/model.pt   relative to the store base path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path where the model was saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is not set</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def save_model(self, path: Path | str | None = None) -&gt; Path:\n    \"\"\"\n    Save the model and its metadata to the store.\n\n    Args:\n        path: Optional path to save the model. If None, defaults to {model_id}/model.pt\n              relative to the store base path.\n\n    Returns:\n        Path where the model was saved\n\n    Raises:\n        ValueError: If store is not set\n    \"\"\"\n    return save_model(self, path)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.language_model.LanguageModel.tokenize","title":"tokenize","text":"<pre><code>tokenize(texts, **kwargs)\n</code></pre> <p>Tokenize texts using the language model tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of text strings to tokenize</p> required <code>**kwargs</code> <code>Any</code> <p>Additional tokenizer arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tokenized encodings</p> Source code in <code>src/mi_crow/language_model/language_model.py</code> <pre><code>def tokenize(self, texts: Sequence[str], **kwargs: Any) -&gt; Any:\n    \"\"\"\n    Tokenize texts using the language model tokenizer.\n\n    Args:\n        texts: Sequence of text strings to tokenize\n        **kwargs: Additional tokenizer arguments\n\n    Returns:\n        Tokenized encodings\n    \"\"\"\n    return self.lm_tokenizer.tokenize(texts, **kwargs)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.context.LanguageModelContext","title":"mi_crow.language_model.context.LanguageModelContext  <code>dataclass</code>","text":"<pre><code>LanguageModelContext(\n    language_model,\n    model_id=None,\n    tokenizer_params=None,\n    model_params=None,\n    device=\"cpu\",\n    dtype=None,\n    model=None,\n    tokenizer=None,\n    store=None,\n    special_token_ids=None,\n    _hook_registry=dict(),\n    _hook_id_map=dict(),\n)\n</code></pre> <p>Shared context for LanguageModel and its components.</p>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers","title":"mi_crow.language_model.layers.LanguageModelLayers","text":"<pre><code>LanguageModelLayers(context)\n</code></pre> <p>Manages layer access and hook registration for LanguageModel.</p> <p>Initialize LanguageModelLayers.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>LanguageModelContext instance</p> required Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def __init__(\n        self,\n        context: \"LanguageModelContext\",\n):\n    \"\"\"\n    Initialize LanguageModelLayers.\n\n    Args:\n        context: LanguageModelContext instance\n    \"\"\"\n    self.context = context\n    self.name_to_layer: Dict[str, nn.Module] = {}\n    self.idx_to_layer: Dict[int, nn.Module] = {}\n    self._flatten_layer_names()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.disable_all_hooks","title":"disable_all_hooks","text":"<pre><code>disable_all_hooks()\n</code></pre> <p>Disable all registered hooks.</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def disable_all_hooks(self) -&gt; None:\n    \"\"\"Disable all registered hooks.\"\"\"\n    for _, _, hook in self.context._hook_id_map.values():\n        hook.disable()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.disable_hook","title":"disable_hook","text":"<pre><code>disable_hook(hook_id)\n</code></pre> <p>Disable a specific hook by ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook_id</code> <code>str</code> <p>Hook ID to disable</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hook was found and disabled, False otherwise</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def disable_hook(self, hook_id: str) -&gt; bool:\n    \"\"\"\n    Disable a specific hook by ID.\n\n    Args:\n        hook_id: Hook ID to disable\n\n    Returns:\n        True if hook was found and disabled, False otherwise\n    \"\"\"\n    if hook_id in self.context._hook_id_map:\n        _, _, hook = self.context._hook_id_map[hook_id]\n        hook.disable()\n        return True\n    return False\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.enable_all_hooks","title":"enable_all_hooks","text":"<pre><code>enable_all_hooks()\n</code></pre> <p>Enable all registered hooks.</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def enable_all_hooks(self) -&gt; None:\n    \"\"\"Enable all registered hooks.\"\"\"\n    for _, _, hook in self.context._hook_id_map.values():\n        hook.enable()\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.enable_hook","title":"enable_hook","text":"<pre><code>enable_hook(hook_id)\n</code></pre> <p>Enable a specific hook by ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook_id</code> <code>str</code> <p>Hook ID to enable</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hook was found and enabled, False otherwise</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def enable_hook(self, hook_id: str) -&gt; bool:\n    \"\"\"\n    Enable a specific hook by ID.\n\n    Args:\n        hook_id: Hook ID to enable\n\n    Returns:\n        True if hook was found and enabled, False otherwise\n    \"\"\"\n    if hook_id in self.context._hook_id_map:\n        _, _, hook = self.context._hook_id_map[hook_id]\n        hook.enable()\n        return True\n    return False\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_controllers","title":"get_controllers","text":"<pre><code>get_controllers()\n</code></pre> <p>Get all registered Controller hooks.</p> <p>Returns:</p> Type Description <code>List[Controller]</code> <p>List of Controller instances</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_controllers(self) -&gt; List[Controller]:\n    \"\"\"\n    Get all registered Controller hooks.\n\n    Returns:\n        List of Controller instances\n    \"\"\"\n    return [hook for hook in self.get_hooks() if isinstance(hook, Controller)]\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_detectors","title":"get_detectors","text":"<pre><code>get_detectors()\n</code></pre> <p>Get all registered Detector hooks.</p> <p>Returns:</p> Type Description <code>List[Detector]</code> <p>List of Detector instances</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_detectors(self) -&gt; List[Detector]:\n    \"\"\"\n    Get all registered Detector hooks.\n\n    Returns:\n        List of Detector instances\n    \"\"\"\n    return [hook for hook in self.get_hooks() if isinstance(hook, Detector)]\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_hooks","title":"get_hooks","text":"<pre><code>get_hooks(layer_signature=None, hook_type=None)\n</code></pre> <p>Get registered hooks, optionally filtered by layer and/or type.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int | None</code> <p>Optional layer to filter by</p> <code>None</code> <code>hook_type</code> <code>HookType | str | None</code> <p>Optional hook type to filter by (HookType.FORWARD or HookType.PRE_FORWARD)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Hook]</code> <p>List of Hook instances</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_hooks(\n        self,\n        layer_signature: str | int | None = None,\n        hook_type: HookType | str | None = None\n) -&gt; List[Hook]:\n    \"\"\"\n    Get registered hooks, optionally filtered by layer and/or type.\n\n    Args:\n        layer_signature: Optional layer to filter by\n        hook_type: Optional hook type to filter by (HookType.FORWARD or HookType.PRE_FORWARD)\n\n    Returns:\n        List of Hook instances\n    \"\"\"\n    # Normalize hook_type if string\n    normalized_hook_type = None\n    if hook_type is not None:\n        if isinstance(hook_type, str):\n            normalized_hook_type = HookType(hook_type)\n        else:\n            normalized_hook_type = hook_type\n\n    return self._get_hooks_from_registry(layer_signature, normalized_hook_type)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.get_layer_names","title":"get_layer_names","text":"<pre><code>get_layer_names()\n</code></pre> <p>Get all layer names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of layer names</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def get_layer_names(self) -&gt; List[str]:\n    \"\"\"\n    Get all layer names.\n\n    Returns:\n        List of layer names\n    \"\"\"\n    return list(self.name_to_layer.keys())\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.print_layer_names","title":"print_layer_names","text":"<pre><code>print_layer_names()\n</code></pre> <p>Print layer names with basic info.</p> <p>Useful for debugging and exploring model structure.</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def print_layer_names(self) -&gt; None:\n    \"\"\"\n    Print layer names with basic info.\n\n    Useful for debugging and exploring model structure.\n    \"\"\"\n    names = self.get_layer_names()\n    for name in names:\n        layer = self.name_to_layer[name]\n        weight_shape = getattr(layer, 'weight', None)\n        weight_info = weight_shape.shape if weight_shape is not None else 'No weight'\n        print(f\"{name}: {weight_info}\")\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.register_forward_hook_for_layer","title":"register_forward_hook_for_layer","text":"<pre><code>register_forward_hook_for_layer(\n    layer_signature, hook, hook_args=None\n)\n</code></pre> <p>Register a forward hook directly on a layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer name or index</p> required <code>hook</code> <code>Callable</code> <p>Hook callable</p> required <code>hook_args</code> <code>dict</code> <p>Optional arguments for register_forward_hook</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Hook handle</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def register_forward_hook_for_layer(\n        self,\n        layer_signature: str | int,\n        hook: Callable,\n        hook_args: dict = None\n) -&gt; Any:\n    \"\"\"\n    Register a forward hook directly on a layer.\n\n    Args:\n        layer_signature: Layer name or index\n        hook: Hook callable\n        hook_args: Optional arguments for register_forward_hook\n\n    Returns:\n        Hook handle\n    \"\"\"\n    layer = self._resolve_layer(layer_signature)\n    return layer.register_forward_hook(hook, **(hook_args or {}))\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.register_hook","title":"register_hook","text":"<pre><code>register_hook(layer_signature, hook, hook_type=None)\n</code></pre> <p>Register a hook on a layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer name or index</p> required <code>hook</code> <code>Hook</code> <p>Hook instance to register</p> required <code>hook_type</code> <code>HookType | str | None</code> <p>Type of hook (HookType.FORWARD or HookType.PRE_FORWARD).        If None, uses hook.hook_type</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The hook's ID</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If hook ID is not unique or if mixing hook types on same layer</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def register_hook(\n        self,\n        layer_signature: str | int,\n        hook: Hook,\n        hook_type: HookType | str | None = None\n) -&gt; str:\n    \"\"\"\n    Register a hook on a layer.\n\n    Args:\n        layer_signature: Layer name or index\n        hook: Hook instance to register\n        hook_type: Type of hook (HookType.FORWARD or HookType.PRE_FORWARD). \n                  If None, uses hook.hook_type\n\n    Returns:\n        The hook's ID\n\n    Raises:\n        ValueError: If hook ID is not unique or if mixing hook types on same layer\n    \"\"\"\n    layer = self._resolve_layer(layer_signature)\n\n    if hook_type is None:\n        hook_type = self._get_hook_type_from_hook(hook)\n    elif isinstance(hook_type, str):\n        hook_type = HookType(hook_type)\n\n    self._validate_hook_registration(layer_signature, hook)\n\n    hook.layer_signature = layer_signature\n    hook.set_context(self.context)\n\n    if layer_signature not in self.context._hook_registry:\n        self.context._hook_registry[layer_signature] = {}\n\n    if hook_type not in self.context._hook_registry[layer_signature]:\n        self.context._hook_registry[layer_signature][hook_type] = []\n\n    torch_hook_fn = hook.get_torch_hook()\n\n    if hook_type == HookType.PRE_FORWARD:\n        handle = layer.register_forward_pre_hook(torch_hook_fn)\n    else:\n        handle = layer.register_forward_hook(torch_hook_fn)\n\n    self.context._hook_registry[layer_signature][hook_type].append((hook, handle))\n    self.context._hook_id_map[hook.id] = (layer_signature, hook_type, hook)\n\n    return hook.id\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.register_pre_forward_hook_for_layer","title":"register_pre_forward_hook_for_layer","text":"<pre><code>register_pre_forward_hook_for_layer(\n    layer_signature, hook, hook_args=None\n)\n</code></pre> <p>Register a pre-forward hook directly on a layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer_signature</code> <code>str | int</code> <p>Layer name or index</p> required <code>hook</code> <code>Callable</code> <p>Hook callable</p> required <code>hook_args</code> <code>dict</code> <p>Optional arguments for register_forward_pre_hook</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Hook handle</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def register_pre_forward_hook_for_layer(\n        self,\n        layer_signature: str | int,\n        hook: Callable,\n        hook_args: dict = None\n) -&gt; Any:\n    \"\"\"\n    Register a pre-forward hook directly on a layer.\n\n    Args:\n        layer_signature: Layer name or index\n        hook: Hook callable\n        hook_args: Optional arguments for register_forward_pre_hook\n\n    Returns:\n        Hook handle\n    \"\"\"\n    layer = self._resolve_layer(layer_signature)\n    return layer.register_forward_pre_hook(hook, **(hook_args or {}))\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.layers.LanguageModelLayers.unregister_hook","title":"unregister_hook","text":"<pre><code>unregister_hook(hook_or_id)\n</code></pre> <p>Unregister a hook by Hook instance or ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook_or_id</code> <code>Hook | str</code> <p>Hook instance or hook ID string</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if hook was found and removed, False otherwise</p> Source code in <code>src/mi_crow/language_model/layers.py</code> <pre><code>def unregister_hook(self, hook_or_id: Hook | str) -&gt; bool:\n    \"\"\"\n    Unregister a hook by Hook instance or ID.\n\n    Args:\n        hook_or_id: Hook instance or hook ID string\n\n    Returns:\n        True if hook was found and removed, False otherwise\n    \"\"\"\n    # Get hook ID\n    if isinstance(hook_or_id, Hook):\n        hook_id = hook_or_id.id\n    else:\n        hook_id = hook_or_id\n\n    # Look up hook\n    if hook_id not in self.context._hook_id_map:\n        return False\n\n    layer_signature, hook_type, hook = self.context._hook_id_map[hook_id]\n\n    # Find and remove from registry\n    if layer_signature in self.context._hook_registry:\n        if hook_type in self.context._hook_registry[layer_signature]:\n            hooks_list = self.context._hook_registry[layer_signature][hook_type]\n            for i, (h, handle) in enumerate(hooks_list):\n                if h.id == hook_id:\n                    # Remove PyTorch hook\n                    handle.remove()\n                    # Remove from our list\n                    hooks_list.pop(i)\n                    break\n\n    # Remove from ID map\n    del self.context._hook_id_map[hook_id]\n    return True\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.tokenizer.LanguageModelTokenizer","title":"mi_crow.language_model.tokenizer.LanguageModelTokenizer","text":"<pre><code>LanguageModelTokenizer(context)\n</code></pre> <p>Handles tokenization for LanguageModel.</p> <p>Initialize LanguageModelTokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>LanguageModelContext instance</p> required Source code in <code>src/mi_crow/language_model/tokenizer.py</code> <pre><code>def __init__(\n        self,\n        context: \"LanguageModelContext\"\n):\n    \"\"\"\n    Initialize LanguageModelTokenizer.\n\n    Args:\n        context: LanguageModelContext instance\n    \"\"\"\n    self.context = context\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.tokenizer.LanguageModelTokenizer.split_to_tokens","title":"split_to_tokens","text":"<pre><code>split_to_tokens(text, add_special_tokens=False)\n</code></pre> <p>Split text into token strings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, Sequence[str]]</code> <p>Single string or sequence of strings to tokenize</p> required <code>add_special_tokens</code> <code>bool</code> <p>Whether to add special tokens (e.g., BOS, EOS)</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List[str], List[List[str]]]</code> <p>For a single string: list of token strings</p> <code>Union[List[str], List[List[str]]]</code> <p>For a sequence of strings: list of lists of token strings</p> Source code in <code>src/mi_crow/language_model/tokenizer.py</code> <pre><code>def split_to_tokens(\n        self,\n        text: Union[str, Sequence[str]],\n        add_special_tokens: bool = False\n) -&gt; Union[List[str], List[List[str]]]:\n    \"\"\"\n    Split text into token strings.\n\n    Args:\n        text: Single string or sequence of strings to tokenize\n        add_special_tokens: Whether to add special tokens (e.g., BOS, EOS)\n\n    Returns:\n        For a single string: list of token strings\n        For a sequence of strings: list of lists of token strings\n    \"\"\"\n    if isinstance(text, str):\n        return self._split_single_text_to_tokens(text, add_special_tokens)\n\n    return [self._split_single_text_to_tokens(t, add_special_tokens) for t in text]\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.tokenizer.LanguageModelTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(texts, padding=False, pad_token='[PAD]', **kwargs)\n</code></pre> <p>Robust batch tokenization that works across tokenizer variants.</p> <p>Tries methods in order: - callable tokenizer (most HF tokenizers) - batch_encode_plus - encode_plus per item + tokenizer.pad to collate</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of text strings to tokenize</p> required <code>padding</code> <code>bool</code> <p>Whether to pad sequences</p> <code>False</code> <code>pad_token</code> <code>str</code> <p>Pad token string</p> <code>'[PAD]'</code> <code>**kwargs</code> <code>Any</code> <p>Additional tokenizer arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tokenized encodings</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokenizer is not initialized</p> <code>TypeError</code> <p>If tokenizer is not usable for batch tokenization</p> Source code in <code>src/mi_crow/language_model/tokenizer.py</code> <pre><code>def tokenize(\n        self,\n        texts: Sequence[str],\n        padding: bool = False,\n        pad_token: str = \"[PAD]\",\n        **kwargs: Any\n) -&gt; Any:\n    \"\"\"\n    Robust batch tokenization that works across tokenizer variants.\n\n    Tries methods in order:\n    - callable tokenizer (most HF tokenizers)\n    - batch_encode_plus\n    - encode_plus per item + tokenizer.pad to collate\n\n    Args:\n        texts: Sequence of text strings to tokenize\n        padding: Whether to pad sequences\n        pad_token: Pad token string\n        **kwargs: Additional tokenizer arguments\n\n    Returns:\n        Tokenized encodings\n\n    Raises:\n        ValueError: If tokenizer is not initialized\n        TypeError: If tokenizer is not usable for batch tokenization\n    \"\"\"\n    tokenizer = self.context.tokenizer\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be initialized before tokenization\")\n\n    model = self.context.model\n\n    if padding and pad_token and getattr(tokenizer, \"pad_token\", None) is None:\n        self._setup_pad_token(tokenizer, model)\n\n    kwargs[\"padding\"] = padding\n\n    # Try callable tokenizer first (most common case)\n    if callable(tokenizer):\n        try:\n            return tokenizer(texts, **kwargs)\n        except TypeError:\n            pass\n\n    # Try batch_encode_plus\n    if hasattr(tokenizer, \"batch_encode_plus\"):\n        return tokenizer.batch_encode_plus(texts, **kwargs)\n\n    # Fallback to encode_plus per item\n    if hasattr(tokenizer, \"encode_plus\"):\n        encoded = [tokenizer.encode_plus(t, **kwargs) for t in texts]\n        if hasattr(tokenizer, \"pad\"):\n            rt = kwargs.get(\"return_tensors\") or \"pt\"\n            return tokenizer.pad(encoded, return_tensors=rt)\n        return encoded\n\n    raise TypeError(\"Tokenizer object on LanguageModel is not usable for batch tokenization\")\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.activations.LanguageModelActivations","title":"mi_crow.language_model.activations.LanguageModelActivations","text":"<pre><code>LanguageModelActivations(context)\n</code></pre> <p>Handles activation saving and processing for LanguageModel.</p> <p>Initialize LanguageModelActivations.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>LanguageModelContext instance</p> required Source code in <code>src/mi_crow/language_model/activations.py</code> <pre><code>def __init__(self, context: \"LanguageModelContext\"):  # noqa: F821\n    \"\"\"\n    Initialize LanguageModelActivations.\n\n    Args:\n        context: LanguageModelContext instance\n    \"\"\"\n    self.context = context\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.activations.LanguageModelActivations.save_activations","title":"save_activations","text":"<pre><code>save_activations(\n    texts,\n    layer_signature,\n    run_name=None,\n    batch_size=None,\n    *,\n    dtype=None,\n    max_length=None,\n    autocast=True,\n    autocast_dtype=None,\n    free_cuda_cache_every=0,\n    verbose=False,\n    save_in_batches=True,\n    save_attention_mask=False\n)\n</code></pre> <p>Save activations from a list of texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of text strings to process</p> required <code>layer_signature</code> <code>str | int | list[str | int]</code> <p>Layer signature (or list of signatures) to capture activations from</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name (generated if None)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Optional batch size for processing (if None, processes all at once)</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to convert activations to</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional max length for tokenization</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use autocast</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>free_cuda_cache_every</code> <code>int | None</code> <p>Clear CUDA cache every N batches (0 or None to disable)</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>save_attention_mask</code> <code>bool</code> <p>Whether to also save attention masks (automatically attaches ModelInputDetector)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Run name used for saving</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model or store is not initialized</p> Source code in <code>src/mi_crow/language_model/activations.py</code> <pre><code>def save_activations(\n    self,\n    texts: Sequence[str],\n    layer_signature: str | int | list[str | int],\n    run_name: str | None = None,\n    batch_size: int | None = None,\n    *,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    free_cuda_cache_every: int | None = 0,\n    verbose: bool = False,\n    save_in_batches: bool = True,\n    save_attention_mask: bool = False,\n) -&gt; str:\n    \"\"\"\n    Save activations from a list of texts.\n\n    Args:\n        texts: Sequence of text strings to process\n        layer_signature: Layer signature (or list of signatures) to capture activations from\n        run_name: Optional run name (generated if None)\n        batch_size: Optional batch size for processing (if None, processes all at once)\n        dtype: Optional dtype to convert activations to\n        max_length: Optional max length for tokenization\n        autocast: Whether to use autocast\n        autocast_dtype: Optional dtype for autocast\n        free_cuda_cache_every: Clear CUDA cache every N batches (0 or None to disable)\n        verbose: Whether to log progress\n        save_attention_mask: Whether to also save attention masks (automatically attaches ModelInputDetector)\n\n    Returns:\n        Run name used for saving\n\n    Raises:\n        ValueError: If model or store is not initialized\n    \"\"\"\n    model: nn.Module | None = self.context.model\n    if model is None:\n        raise ValueError(\"Model must be initialized before running\")\n\n    _, layer_sig_list = self._normalize_layer_signatures(layer_signature)\n\n    store = self.context.store\n    if store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    device = get_device_from_model(model)\n    device_type = str(device.type)\n\n    if batch_size is None:\n        batch_size = len(texts)\n\n    options = {\n        \"dtype\": str(dtype) if dtype is not None else None,\n        \"max_length\": max_length,\n        \"batch_size\": int(batch_size),\n    }\n\n    run_name, meta = self._prepare_run_metadata(\n        layer_signature, dataset=None, run_name=run_name, options=options\n    )\n\n    if verbose:\n        logger.info(\n            f\"Starting save_activations: run={run_name}, layers={layer_sig_list}, \"\n            f\"batch_size={batch_size}, device={device_type}\"\n        )\n\n    self._save_run_metadata(store, run_name, meta, verbose)\n\n    hook_ids: list[str] = []\n    for sig in layer_sig_list:\n        _, hook_id = self._setup_detector(sig, f\"save_{run_name}_{sig}\")\n        hook_ids.append(hook_id)\n\n    # Setup attention mask detector if requested\n    attention_mask_hook_id: str | None = None\n    if save_attention_mask:\n        _, attention_mask_hook_id = self._setup_attention_mask_detector(run_name)\n\n    batch_counter = 0\n\n    try:\n        with torch.inference_mode():\n            for i in range(0, len(texts), batch_size):\n                batch_texts = texts[i:i + batch_size]\n                batch_index = i // batch_size\n\n                self._process_batch(\n                    batch_texts,\n                    run_name,\n                    batch_index,\n                    max_length,\n                    autocast,\n                    autocast_dtype,\n                    dtype,\n                    verbose,\n                    save_in_batches=save_in_batches,\n                )\n                batch_counter += 1\n                self._manage_cuda_cache(batch_counter, free_cuda_cache_every, device_type, verbose)\n    finally:\n        for hook_id in hook_ids:\n            self._cleanup_detector(hook_id)\n        if attention_mask_hook_id is not None:\n            self._cleanup_detector(attention_mask_hook_id)\n        if verbose:\n            logger.info(f\"Completed save_activations: run={run_name}, batches_saved={batch_counter}\")\n\n    return run_name\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.activations.LanguageModelActivations.save_activations_dataset","title":"save_activations_dataset","text":"<pre><code>save_activations_dataset(\n    dataset,\n    layer_signature,\n    run_name=None,\n    batch_size=32,\n    *,\n    dtype=None,\n    max_length=None,\n    autocast=True,\n    autocast_dtype=None,\n    free_cuda_cache_every=0,\n    verbose=False,\n    save_in_batches=True,\n    save_attention_mask=False\n)\n</code></pre> <p>Save activations from a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>BaseDataset</code> <p>Dataset to process</p> required <code>layer_signature</code> <code>str | int | list[str | int]</code> <p>Layer signature (or list of signatures) to capture activations from</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name (generated if None)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>32</code> <code>dtype</code> <code>dtype | None</code> <p>Optional dtype to convert activations to</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Optional max length for tokenization</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use autocast</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>free_cuda_cache_every</code> <code>int | None</code> <p>Clear CUDA cache every N batches (0 or None to disable)</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>save_attention_mask</code> <code>bool</code> <p>Whether to also save attention masks (automatically attaches ModelInputDetector)</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Run name used for saving</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model or store is not initialized</p> Source code in <code>src/mi_crow/language_model/activations.py</code> <pre><code>def save_activations_dataset(\n    self,\n    dataset: BaseDataset,\n    layer_signature: str | int | list[str | int],\n    run_name: str | None = None,\n    batch_size: int = 32,\n    *,\n    dtype: torch.dtype | None = None,\n    max_length: int | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    free_cuda_cache_every: int | None = 0,\n    verbose: bool = False,\n    save_in_batches: bool = True,\n    save_attention_mask: bool = False,\n) -&gt; str:\n    \"\"\"\n    Save activations from a dataset.\n\n    Args:\n        dataset: Dataset to process\n        layer_signature: Layer signature (or list of signatures) to capture activations from\n        run_name: Optional run name (generated if None)\n        batch_size: Batch size for processing\n        dtype: Optional dtype to convert activations to\n        max_length: Optional max length for tokenization\n        autocast: Whether to use autocast\n        autocast_dtype: Optional dtype for autocast\n        free_cuda_cache_every: Clear CUDA cache every N batches (0 or None to disable)\n        verbose: Whether to log progress\n        save_attention_mask: Whether to also save attention masks (automatically attaches ModelInputDetector)\n\n    Returns:\n        Run name used for saving\n\n    Raises:\n        ValueError: If model or store is not initialized\n    \"\"\"\n    model: nn.Module | None = self.context.model\n    if model is None:\n        raise ValueError(\"Model must be initialized before running\")\n\n    _, layer_sig_list = self._normalize_layer_signatures(layer_signature)\n\n    store = self.context.store\n    if store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n\n    device = get_device_from_model(model)\n    device_type = str(device.type)\n\n    options = {\n        \"dtype\": str(dtype) if dtype is not None else None,\n        \"max_length\": max_length,\n        \"batch_size\": int(batch_size),\n    }\n\n    run_name, meta = self._prepare_run_metadata(\n        layer_signature, dataset=dataset, run_name=run_name, options=options\n    )\n\n    if verbose:\n        logger.info(\n            f\"Starting save_activations_dataset: run={run_name}, layers={layer_sig_list}, \"\n            f\"batch_size={batch_size}, device={device_type}\"\n        )\n\n    self._save_run_metadata(store, run_name, meta, verbose)\n\n    hook_ids: list[str] = []\n    for sig in layer_sig_list:\n        _, hook_id = self._setup_detector(sig, f\"save_{run_name}_{sig}\")\n        hook_ids.append(hook_id)\n\n    # Setup attention mask detector if requested\n    attention_mask_hook_id: str | None = None\n    if save_attention_mask:\n        _, attention_mask_hook_id = self._setup_attention_mask_detector(run_name)\n\n    batch_counter = 0\n\n    try:\n        with torch.inference_mode():\n            for batch_index, batch in enumerate(dataset.iter_batches(batch_size)):\n                texts = dataset.extract_texts_from_batch(batch)\n                self._process_batch(\n                    texts,\n                    run_name,\n                    batch_index,\n                    max_length,\n                    autocast,\n                    autocast_dtype,\n                    dtype,\n                    verbose,\n                    save_in_batches=save_in_batches,\n                )\n                batch_counter += 1\n                self._manage_cuda_cache(batch_counter, free_cuda_cache_every, device_type, verbose)\n    finally:\n        for hook_id in hook_ids:\n            self._cleanup_detector(hook_id)\n        if attention_mask_hook_id is not None:\n            self._cleanup_detector(attention_mask_hook_id)\n        if verbose:\n            logger.info(f\"Completed save_activations_dataset: run={run_name}, batches_saved={batch_counter}\")\n\n    return run_name\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine","title":"mi_crow.language_model.inference.InferenceEngine","text":"<pre><code>InferenceEngine(language_model)\n</code></pre> <p>Handles inference operations for LanguageModel.</p> <p>Initialize inference engine.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>'LanguageModel'</code> <p>LanguageModel instance</p> required Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def __init__(self, language_model: \"LanguageModel\"):\n    \"\"\"\n    Initialize inference engine.\n\n    Args:\n        language_model: LanguageModel instance\n    \"\"\"\n    self.lm = language_model\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.execute_inference","title":"execute_inference","text":"<pre><code>execute_inference(\n    texts,\n    tok_kwargs=None,\n    autocast=True,\n    autocast_dtype=None,\n    with_controllers=True,\n    stop_after_layer=None,\n)\n</code></pre> <p>Execute inference on texts.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of input texts</p> required <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>stop_after_layer</code> <code>str | int | None</code> <p>Optional layer signature (name or index) after which the forward pass should be stopped early</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Any, Dict[str, Tensor]]</code> <p>Tuple of (model_output, encodings)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If texts is empty or tokenizer is not initialized</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def execute_inference(\n        self,\n        texts: Sequence[str],\n        tok_kwargs: Dict | None = None,\n        autocast: bool = True,\n        autocast_dtype: torch.dtype | None = None,\n        with_controllers: bool = True,\n        stop_after_layer: str | int | None = None,\n) -&gt; tuple[Any, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Execute inference on texts.\n\n    Args:\n        texts: Sequence of input texts\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        stop_after_layer: Optional layer signature (name or index) after which\n            the forward pass should be stopped early\n\n    Returns:\n        Tuple of (model_output, encodings)\n\n    Raises:\n        ValueError: If texts is empty or tokenizer is not initialized\n    \"\"\"\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    if self.lm.tokenizer is None:\n        raise ValueError(\"Tokenizer must be initialized before running inference\")\n\n    tok_kwargs = self._prepare_tokenizer_kwargs(tok_kwargs)\n    enc = self.lm.tokenize(texts, **tok_kwargs)\n\n    device = get_device_from_model(self.lm.model)\n    device_type = str(device.type)\n    enc = move_tensors_to_device(enc, device)\n\n    self.lm.model.eval()\n\n    self._setup_trackers(texts)\n    self._setup_model_input_detectors(enc)\n\n    controllers_to_restore = self._prepare_controllers(with_controllers)\n\n    hook_handle = None\n    try:\n        if stop_after_layer is not None:\n            # Register a temporary forward hook that stops the forward pass\n            def _early_stop_hook(module: nn.Module, inputs: tuple, output: Any):\n                raise _EarlyStopInference(output)\n\n            hook_handle = self.lm.layers.register_forward_hook_for_layer(\n                stop_after_layer, _early_stop_hook\n            )\n\n        output = self._run_model_forward(enc, autocast, device_type, autocast_dtype)\n        return output, enc\n    finally:\n        if hook_handle is not None:\n            try:\n                hook_handle.remove()\n            except Exception:\n                pass\n        self._restore_controllers(controllers_to_restore)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.extract_logits","title":"extract_logits","text":"<pre><code>extract_logits(output)\n</code></pre> <p>Extract logits tensor from model output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>Model output</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Logits tensor</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def extract_logits(self, output: Any) -&gt; torch.Tensor:\n    \"\"\"\n    Extract logits tensor from model output.\n\n    Args:\n        output: Model output\n\n    Returns:\n        Logits tensor\n    \"\"\"\n    return extract_logits_from_output(output)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.infer_dataset","title":"infer_dataset","text":"<pre><code>infer_dataset(\n    dataset,\n    run_name=None,\n    batch_size=32,\n    tok_kwargs=None,\n    autocast=True,\n    autocast_dtype=None,\n    with_controllers=True,\n    free_cuda_cache_every=0,\n    clear_detectors_before=False,\n    verbose=False,\n    stop_after_layer=None,\n    save_in_batches=True,\n)\n</code></pre> <p>Run inference on whole dataset with metadata saving.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>'BaseDataset'</code> <p>Dataset to process</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name (generated if None)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>32</code> <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>free_cuda_cache_every</code> <code>int | None</code> <p>Clear CUDA cache every N batches (0 or None to disable)</p> <code>0</code> <code>clear_detectors_before</code> <code>bool</code> <p>If True, clears all detector state before running</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>stop_after_layer</code> <code>str | int | None</code> <p>Optional layer signature (name or index) after which the forward pass should be stopped early</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Run name used for saving</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model or store is not initialized</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def infer_dataset(\n    self,\n    dataset: \"BaseDataset\",\n    run_name: str | None = None,\n    batch_size: int = 32,\n    tok_kwargs: Dict | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    with_controllers: bool = True,\n    free_cuda_cache_every: int | None = 0,\n    clear_detectors_before: bool = False,\n    verbose: bool = False,\n    stop_after_layer: str | int | None = None,\n    save_in_batches: bool = True,\n) -&gt; str:\n    \"\"\"\n    Run inference on whole dataset with metadata saving.\n\n    Args:\n        dataset: Dataset to process\n        run_name: Optional run name (generated if None)\n        batch_size: Batch size for processing\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        free_cuda_cache_every: Clear CUDA cache every N batches (0 or None to disable)\n        clear_detectors_before: If True, clears all detector state before running\n        verbose: Whether to log progress\n        stop_after_layer: Optional layer signature (name or index) after which\n            the forward pass should be stopped early\n\n    Returns:\n        Run name used for saving\n\n    Raises:\n        ValueError: If model or store is not initialized\n    \"\"\"\n    if clear_detectors_before:\n        self.lm.clear_detectors()\n\n    model: nn.Module | None = self.lm.model\n    if model is None:\n        raise ValueError(\"Model must be initialized before running\")\n\n    store = self.lm.store\n    if store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n    device = get_device_from_model(model)\n    device_type = str(device.type)\n\n    options = {\n        \"max_length\": tok_kwargs.get(\"max_length\") if tok_kwargs else None,\n        \"batch_size\": int(batch_size),\n    }\n\n    run_name, meta = self._prepare_run_metadata(dataset=dataset, run_name=run_name, options=options)\n\n    if verbose:\n        logger.info(\n            f\"Starting infer_dataset: run={run_name}, \"\n            f\"batch_size={batch_size}, device={device_type}\"\n        )\n\n    self._save_run_metadata(store, run_name, meta, verbose)\n\n    batch_counter = 0\n\n    with torch.inference_mode():\n        for batch_index, batch in enumerate(dataset.iter_batches(batch_size)):\n            if not batch:\n                continue\n\n            texts = dataset.extract_texts_from_batch(batch)\n\n            self.execute_inference(\n                texts,\n                tok_kwargs=tok_kwargs,\n                autocast=autocast,\n                autocast_dtype=autocast_dtype,\n                with_controllers=with_controllers,\n                stop_after_layer=stop_after_layer,\n            )\n\n            self.lm.save_detector_metadata(run_name, batch_index, unified=not save_in_batches)\n\n            batch_counter += 1\n\n            if device_type == \"cuda\" and free_cuda_cache_every and free_cuda_cache_every &gt; 0:\n                if (batch_counter % free_cuda_cache_every) == 0:\n                    torch.cuda.empty_cache()\n                    if verbose:\n                        logger.info(\"Emptied CUDA cache\")\n\n            if verbose:\n                logger.info(f\"Saved batch {batch_index} for run={run_name}\")\n\n    if verbose:\n        logger.info(f\"Completed infer_dataset: run={run_name}, batches_saved={batch_counter}\")\n\n    return run_name\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.inference.InferenceEngine.infer_texts","title":"infer_texts","text":"<pre><code>infer_texts(\n    texts,\n    run_name=None,\n    batch_size=None,\n    tok_kwargs=None,\n    autocast=True,\n    autocast_dtype=None,\n    with_controllers=True,\n    clear_detectors_before=False,\n    verbose=False,\n    stop_after_layer=None,\n    save_in_batches=True,\n)\n</code></pre> <p>Run inference on list of strings with optional metadata saving.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>Sequence[str]</code> <p>Sequence of input texts</p> required <code>run_name</code> <code>str | None</code> <p>Optional run name for saving metadata (if None, no metadata saved)</p> <code>None</code> <code>batch_size</code> <code>int | None</code> <p>Optional batch size for processing (if None, processes all at once)</p> <code>None</code> <code>tok_kwargs</code> <code>Dict | None</code> <p>Optional tokenizer keyword arguments</p> <code>None</code> <code>autocast</code> <code>bool</code> <p>Whether to use automatic mixed precision</p> <code>True</code> <code>autocast_dtype</code> <code>dtype | None</code> <p>Optional dtype for autocast</p> <code>None</code> <code>with_controllers</code> <code>bool</code> <p>Whether to use controllers during inference</p> <code>True</code> <code>clear_detectors_before</code> <code>bool</code> <p>If True, clears all detector state before running</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>False</code> <code>stop_after_layer</code> <code>str | int | None</code> <p>Optional layer signature (name or index) after which the forward pass should be stopped early</p> <code>None</code> <code>save_in_batches</code> <code>bool</code> <p>If True, save detector metadata in per\u2011batch directories. If False, aggregate all detector metadata for the run under a single detectors directory.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Any, Dict[str, Tensor]] | tuple[List[Any], List[Dict[str, Tensor]]]</code> <p>If batch_size is None or &gt;= len(texts): Tuple of (model_output, encodings)</p> <code>tuple[Any, Dict[str, Tensor]] | tuple[List[Any], List[Dict[str, Tensor]]]</code> <p>If batch_size &lt; len(texts): Tuple of (list of outputs, list of encodings)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If texts is empty or tokenizer is not initialized</p> Source code in <code>src/mi_crow/language_model/inference.py</code> <pre><code>def infer_texts(\n    self,\n    texts: Sequence[str],\n    run_name: str | None = None,\n    batch_size: int | None = None,\n    tok_kwargs: Dict | None = None,\n    autocast: bool = True,\n    autocast_dtype: torch.dtype | None = None,\n    with_controllers: bool = True,\n    clear_detectors_before: bool = False,\n    verbose: bool = False,\n    stop_after_layer: str | int | None = None,\n    save_in_batches: bool = True,\n) -&gt; tuple[Any, Dict[str, torch.Tensor]] | tuple[List[Any], List[Dict[str, torch.Tensor]]]:\n    \"\"\"\n    Run inference on list of strings with optional metadata saving.\n\n    Args:\n        texts: Sequence of input texts\n        run_name: Optional run name for saving metadata (if None, no metadata saved)\n        batch_size: Optional batch size for processing (if None, processes all at once)\n        tok_kwargs: Optional tokenizer keyword arguments\n        autocast: Whether to use automatic mixed precision\n        autocast_dtype: Optional dtype for autocast\n        with_controllers: Whether to use controllers during inference\n        clear_detectors_before: If True, clears all detector state before running\n        verbose: Whether to log progress\n        stop_after_layer: Optional layer signature (name or index) after which\n            the forward pass should be stopped early\n        save_in_batches: If True, save detector metadata in per\u2011batch\n            directories. If False, aggregate all detector metadata for\n            the run under a single detectors directory.\n\n    Returns:\n        If batch_size is None or &gt;= len(texts): Tuple of (model_output, encodings)\n        If batch_size &lt; len(texts): Tuple of (list of outputs, list of encodings)\n\n    Raises:\n        ValueError: If texts is empty or tokenizer is not initialized\n    \"\"\"\n    if not texts:\n        raise ValueError(\"Texts list cannot be empty\")\n\n    if self.lm.tokenizer is None:\n        raise ValueError(\"Tokenizer must be initialized before running inference\")\n\n    if clear_detectors_before:\n        self.lm.clear_detectors()\n\n    store = self.lm.store\n    if run_name is not None and store is None:\n        raise ValueError(\"Store must be provided to save metadata\")\n\n    if batch_size is None or batch_size &gt;= len(texts):\n        output, enc = self.execute_inference(\n            texts,\n            tok_kwargs=tok_kwargs,\n            autocast=autocast,\n            autocast_dtype=autocast_dtype,\n            with_controllers=with_controllers,\n            stop_after_layer=stop_after_layer,\n        )\n\n        if run_name is not None:\n            options = {\n                \"batch_size\": len(texts),\n                \"max_length\": tok_kwargs.get(\"max_length\") if tok_kwargs else None,\n            }\n            _, meta = self._prepare_run_metadata(dataset=None, run_name=run_name, options=options)\n            self._save_run_metadata(store, run_name, meta, verbose)\n            self.lm.save_detector_metadata(run_name, 0, unified=not save_in_batches)\n\n        return output, enc\n\n    all_outputs = []\n    all_encodings = []\n    batch_counter = 0\n\n    if run_name is not None:\n        options = {\n            \"batch_size\": batch_size,\n            \"max_length\": tok_kwargs.get(\"max_length\") if tok_kwargs else None,\n        }\n        _, meta = self._prepare_run_metadata(dataset=None, run_name=run_name, options=options)\n        self._save_run_metadata(store, run_name, meta, verbose)\n\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i + batch_size]\n        output, enc = self.execute_inference(\n            batch_texts,\n            tok_kwargs=tok_kwargs,\n            autocast=autocast,\n            autocast_dtype=autocast_dtype,\n            with_controllers=with_controllers,\n            stop_after_layer=stop_after_layer,\n        )\n\n        all_outputs.append(output)\n        all_encodings.append(enc)\n\n        if run_name is not None:\n            self.lm.save_detector_metadata(run_name, batch_counter, unified=not save_in_batches)\n            if verbose:\n                logger.info(f\"Saved batch {batch_counter} for run={run_name}\")\n\n        batch_counter += 1\n\n    return all_outputs, all_encodings\n</code></pre>"},{"location":"api/language_model/#utilities","title":"Utilities","text":""},{"location":"api/language_model/#mi_crow.language_model.initialization","title":"mi_crow.language_model.initialization","text":"<p>Model initialization and factory methods.</p>"},{"location":"api/language_model/#mi_crow.language_model.initialization.create_from_huggingface","title":"create_from_huggingface","text":"<pre><code>create_from_huggingface(\n    cls,\n    model_name,\n    store,\n    tokenizer_params=None,\n    model_params=None,\n)\n</code></pre> <p>Load a language model from HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type['LanguageModel']</code> <p>LanguageModel class</p> required <code>model_name</code> <code>str</code> <p>HuggingFace model identifier</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <code>tokenizer_params</code> <code>dict | None</code> <p>Optional tokenizer parameters</p> <code>None</code> <code>model_params</code> <code>dict | None</code> <p>Optional model parameters</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model_name is invalid</p> <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/mi_crow/language_model/initialization.py</code> <pre><code>def create_from_huggingface(\n        cls: type[\"LanguageModel\"],\n        model_name: str,\n        store: Store,\n        tokenizer_params: dict | None = None,\n        model_params: dict | None = None,\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from HuggingFace Hub.\n\n    Args:\n        cls: LanguageModel class\n        model_name: HuggingFace model identifier\n        store: Store instance for persistence\n        tokenizer_params: Optional tokenizer parameters\n        model_params: Optional model parameters\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        ValueError: If model_name is invalid\n        RuntimeError: If model loading fails\n    \"\"\"\n    if not model_name or not isinstance(model_name, str) or not model_name.strip():\n        raise ValueError(f\"model_name must be a non-empty string, got: {model_name!r}\")\n\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    if tokenizer_params is None:\n        tokenizer_params = {}\n    if model_params is None:\n        model_params = {}\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_params)\n        model = AutoModelForCausalLM.from_pretrained(model_name, **model_params)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load model '{model_name}' from HuggingFace. Error: {e}\"\n        ) from e\n\n    return cls(model, tokenizer, store)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.initialization.create_from_local_torch","title":"create_from_local_torch","text":"<pre><code>create_from_local_torch(\n    cls, model_path, tokenizer_path, store\n)\n</code></pre> <p>Load a language model from local HuggingFace paths.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type['LanguageModel']</code> <p>LanguageModel class</p> required <code>model_path</code> <code>str</code> <p>Path to the model directory or file</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to the tokenizer directory or file</p> required <code>store</code> <code>Store</code> <p>Store instance for persistence</p> required <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If model or tokenizer paths don't exist</p> <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/mi_crow/language_model/initialization.py</code> <pre><code>def create_from_local_torch(\n        cls: type[\"LanguageModel\"],\n        model_path: str,\n        tokenizer_path: str,\n        store: Store\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from local HuggingFace paths.\n\n    Args:\n        cls: LanguageModel class\n        model_path: Path to the model directory or file\n        tokenizer_path: Path to the tokenizer directory or file\n        store: Store instance for persistence\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        FileNotFoundError: If model or tokenizer paths don't exist\n        RuntimeError: If model loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    model_path_obj = Path(model_path)\n    tokenizer_path_obj = Path(tokenizer_path)\n\n    if not model_path_obj.exists():\n        raise FileNotFoundError(f\"Model path does not exist: {model_path}\")\n\n    if not tokenizer_path_obj.exists():\n        raise FileNotFoundError(f\"Tokenizer path does not exist: {tokenizer_path}\")\n\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load model from local paths. \"\n            f\"model_path={model_path!r}, tokenizer_path={tokenizer_path!r}. Error: {e}\"\n        ) from e\n\n    return cls(model, tokenizer, store)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.initialization.initialize_model_id","title":"initialize_model_id","text":"<pre><code>initialize_model_id(model, provided_model_id=None)\n</code></pre> <p>Initialize model ID for LanguageModel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model module</p> required <code>provided_model_id</code> <code>str | None</code> <p>Optional model ID provided by user</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Model ID string</p> Source code in <code>src/mi_crow/language_model/initialization.py</code> <pre><code>def initialize_model_id(\n        model: nn.Module,\n        provided_model_id: str | None = None\n) -&gt; str:\n    \"\"\"\n    Initialize model ID for LanguageModel.\n\n    Args:\n        model: PyTorch model module\n        provided_model_id: Optional model ID provided by user\n\n    Returns:\n        Model ID string\n    \"\"\"\n    return extract_model_id(model, provided_model_id)\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.persistence","title":"mi_crow.language_model.persistence","text":"<p>Model persistence (save/load) operations.</p>"},{"location":"api/language_model/#mi_crow.language_model.persistence.load_model_from_saved_file","title":"load_model_from_saved_file","text":"<pre><code>load_model_from_saved_file(\n    cls, saved_path, store, model_id=None\n)\n</code></pre> <p>Load a language model from a saved file (created by save_model).</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type['LanguageModel']</code> <p>LanguageModel class</p> required <code>saved_path</code> <code>Path | str</code> <p>Path to the saved model file (.pt file)</p> required <code>store</code> <code>'Store'</code> <p>Store instance for persistence</p> required <code>model_id</code> <code>str | None</code> <p>Optional model identifier. If not provided, will use the model_id from saved metadata.      If provided, will be used to load the model architecture from HuggingFace.</p> <code>None</code> <p>Returns:</p> Type Description <code>'LanguageModel'</code> <p>LanguageModel instance</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the saved file doesn't exist</p> <code>ValueError</code> <p>If the saved file format is invalid or model_id is required but not provided</p> <code>RuntimeError</code> <p>If model loading fails</p> Source code in <code>src/mi_crow/language_model/persistence.py</code> <pre><code>def load_model_from_saved_file(\n        cls: type[\"LanguageModel\"],\n        saved_path: Path | str,\n        store: \"Store\",\n        model_id: str | None = None\n) -&gt; \"LanguageModel\":\n    \"\"\"\n    Load a language model from a saved file (created by save_model).\n\n    Args:\n        cls: LanguageModel class\n        saved_path: Path to the saved model file (.pt file)\n        store: Store instance for persistence\n        model_id: Optional model identifier. If not provided, will use the model_id from saved metadata.\n                 If provided, will be used to load the model architecture from HuggingFace.\n\n    Returns:\n        LanguageModel instance\n\n    Raises:\n        FileNotFoundError: If the saved file doesn't exist\n        ValueError: If the saved file format is invalid or model_id is required but not provided\n        RuntimeError: If model loading fails\n    \"\"\"\n    if store is None:\n        raise ValueError(\"store cannot be None\")\n\n    saved_path = Path(saved_path)\n    if not saved_path.exists():\n        raise FileNotFoundError(f\"Saved model file not found: {saved_path}\")\n\n    # Load the saved payload\n    try:\n        payload = torch.load(saved_path, map_location='cpu')\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load model file {saved_path}. Error: {e}\"\n        ) from e\n\n    # Validate payload structure\n    if \"model_state_dict\" not in payload:\n        raise ValueError(f\"Invalid saved model format: missing 'model_state_dict' key in {saved_path}\")\n    if \"metadata\" not in payload:\n        raise ValueError(f\"Invalid saved model format: missing 'metadata' key in {saved_path}\")\n\n    model_state_dict = payload[\"model_state_dict\"]\n    metadata_dict = payload[\"metadata\"]\n\n    # Get model_id from metadata or use provided one\n    saved_model_id = metadata_dict.get(\"model_id\")\n    if model_id is None:\n        if saved_model_id is None:\n            raise ValueError(\n                f\"model_id not found in saved metadata and not provided. \"\n                f\"Please provide model_id parameter.\"\n            )\n        model_id = saved_model_id\n\n    # Load model and tokenizer from HuggingFace using model_id\n    # This assumes model_id is a valid HuggingFace model name\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        model = AutoModelForCausalLM.from_pretrained(model_id)\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to load model '{model_id}' from HuggingFace. \"\n            f\"Error: {e}. \"\n            f\"Please ensure model_id is a valid HuggingFace model name.\"\n        ) from e\n\n    # Load the saved state dict into the model\n    try:\n        model.load_state_dict(model_state_dict)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to load state dict into model '{model_id}'. Error: {e}\"\n        ) from e\n\n    # Create LanguageModel instance\n    lm = cls(model, tokenizer, store, model_id=model_id)\n\n    # Note: Hooks are not automatically restored as they require hook instances\n    # The hook metadata is available in metadata_dict[\"hooks\"] if needed\n\n    from mi_crow.utils import get_logger\n    logger = get_logger(__name__)\n    logger.info(f\"Loaded model from {saved_path} (model_id: {model_id})\")\n\n    return lm\n</code></pre>"},{"location":"api/language_model/#mi_crow.language_model.persistence.save_model","title":"save_model","text":"<pre><code>save_model(language_model, path=None)\n</code></pre> <p>Save the model and its metadata to the store.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>'LanguageModel'</code> <p>LanguageModel instance to save</p> required <code>path</code> <code>Path | str | None</code> <p>Optional path to save the model. If None, defaults to {model_id}/model.pt   relative to the store base path.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path where the model was saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If store is not set</p> <code>OSError</code> <p>If file operations fail</p> Source code in <code>src/mi_crow/language_model/persistence.py</code> <pre><code>def save_model(\n        language_model: \"LanguageModel\",\n        path: Path | str | None = None\n) -&gt; Path:\n    \"\"\"\n    Save the model and its metadata to the store.\n\n    Args:\n        language_model: LanguageModel instance to save\n        path: Optional path to save the model. If None, defaults to {model_id}/model.pt\n              relative to the store base path.\n\n    Returns:\n        Path where the model was saved\n\n    Raises:\n        ValueError: If store is not set\n        OSError: If file operations fail\n    \"\"\"\n    if language_model.store is None:\n        raise ValueError(\"Store must be provided or set on the language model\")\n\n    # Determine save path\n    if path is None:\n        save_path = Path(language_model.store.base_path) / language_model.model_id / \"model.pt\"\n    else:\n        save_path = Path(path)\n        # If path is relative, make it relative to store base path\n        if not save_path.is_absolute():\n            save_path = Path(language_model.store.base_path) / save_path\n\n    # Ensure parent directory exists\n    save_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Collect hooks information\n    hooks_info = collect_hooks_metadata(language_model.context)\n\n    # Save model state dict\n    model_state_dict = language_model.model.state_dict()\n\n    # Create metadata\n    metadata = ModelMetadata(\n        model_id=language_model.model_id,\n        hooks=hooks_info,\n        model_path=str(save_path)\n    )\n\n    # Save everything in a single file\n    payload = {\n        \"model_state_dict\": model_state_dict,\n        \"metadata\": asdict(metadata),\n    }\n\n    try:\n        torch.save(payload, save_path)\n    except OSError as e:\n        raise OSError(\n            f\"Failed to save model to {save_path}. Error: {e}\"\n        ) from e\n\n    from mi_crow.utils import get_logger\n    logger = get_logger(__name__)\n    logger.info(f\"Saved model to {save_path}\")\n\n    return save_path\n</code></pre>"},{"location":"api/sae/","title":"Sparse Autoencoder (SAE) API","text":"<p>Sparse Autoencoders, training, concepts, and related modules for mechanistic interpretability.</p>"},{"location":"api/sae/#core-sae-classes","title":"Core SAE Classes","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae","title":"mi_crow.mechanistic.sae.sae.Sae","text":"<pre><code>Sae(\n    n_latents,\n    n_inputs,\n    hook_id=None,\n    device=\"cpu\",\n    store=None,\n    *args,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Controller</code>, <code>Detector</code>, <code>ABC</code></p> Source code in <code>src/mi_crow/mechanistic/sae/sae.py</code> <pre><code>def __init__(\n        self,\n        n_latents: int,\n        n_inputs: int,\n        hook_id: str | None = None,\n        device: str = 'cpu',\n        store: Store | None = None,\n        *args: Any,\n        **kwargs: Any\n) -&gt; None:\n    # Initialize both Controller and Detector\n    Controller.__init__(self, hook_type=HookType.FORWARD, hook_id=hook_id)\n    Detector.__init__(self, hook_type=HookType.FORWARD, hook_id=hook_id, store=store)\n\n    self._autoencoder_context = AutoencoderContext(\n        autoencoder=self,\n        n_latents=n_latents,\n        n_inputs=n_inputs\n    )\n    self._autoencoder_context.device = device\n    self.sae_engine: OvercompleteSAE = self._initialize_sae_engine()\n    self.concepts = AutoencoderConcepts(self._autoencoder_context)\n\n    # Text tracking flag\n    self._text_tracking_enabled: bool = False\n\n    # Training component\n    self.trainer = SaeTrainer(self)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae.context","title":"context  <code>property</code> <code>writable</code>","text":"<pre><code>context\n</code></pre> <p>Get the AutoencoderContext associated with this SAE.</p>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae.process_activations","title":"process_activations  <code>abstractmethod</code>","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Process activations to save neuron activations in metadata.</p> <p>This implements the Detector interface. It extracts activations, encodes them to get neuron activations (latents), and saves metadata for each item in the batch individually, including nonzero latent indices and activations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required Source code in <code>src/mi_crow/mechanistic/sae/sae.py</code> <pre><code>@abc.abstractmethod\ndef process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Process activations to save neuron activations in metadata.\n\n    This implements the Detector interface. It extracts activations, encodes them\n    to get neuron activations (latents), and saves metadata for each item in the batch\n    individually, including nonzero latent indices and activations.\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n    \"\"\"\n    raise NotImplementedError(\"process_activations method not implemented.\")\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae.Sae.set_context","title":"set_context","text":"<pre><code>set_context(context)\n</code></pre> <p>Set the LanguageModelContext for this hook and sync to AutoencoderContext.</p> <p>When the hook is registered, this method is called with the LanguageModelContext. It automatically syncs relevant values to the AutoencoderContext.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>LanguageModelContext</code> <p>The LanguageModelContext instance from the LanguageModel</p> required Source code in <code>src/mi_crow/mechanistic/sae/sae.py</code> <pre><code>def set_context(self, context: \"LanguageModelContext\") -&gt; None:\n    \"\"\"Set the LanguageModelContext for this hook and sync to AutoencoderContext.\n\n    When the hook is registered, this method is called with the LanguageModelContext.\n    It automatically syncs relevant values to the AutoencoderContext.\n\n    Args:\n        context: The LanguageModelContext instance from the LanguageModel\n    \"\"\"\n    Hook.set_context(self, context)\n    self._context = context\n    if context is not None:\n        self._autoencoder_context.lm = context.language_model\n        if context.model_id is not None:\n            self._autoencoder_context.model_id = context.model_id\n        if context.store is not None and self._autoencoder_context.store is None:\n            self._autoencoder_context.store = context.store\n        if self.layer_signature is not None:\n            self._autoencoder_context.lm_layer_signature = self.layer_signature\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae","title":"mi_crow.mechanistic.sae.modules.topk_sae.TopKSae","text":"<pre><code>TopKSae(\n    n_latents,\n    n_inputs,\n    hook_id=None,\n    device=\"cpu\",\n    store=None,\n    *args,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Sae</code></p> <p>Initialize TopK SAE.</p> <p>Parameters:</p> Name Type Description Default <code>n_latents</code> <code>int</code> <p>Number of latent dimensions (concepts)</p> required <code>n_inputs</code> <code>int</code> <p>Number of input dimensions</p> required <code>hook_id</code> <code>str | None</code> <p>Optional hook identifier</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to run on ('cpu', 'cuda', 'mps')</p> <code>'cpu'</code> <code>store</code> <code>Store | None</code> <p>Optional store instance</p> <code>None</code> Note <p>The <code>k</code> parameter must be provided in TopKSaeTrainingConfig during training. For loaded models, <code>k</code> is restored from saved metadata. A temporary default k=1 is used for engine initialization and will be overridden with the actual k value from config during training.</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def __init__(\n        self,\n        n_latents: int,\n        n_inputs: int,\n        hook_id: str | None = None,\n        device: str = 'cpu',\n        store: Store | None = None,\n        *args: Any,\n        **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initialize TopK SAE.\n\n    Args:\n        n_latents: Number of latent dimensions (concepts)\n        n_inputs: Number of input dimensions\n        hook_id: Optional hook identifier\n        device: Device to run on ('cpu', 'cuda', 'mps')\n        store: Optional store instance\n\n    Note:\n        The `k` parameter must be provided in TopKSaeTrainingConfig during training.\n        For loaded models, `k` is restored from saved metadata.\n        A temporary default k=1 is used for engine initialization and will be\n        overridden with the actual k value from config during training.\n    \"\"\"\n    # Set temporary default k for engine initialization (base class calls _initialize_sae_engine)\n    # This will be overridden with the actual k from config during training\n    self.k: int = 1\n    super().__init__(n_latents, n_inputs, hook_id, device, store, *args, **kwargs)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.decode","title":"decode","text":"<pre><code>decode(x)\n</code></pre> <p>Decode latents using sae_engine.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Encoded tensor of shape [batch_size, n_latents]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed tensor of shape [batch_size, n_inputs]</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Decode latents using sae_engine.\n\n    Args:\n        x: Encoded tensor of shape [batch_size, n_latents]\n\n    Returns:\n        Reconstructed tensor of shape [batch_size, n_inputs]\n    \"\"\"\n    return self.sae_engine.decode(x)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.encode","title":"encode","text":"<pre><code>encode(x)\n</code></pre> <p>Encode input using sae_engine.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, n_inputs]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded latents (TopK sparse activations)</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Encode input using sae_engine.\n\n    Args:\n        x: Input tensor of shape [batch_size, n_inputs]\n\n    Returns:\n        Encoded latents (TopK sparse activations)\n    \"\"\"\n    # Overcomplete TopKSAE encode returns (pre_codes, codes)\n    _, codes = self.sae_engine.encode(x)\n    return codes\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass using sae_engine.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, n_inputs]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed tensor of shape [batch_size, n_inputs]</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass using sae_engine.\n\n    Args:\n        x: Input tensor of shape [batch_size, n_inputs]\n\n    Returns:\n        Reconstructed tensor of shape [batch_size, n_inputs]\n    \"\"\"\n    # Overcomplete TopKSAE forward returns (pre_codes, codes, x_reconstructed)\n    _, _, x_reconstructed = self.sae_engine.forward(x)\n    return x_reconstructed\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.load","title":"load  <code>staticmethod</code>","text":"<pre><code>load(path)\n</code></pre> <p>Load TopKSAE from saved file using overcomplete's load method + our metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to saved model file</p> required <p>Returns:</p> Type Description <code>TopKSae</code> <p>Loaded TopKSAE instance</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>@staticmethod\ndef load(path: Path) -&gt; \"TopKSae\":\n    \"\"\"\n    Load TopKSAE from saved file using overcomplete's load method + our metadata.\n\n    Args:\n        path: Path to saved model file\n\n    Returns:\n        Loaded TopKSAE instance\n    \"\"\"\n    p = Path(path)\n\n    # Load payload\n    if torch.cuda.is_available():\n        map_location = 'cuda'\n    elif torch.backends.mps.is_available():\n        map_location = 'mps'\n    else:\n        map_location = 'cpu'\n    payload = torch.load(p, map_location=map_location)\n\n    # Extract our metadata\n    if \"mi_crow_metadata\" not in payload:\n        raise ValueError(f\"Invalid TopKSAE save format: missing 'mi_crow_metadata' key in {p}\")\n\n    mi_crow_meta = payload[\"mi_crow_metadata\"]\n    n_latents = int(mi_crow_meta[\"n_latents\"])\n    n_inputs = int(mi_crow_meta[\"n_inputs\"])\n    k = int(mi_crow_meta[\"k\"])\n    device = mi_crow_meta.get(\"device\", \"cpu\")\n    layer_signature = mi_crow_meta.get(\"layer_signature\")\n    model_id = mi_crow_meta.get(\"model_id\")\n    concepts_state = mi_crow_meta.get(\"concepts_state\", {})\n\n    # Create TopKSAE instance\n    topk_sae = TopKSae(\n        n_latents=n_latents,\n        n_inputs=n_inputs,\n        device=device\n    )\n\n    # Set k from saved metadata and reinitialize engine with correct k\n    topk_sae.k = k\n    topk_sae.sae_engine = topk_sae._initialize_sae_engine()\n\n    # Load overcomplete model state dict\n    if \"sae_state_dict\" in payload:\n        topk_sae.sae_engine.load_state_dict(payload[\"sae_state_dict\"])\n    elif \"model\" in payload:\n        # Backward compatibility with old format\n        topk_sae.sae_engine.load_state_dict(payload[\"model\"])\n    else:\n        # Assume payload is the state dict itself (backward compatibility)\n        topk_sae.sae_engine.load_state_dict(payload)\n\n    # Load concepts state\n    if concepts_state:\n        device = topk_sae.context.device\n        if isinstance(device, str):\n            device = torch.device(device)\n        if \"multiplication\" in concepts_state:\n            topk_sae.concepts.multiplication.data = concepts_state[\"multiplication\"].to(device)\n        if \"bias\" in concepts_state:\n            topk_sae.concepts.bias.data = concepts_state[\"bias\"].to(device)\n\n    # Note: Top texts loading was removed as serialization methods were removed\n    # Top texts should be exported/imported separately if needed\n\n    # Set context metadata\n    topk_sae.context.lm_layer_signature = layer_signature\n    topk_sae.context.model_id = model_id\n\n    params_str = f\"n_latents={n_latents}, n_inputs={n_inputs}, k={k}\"\n    logger.info(f\"\\nLoaded TopKSAE from {p}\\n{params_str}\")\n\n    return topk_sae\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.modify_activations","title":"modify_activations","text":"<pre><code>modify_activations(module, inputs, output)\n</code></pre> <p>Modify activations using TopKSAE (Controller hook interface).</p> <p>Extracts tensor from inputs/output, applies SAE forward pass, and optionally applies concept manipulation.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>inputs</code> <code>Tensor | None</code> <p>Tuple of inputs to the module</p> required <code>output</code> <code>Tensor | None</code> <p>Output from the module (None for pre_forward hooks)</p> required <p>Returns:</p> Type Description <code>Tensor | None</code> <p>Modified activations with same shape as input</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def modify_activations(\n        self,\n        module: \"torch.nn.Module\",\n        inputs: torch.Tensor | None,\n        output: torch.Tensor | None\n) -&gt; torch.Tensor | None:\n    \"\"\"\n    Modify activations using TopKSAE (Controller hook interface).\n\n    Extracts tensor from inputs/output, applies SAE forward pass,\n    and optionally applies concept manipulation.\n\n    Args:\n        module: The PyTorch module being hooked\n        inputs: Tuple of inputs to the module\n        output: Output from the module (None for pre_forward hooks)\n\n    Returns:\n        Modified activations with same shape as input\n    \"\"\"\n    # Extract tensor from output/inputs, handling objects with last_hidden_state\n    if self.hook_type == HookType.FORWARD:\n        if isinstance(output, torch.Tensor):\n            tensor = output\n        elif hasattr(output, \"last_hidden_state\") and isinstance(output.last_hidden_state, torch.Tensor):\n            tensor = output.last_hidden_state\n        elif isinstance(output, (tuple, list)):\n            # Try to find first tensor in tuple/list\n            tensor = next((item for item in output if isinstance(item, torch.Tensor)), None)\n        else:\n            tensor = None\n    else:\n        tensor = inputs[0] if len(inputs) &gt; 0 and isinstance(inputs[0], torch.Tensor) else None\n\n    if tensor is None or not isinstance(tensor, torch.Tensor):\n        return output if self.hook_type == HookType.FORWARD else inputs\n\n    original_shape = tensor.shape\n\n    # Flatten to 2D for SAE processing: (batch, seq_len, hidden) -&gt; (batch * seq_len, hidden)\n    # or keep as 2D if already 2D: (batch, hidden)\n    if len(original_shape) &gt; 2:\n        batch_size, seq_len = original_shape[:2]\n        tensor_flat = tensor.reshape(-1, original_shape[-1])\n    else:\n        batch_size = original_shape[0]\n        seq_len = 1\n        tensor_flat = tensor\n\n    # Get full activations (pre_codes) and sparse codes\n    # Overcomplete TopKSAE encode returns (pre_codes, codes)\n    pre_codes, codes = self.sae_engine.encode(tensor_flat)\n\n    # Save SAE activations (pre_codes) as 3D tensor: (batch, seq, n_latents)\n    latents_cpu = pre_codes.detach().cpu()\n    latents_3d = latents_cpu.reshape(batch_size, seq_len, -1)\n\n    # Save to tensor_metadata\n    self.tensor_metadata['neurons'] = latents_3d\n    self.tensor_metadata['activations'] = latents_3d\n\n    # Process each item in the batch individually for metadata\n    batch_items = []\n    n_items = latents_cpu.shape[0]\n    for item_idx in range(n_items):\n        item_latents = latents_cpu[item_idx]  # [n_latents]\n\n        # Find nonzero indices for this item\n        nonzero_mask = item_latents != 0\n        nonzero_indices = torch.nonzero(nonzero_mask, as_tuple=False).flatten().tolist()\n\n        # Create map of nonzero indices to activations\n        activations_map = {\n            int(idx): float(item_latents[idx].item())\n            for idx in nonzero_indices\n        }\n\n        # Create item metadata\n        item_metadata = {\n            \"nonzero_indices\": nonzero_indices,\n            \"activations\": activations_map\n        }\n        batch_items.append(item_metadata)\n\n    # Save batch items metadata\n    self.metadata['batch_items'] = batch_items\n\n    # Use sparse codes for reconstruction\n    latents = codes\n\n    # Update top texts if text tracking is enabled\n    if self._text_tracking_enabled and self.context.lm is not None:\n        input_tracker = self.context.lm.get_input_tracker()\n        if input_tracker is not None:\n            texts = input_tracker.get_current_texts()\n            if texts:\n                # Use pre_codes (full activations) for text tracking\n                self.concepts.update_top_texts_from_latents(\n                    latents_cpu,\n                    texts,\n                    original_shape\n                )\n\n    # Apply concept manipulation if parameters are set\n    # Check if multiplication or bias differ from defaults (ones)\n    if not torch.allclose(self.concepts.multiplication, torch.ones_like(self.concepts.multiplication)) or \\\n            not torch.allclose(self.concepts.bias, torch.ones_like(self.concepts.bias)):\n        # Apply manipulation: latents = latents * multiplication + bias\n        latents = latents * self.concepts.multiplication + self.concepts.bias\n\n    # Decode to get reconstruction\n    reconstructed = self.decode(latents)\n\n    # Reshape back to original shape\n    if len(original_shape) &gt; 2:\n        reconstructed = reconstructed.reshape(original_shape)\n\n    # Return in appropriate format\n    if self.hook_type == HookType.FORWARD:\n        if isinstance(output, torch.Tensor):\n            return reconstructed\n        elif isinstance(output, (tuple, list)):\n            # Replace first tensor in tuple/list\n            result = list(output)\n            for i, item in enumerate(result):\n                if isinstance(item, torch.Tensor):\n                    result[i] = reconstructed\n                    break\n            return tuple(result) if isinstance(output, tuple) else result\n        else:\n            # For objects with attributes, try to set last_hidden_state\n            if hasattr(output, \"last_hidden_state\"):\n                output.last_hidden_state = reconstructed\n            return output\n    else:  # PRE_FORWARD\n        # Return modified inputs tuple\n        result = list(inputs)\n        if len(result) &gt; 0:\n            result[0] = reconstructed\n        return tuple(result)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.process_activations","title":"process_activations","text":"<pre><code>process_activations(module, input, output)\n</code></pre> <p>Process activations (Detector interface).</p> <p>Metadata saving is handled in modify_activations to avoid duplicate work. This method is kept for interface compatibility but does nothing since modify_activations already saves the metadata when called.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The PyTorch module being hooked</p> required <code>input</code> <code>HOOK_FUNCTION_INPUT</code> <p>Tuple of input tensors to the module</p> required <code>output</code> <code>HOOK_FUNCTION_OUTPUT</code> <p>Output tensor(s) from the module</p> required Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def process_activations(\n        self,\n        module: torch.nn.Module,\n        input: HOOK_FUNCTION_INPUT,\n        output: HOOK_FUNCTION_OUTPUT\n) -&gt; None:\n    \"\"\"\n    Process activations (Detector interface).\n\n    Metadata saving is handled in modify_activations to avoid duplicate work.\n    This method is kept for interface compatibility but does nothing since\n    modify_activations already saves the metadata when called.\n\n    Args:\n        module: The PyTorch module being hooked\n        input: Tuple of input tensors to the module\n        output: Output tensor(s) from the module\n    \"\"\"\n    # Metadata saving is done in modify_activations to avoid duplicate encoding\n    pass\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.save","title":"save","text":"<pre><code>save(name, path=None)\n</code></pre> <p>Save model using overcomplete's state dict + our metadata.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model name</p> required <code>path</code> <code>str | Path | None</code> <p>Directory path to save to (defaults to current directory)</p> <code>None</code> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def save(self, name: str, path: str | Path | None = None) -&gt; None:\n    \"\"\"\n    Save model using overcomplete's state dict + our metadata.\n\n    Args:\n        name: Model name\n        path: Directory path to save to (defaults to current directory)\n    \"\"\"\n    if path is None:\n        path = Path.cwd()\n    save_dir = Path(path)\n    save_dir.mkdir(parents=True, exist_ok=True)\n    save_path = save_dir / f\"{name}.pt\"\n\n    # Save overcomplete model state dict\n    sae_state_dict = self.sae_engine.state_dict()\n\n    mi_crow_metadata = {\n        \"concepts_state\": {\n            'multiplication': self.concepts.multiplication.data,\n            'bias': self.concepts.bias.data,\n        },\n        \"n_latents\": self.context.n_latents,\n        \"n_inputs\": self.context.n_inputs,\n        \"k\": self.k,\n        \"device\": self.context.device,\n        \"layer_signature\": self.context.lm_layer_signature,\n        \"model_id\": self.context.model_id,\n    }\n\n    payload = {\n        \"sae_state_dict\": sae_state_dict,\n        \"mi_crow_metadata\": mi_crow_metadata,\n    }\n\n    torch.save(payload, save_path)\n    logger.info(f\"Saved TopKSAE to {save_path}\")\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSae.train","title":"train","text":"<pre><code>train(\n    store,\n    run_id,\n    layer_signature,\n    config=None,\n    training_run_id=None,\n)\n</code></pre> <p>Train TopKSAE using activations from a Store.</p> <p>This method delegates to the SaeTrainer composite class. If k is provided in the config and differs from the current k, the SAE engine will be reinitialized with the config's k value.</p> <p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>Store instance containing activations</p> required <code>run_id</code> <code>str</code> <p>Run ID to train on</p> required <code>config</code> <code>TopKSaeTrainingConfig | None</code> <p>Training configuration (must include k parameter)</p> <code>None</code> <code>training_run_id</code> <code>str | None</code> <p>Optional training run ID</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with keys: - \"history\": Training history dictionary - \"training_run_id\": Training run ID where outputs were saved</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If config is None or config.k is not set</p> Source code in <code>src/mi_crow/mechanistic/sae/modules/topk_sae.py</code> <pre><code>def train(\n        self,\n        store: Store,\n        run_id: str,\n        layer_signature: str | int,\n        config: TopKSaeTrainingConfig | None = None,\n        training_run_id: str | None = None\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Train TopKSAE using activations from a Store.\n\n    This method delegates to the SaeTrainer composite class.\n    If k is provided in the config and differs from the current k, the SAE engine\n    will be reinitialized with the config's k value.\n\n    Args:\n        store: Store instance containing activations\n        run_id: Run ID to train on\n        config: Training configuration (must include k parameter)\n        training_run_id: Optional training run ID\n\n    Returns:\n        Dictionary with keys:\n            - \"history\": Training history dictionary\n            - \"training_run_id\": Training run ID where outputs were saved\n\n    Raises:\n        ValueError: If config is None or config.k is not set\n    \"\"\"\n    if config is None:\n        config = TopKSaeTrainingConfig()\n\n    # Ensure k is set in config\n    if not hasattr(config, 'k') or config.k is None:\n        raise ValueError(\n            \"TopKSaeTrainingConfig must have k parameter set. \"\n            \"Example: TopKSaeTrainingConfig(k=10, epochs=100, ...)\"\n        )\n\n    # Set k from config and initialize/reinitialize the engine if needed\n    if self.k != config.k:\n        if self.k is None:\n            logger.info(f\"Initializing SAE engine with k={config.k}\")\n        else:\n            logger.info(f\"Reinitializing SAE engine with k={config.k} (was k={self.k})\")\n        self.k = config.k\n        # Initialize or reinitialize the SAE engine with k from config\n        self.sae_engine = self._initialize_sae_engine()\n\n    return self.trainer.train(store, run_id, layer_signature, config, training_run_id)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.autoencoder_context.AutoencoderContext","title":"mi_crow.mechanistic.sae.autoencoder_context.AutoencoderContext  <code>dataclass</code>","text":"<pre><code>AutoencoderContext(\n    autoencoder,\n    n_latents,\n    n_inputs,\n    lm=None,\n    lm_layer_signature=None,\n    model_id=None,\n    device=\"cpu\",\n    experiment_name=None,\n    run_id=None,\n    text_tracking_enabled=False,\n    text_tracking_k=5,\n    text_tracking_negative=False,\n    store=None,\n    tied=False,\n    bias_init=0.0,\n    init_method=\"kaiming\",\n)\n</code></pre> <p>Shared context for Autoencoder and its nested components.</p>"},{"location":"api/sae/#training","title":"Training","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.sae_trainer.SaeTrainer","title":"mi_crow.mechanistic.sae.sae_trainer.SaeTrainer","text":"<pre><code>SaeTrainer(sae)\n</code></pre> <p>Composite trainer class for SAE models using overcomplete's training functions.</p> <p>This trainer handles training of any SAE that has a sae_engine attribute compatible with overcomplete's train_sae functions.</p> <p>Initialize SaeTrainer.</p> <p>Parameters:</p> Name Type Description Default <code>sae</code> <code>Sae</code> <p>The SAE instance to train</p> required Source code in <code>src/mi_crow/mechanistic/sae/sae_trainer.py</code> <pre><code>def __init__(self, sae: \"Sae\") -&gt; None:\n    \"\"\"\n    Initialize SaeTrainer.\n\n    Args:\n        sae: The SAE instance to train\n    \"\"\"\n    self.sae = sae\n    self.logger = get_logger(__name__)\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.sae_trainer.SaeTrainingConfig","title":"mi_crow.mechanistic.sae.sae_trainer.SaeTrainingConfig  <code>dataclass</code>","text":"<pre><code>SaeTrainingConfig(\n    epochs=1,\n    batch_size=1024,\n    lr=0.001,\n    l1_lambda=0.0,\n    device=\"cpu\",\n    dtype=None,\n    max_batches_per_epoch=None,\n    verbose=False,\n    use_amp=True,\n    amp_dtype=None,\n    grad_accum_steps=1,\n    clip_grad=1.0,\n    monitoring=1,\n    scheduler=None,\n    max_nan_fallbacks=5,\n    use_wandb=False,\n    wandb_project=None,\n    wandb_entity=None,\n    wandb_name=None,\n    wandb_tags=None,\n    wandb_config=None,\n    wandb_mode=\"online\",\n    wandb_slow_metrics_frequency=50,\n    memory_efficient=False,\n)\n</code></pre> <p>Configuration for SAE training (compatible with overcomplete.train_sae).</p>"},{"location":"api/sae/#mi_crow.mechanistic.sae.modules.topk_sae.TopKSaeTrainingConfig","title":"mi_crow.mechanistic.sae.modules.topk_sae.TopKSaeTrainingConfig  <code>dataclass</code>","text":"<pre><code>TopKSaeTrainingConfig(\n    epochs=1,\n    batch_size=1024,\n    lr=0.001,\n    l1_lambda=0.0,\n    device=\"cpu\",\n    dtype=None,\n    max_batches_per_epoch=None,\n    verbose=False,\n    use_amp=True,\n    amp_dtype=None,\n    grad_accum_steps=1,\n    clip_grad=1.0,\n    monitoring=1,\n    scheduler=None,\n    max_nan_fallbacks=5,\n    use_wandb=False,\n    wandb_project=None,\n    wandb_entity=None,\n    wandb_name=None,\n    wandb_tags=None,\n    wandb_config=None,\n    wandb_mode=\"online\",\n    wandb_slow_metrics_frequency=50,\n    memory_efficient=False,\n    k=10,\n)\n</code></pre> <p>               Bases: <code>SaeTrainingConfig</code></p> <p>Training configuration for TopK SAE models.</p> <p>This class extends SaeTrainingConfig to provide a type-safe configuration interface specifically for TopK SAE models. It adds the <code>k</code> parameter which specifies how many top activations to keep during encoding.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of top activations to keep (required for TopK SAE training)</p> <code>10</code> Note <p>All other parameters are inherited from SaeTrainingConfig.</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>Number of top activations to keep during TopK encoding</p> Example <p>config = TopKSaeTrainingConfig( ...     k=10, ...     epochs=100, ...     batch_size=1024, ...     lr=1e-3, ...     l1_lambda=1e-4 ... )</p>"},{"location":"api/sae/#concepts","title":"Concepts","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts","title":"mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts","text":"<pre><code>AutoencoderConcepts(context)\n</code></pre> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def __init__(\n        self,\n        context: AutoencoderContext\n):\n    self.context = context\n    self._n_size = context.n_latents\n    self.dictionary: ConceptDictionary | None = None\n\n    # Concept manipulation parameters\n    self.multiplication = nn.Parameter(torch.ones(self._n_size))\n    self.bias = nn.Parameter(torch.ones(self._n_size))\n\n    # Top texts tracking\n    self._top_texts_heaps: list[list[tuple[float, tuple[float, str, int]]]] | None = None\n    self._text_tracking_k: int = 5\n    self._text_tracking_negative: bool = False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.enable_text_tracking","title":"enable_text_tracking","text":"<pre><code>enable_text_tracking()\n</code></pre> <p>Enable text tracking using context parameters.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def enable_text_tracking(self):\n    \"\"\"Enable text tracking using context parameters.\"\"\"\n    if self.context.lm is None:\n        raise ValueError(\"LanguageModel must be set in context to enable tracking\")\n\n    # Store tracking parameters\n    self._text_tracking_k = self.context.text_tracking_k\n    self._text_tracking_negative = self.context.text_tracking_negative\n\n    # Ensure InputTracker singleton exists on LanguageModel and enable it\n    input_tracker = self.context.lm._ensure_input_tracker()\n    input_tracker.enable()\n\n    # Enable text tracking on the SAE instance\n    if hasattr(self.context.autoencoder, '_text_tracking_enabled'):\n        self.context.autoencoder._text_tracking_enabled = True\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.generate_concepts_with_llm","title":"generate_concepts_with_llm","text":"<pre><code>generate_concepts_with_llm(llm_provider=None)\n</code></pre> <p>Generate concepts using LLM based on current top texts</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def generate_concepts_with_llm(self, llm_provider: str | None = None):\n    \"\"\"Generate concepts using LLM based on current top texts\"\"\"\n    if self._top_texts_heaps is None:\n        raise ValueError(\"No top texts available. Enable text tracking and run inference first.\")\n\n    from mi_crow.mechanistic.sae.concepts.concept_dictionary import ConceptDictionary\n    neuron_texts = self.get_all_top_texts()\n\n    self.dictionary = ConceptDictionary.from_llm(\n        neuron_texts=neuron_texts,\n        n_size=self._n_size,\n        store=self.dictionary.store if self.dictionary else None,\n        llm_provider=llm_provider\n    )\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.get_all_top_texts","title":"get_all_top_texts","text":"<pre><code>get_all_top_texts()\n</code></pre> <p>Get top texts for all neurons.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def get_all_top_texts(self) -&gt; list[list[NeuronText]]:\n    \"\"\"Get top texts for all neurons.\"\"\"\n    if self._top_texts_heaps is None:\n        return []\n    return [self.get_top_texts_for_neuron(i) for i in range(len(self._top_texts_heaps))]\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.get_top_texts_for_neuron","title":"get_top_texts_for_neuron","text":"<pre><code>get_top_texts_for_neuron(neuron_idx, top_m=None)\n</code></pre> <p>Get top texts for a specific neuron.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def get_top_texts_for_neuron(self, neuron_idx: int, top_m: int | None = None) -&gt; list[NeuronText]:\n    \"\"\"Get top texts for a specific neuron.\"\"\"\n    if self._top_texts_heaps is None or neuron_idx &lt; 0 or neuron_idx &gt;= len(self._top_texts_heaps):\n        return []\n    heap = self._top_texts_heaps[neuron_idx]\n    items = [val for (_, val) in heap]\n    reverse = not self._text_tracking_negative\n    items_sorted = sorted(items, key=lambda s_t: s_t[0], reverse=reverse)\n    if top_m is not None:\n        items_sorted = items_sorted[: top_m]\n\n    neuron_texts = []\n    for score, text, token_idx in items_sorted:\n        token_str = self._decode_token(text, token_idx)\n        neuron_texts.append(NeuronText(score=score, text=text, token_idx=token_idx, token_str=token_str))\n    return neuron_texts\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.reset_top_texts","title":"reset_top_texts","text":"<pre><code>reset_top_texts()\n</code></pre> <p>Reset all tracked top texts.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def reset_top_texts(self) -&gt; None:\n    \"\"\"Reset all tracked top texts.\"\"\"\n    self._top_texts_heaps = None\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.autoencoder_concepts.AutoencoderConcepts.update_top_texts_from_latents","title":"update_top_texts_from_latents","text":"<pre><code>update_top_texts_from_latents(\n    latents, texts, original_shape=None\n)\n</code></pre> <p>Update top texts heaps from latents and texts.</p> <p>Parameters:</p> Name Type Description Default <code>latents</code> <code>Tensor</code> <p>Latent activations tensor, shape [B*T, n_latents] or [B, n_latents] (already flattened)</p> required <code>texts</code> <code>Sequence[str]</code> <p>List of texts corresponding to the batch</p> required <code>original_shape</code> <code>tuple[int, ...] | None</code> <p>Original shape before flattening, e.g., (B, T, D) or (B, D)</p> <code>None</code> Source code in <code>src/mi_crow/mechanistic/sae/concepts/autoencoder_concepts.py</code> <pre><code>def update_top_texts_from_latents(\n        self,\n        latents: torch.Tensor,\n        texts: Sequence[str],\n        original_shape: tuple[int, ...] | None = None\n) -&gt; None:\n    \"\"\"\n    Update top texts heaps from latents and texts.\n\n    Args:\n        latents: Latent activations tensor, shape [B*T, n_latents] or [B, n_latents] (already flattened)\n        texts: List of texts corresponding to the batch\n        original_shape: Original shape before flattening, e.g., (B, T, D) or (B, D)\n    \"\"\"\n    if not texts:\n        return\n\n    n_neurons = latents.shape[-1]\n    self._ensure_heaps(n_neurons)\n\n    # Calculate batch and token dimensions\n    original_B = len(texts)\n    BT = latents.shape[0]  # Total positions (B*T if 3D original, or B if 2D original)\n\n    # Determine if original was 3D or 2D\n    if original_shape is not None and len(original_shape) == 3:\n        # Original was [B, T, D], latents are [B*T, n_latents]\n        B, T, _ = original_shape\n        # Verify batch size matches\n        if B != original_B:\n            logger.warning(f\"Batch size mismatch: original_shape has B={B}, but {original_B} texts provided\")\n            # Use the actual number of texts as batch size\n            B = original_B\n            T = BT // B if B &gt; 0 else 1\n        # Create token indices: [0, 1, 2, ..., T-1, 0, 1, 2, ..., T-1, ...]\n        token_indices = torch.arange(T, device='cpu').unsqueeze(0).expand(B, T).contiguous().view(B * T)\n    else:\n        # Original was [B, D], latents are [B, n_latents]\n        # All tokens are at index 0\n        T = 1\n        token_indices = torch.zeros(BT, dtype=torch.long, device='cpu')\n\n    # For each neuron, find the maximum activation per text\n    # This ensures we only track the best activation for each text, not every token position\n    for j in range(n_neurons):\n        heap = self._top_texts_heaps[j]\n\n        # For each text in the batch, find the max activation and its token position\n        texts_processed = 0\n        texts_added = 0\n        texts_updated = 0\n        texts_skipped_duplicate = 0\n        for batch_idx in range(original_B):\n            if batch_idx &gt;= len(texts):\n                continue\n\n            text = texts[batch_idx]\n            texts_processed += 1\n\n            # Get activations for this text (all token positions)\n            if original_shape is not None and len(original_shape) == 3:\n                # 3D case: [B, T, D] -&gt; get slice for this batch\n                start_idx = batch_idx * T\n                end_idx = start_idx + T\n                text_activations = latents[start_idx:end_idx, j]  # [T]\n                text_token_indices = token_indices[start_idx:end_idx]  # [T]\n            else:\n                # 2D case: [B, D] -&gt; single token\n                text_activations = latents[batch_idx:batch_idx + 1, j]  # [1]\n                text_token_indices = token_indices[batch_idx:batch_idx + 1]  # [1]\n\n            # Find the maximum activation (or minimum if tracking negative)\n            if self._text_tracking_negative:\n                # For negative tracking, find the most negative (minimum) value\n                max_idx = torch.argmin(text_activations)\n                max_score = float(text_activations[max_idx].item())\n                adj = -max_score  # Negate for heap ordering\n            else:\n                # For positive tracking, find the maximum value\n                max_idx = torch.argmax(text_activations)\n                max_score = float(text_activations[max_idx].item())\n                adj = max_score\n\n            # Skip if score is zero (no activation)\n            if max_score == 0.0:\n                continue\n\n            token_idx = int(text_token_indices[max_idx].item())\n\n            # Check if we already have this text in the heap\n            # If so, only update if this activation is better\n            existing_entry = None\n            heap_texts = []\n            for heap_idx, (heap_adj, (heap_score, heap_text, heap_token_idx)) in enumerate(heap):\n                heap_texts.append(heap_text[:50] if len(heap_text) &gt; 50 else heap_text)\n                if heap_text == text:\n                    existing_entry = (heap_idx, heap_adj, heap_score, heap_token_idx)\n                    break\n\n            if existing_entry is not None:\n                # Update existing entry if this activation is better\n                heap_idx, heap_adj, heap_score, heap_token_idx = existing_entry\n                if adj &gt; heap_adj:\n                    # Replace with better activation\n                    heap[heap_idx] = (adj, (max_score, text, token_idx))\n                    heapq.heapify(heap)  # Re-heapify after modification\n                    texts_updated += 1\n                else:\n                    texts_skipped_duplicate += 1\n            else:\n                # New text, add to heap\n                if len(heap) &lt; self._text_tracking_k:\n                    heapq.heappush(heap, (adj, (max_score, text, token_idx)))\n                    texts_added += 1\n                else:\n                    # Compare with smallest adjusted score; replace if better\n                    if adj &gt; heap[0][0]:\n                        heapq.heapreplace(heap, (adj, (max_score, text, token_idx)))\n                        texts_added += 1\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.concept_dictionary.ConceptDictionary","title":"mi_crow.mechanistic.sae.concepts.concept_dictionary.ConceptDictionary","text":"<pre><code>ConceptDictionary(n_size, store=None)\n</code></pre> Source code in <code>src/mi_crow/mechanistic/sae/concepts/concept_dictionary.py</code> <pre><code>def __init__(\n        self,\n        n_size: int,\n        store: Store | None = None\n) -&gt; None:\n    self.n_size = n_size\n    self.concepts_map: Dict[int, Concept] = {}\n    self.store = store\n    self._directory: Path | None = None\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.concept_models","title":"mi_crow.mechanistic.sae.concepts.concept_models","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker","title":"mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker","text":"<pre><code>InputTracker(language_model)\n</code></pre> <p>Simple listener that saves input texts before tokenization.</p> <p>This is a singleton per LanguageModel instance. It's used as a listener during inference to capture texts before they are tokenized. SAE hooks can then access these texts to track top activating texts for their neurons.</p> <p>Initialize InputTracker.</p> <p>Parameters:</p> Name Type Description Default <code>language_model</code> <code>LanguageModel</code> <p>Language model instance</p> required Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def __init__(\n        self,\n        language_model: \"LanguageModel\",\n) -&gt; None:\n    \"\"\"\n    Initialize InputTracker.\n\n    Args:\n        language_model: Language model instance\n    \"\"\"\n    self.language_model = language_model\n\n    # Flag to control whether to save inputs\n    self._enabled: bool = False\n\n    # Runtime state - only stores texts\n    self._current_texts: list[str] = []\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.enabled","title":"enabled  <code>property</code>","text":"<pre><code>enabled\n</code></pre> <p>Whether input tracking is enabled.</p>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.disable","title":"disable","text":"<pre><code>disable()\n</code></pre> <p>Disable input tracking.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def disable(self) -&gt; None:\n    \"\"\"Disable input tracking.\"\"\"\n    self._enabled = False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.enable","title":"enable","text":"<pre><code>enable()\n</code></pre> <p>Enable input tracking.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def enable(self) -&gt; None:\n    \"\"\"Enable input tracking.\"\"\"\n    self._enabled = True\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.get_current_texts","title":"get_current_texts","text":"<pre><code>get_current_texts()\n</code></pre> <p>Get the current batch of texts.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def get_current_texts(self) -&gt; list[str]:\n    \"\"\"Get the current batch of texts.\"\"\"\n    return self._current_texts.copy()\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset stored texts.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset stored texts.\"\"\"\n    self._current_texts.clear()\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.concepts.input_tracker.InputTracker.set_current_texts","title":"set_current_texts","text":"<pre><code>set_current_texts(texts)\n</code></pre> <p>Set the current batch of texts being processed.</p> <p>This is called by LanguageModel._inference() before tokenization if tracking is enabled.</p> Source code in <code>src/mi_crow/mechanistic/sae/concepts/input_tracker.py</code> <pre><code>def set_current_texts(self, texts: Sequence[str]) -&gt; None:\n    \"\"\"\n    Set the current batch of texts being processed.\n\n    This is called by LanguageModel._inference() before tokenization\n    if tracking is enabled.\n    \"\"\"\n    if self._enabled:\n        self._current_texts = list(texts)\n</code></pre>"},{"location":"api/sae/#training-utilities","title":"Training Utilities","text":""},{"location":"api/sae/#mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger","title":"mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger","text":"<pre><code>WandbLogger(config, run_id)\n</code></pre> <p>Handles wandb logging for SAE training.</p> <p>Encapsulates all wandb-related operations including initialization, metric logging, and summary updates.</p> <p>Initialize WandbLogger.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SaeTrainingConfig</code> <p>Training configuration</p> required <code>run_id</code> <code>str</code> <p>Training run identifier</p> required Source code in <code>src/mi_crow/mechanistic/sae/training/wandb_logger.py</code> <pre><code>def __init__(self, config: SaeTrainingConfig, run_id: str):\n    \"\"\"\n    Initialize WandbLogger.\n\n    Args:\n        config: Training configuration\n        run_id: Training run identifier\n    \"\"\"\n    self.config = config\n    self.run_id = run_id\n    self.wandb_run: Optional[Any] = None\n    self._initialized = False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Initialize wandb run if enabled in config.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if wandb was successfully initialized, False otherwise</p> Source code in <code>src/mi_crow/mechanistic/sae/training/wandb_logger.py</code> <pre><code>def initialize(self) -&gt; bool:\n    \"\"\"\n    Initialize wandb run if enabled in config.\n\n    Returns:\n        True if wandb was successfully initialized, False otherwise\n    \"\"\"\n    if not self.config.use_wandb:\n        return False\n\n    try:\n        import wandb\n    except ImportError:\n        logger.warning(\"[WandbLogger] wandb not installed, skipping wandb logging\")\n        logger.warning(\"[WandbLogger] Install with: pip install wandb\")\n        return False\n\n    try:\n        wandb_project = self.config.wandb_project or \"sae-training\"\n        wandb_name = self.config.wandb_name or self.run_id\n        wandb_mode = self.config.wandb_mode.lower() if self.config.wandb_mode else \"online\"\n\n        self.wandb_run = wandb.init(\n            project=wandb_project,\n            entity=self.config.wandb_entity,\n            name=wandb_name,\n            mode=wandb_mode,\n            config=self._build_wandb_config(),\n            tags=self.config.wandb_tags or [],\n        )\n        self._initialized = True\n        return True\n    except Exception as e:\n        logger.warning(f\"[WandbLogger] Unexpected error initializing wandb: {e}\")\n        logger.warning(\"[WandbLogger] Continuing training without wandb logging\")\n        return False\n</code></pre>"},{"location":"api/sae/#mi_crow.mechanistic.sae.training.wandb_logger.WandbLogger.log_metrics","title":"log_metrics","text":"<pre><code>log_metrics(history, verbose=False)\n</code></pre> <p>Log training metrics to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>dict[str, list[float | None]]</code> <p>Dictionary with training history (loss, r2, l1, l0, etc.)</p> required <code>verbose</code> <code>bool</code> <p>Whether to log verbose information</p> <code>False</code> Source code in <code>src/mi_crow/mechanistic/sae/training/wandb_logger.py</code> <pre><code>def log_metrics(\n        self,\n        history: dict[str, list[float | None]],\n        verbose: bool = False\n) -&gt; None:\n    \"\"\"\n    Log training metrics to wandb.\n\n    Args:\n        history: Dictionary with training history (loss, r2, l1, l0, etc.)\n        verbose: Whether to log verbose information\n    \"\"\"\n    if not self._initialized or self.wandb_run is None:\n        return\n\n    try:\n        num_epochs = len(history.get(\"loss\", []))\n        slow_metrics_freq = self.config.wandb_slow_metrics_frequency\n\n        # Helper to get last known value for slow metrics\n        def get_last_known_value(values: list[float | None], idx: int) -&gt; float:\n            \"\"\"Get the last non-None value up to idx, or 0.0 if none found.\"\"\"\n            for i in range(idx, -1, -1):\n                if i &lt; len(values) and values[i] is not None:\n                    return values[i]\n            return 0.0\n\n        # Log metrics for each epoch\n        for epoch in range(1, num_epochs + 1):\n            epoch_idx = epoch - 1\n            should_log_slow = (epoch % slow_metrics_freq == 0) or (epoch == num_epochs)\n\n            metrics = self._build_epoch_metrics(history, epoch_idx, should_log_slow, get_last_known_value)\n            self.wandb_run.log(metrics)\n\n        # Log final summary metrics\n        self._log_summary_metrics(history, get_last_known_value)\n\n        if verbose:\n            self._log_wandb_url()\n\n    except Exception as e:\n        logger.warning(f\"[WandbLogger] Failed to log metrics to wandb: {e}\")\n</code></pre>"},{"location":"api/store/","title":"Store API","text":"<p>Persistence layer for activations, models, and runs.</p>"},{"location":"api/store/#mi_crow.store","title":"mi_crow.store","text":""},{"location":"api/store/#mi_crow.store.LocalStore","title":"LocalStore","text":"<pre><code>LocalStore(\n    base_path=\"\",\n    runs_prefix=\"runs\",\n    dataset_prefix=\"datasets\",\n    model_prefix=\"models\",\n)\n</code></pre> <p>               Bases: <code>Store</code></p> <p>Local filesystem implementation of Store interface.</p> <p>Initialize LocalStore.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path | str</code> <p>Base directory path for the store</p> <code>''</code> <code>runs_prefix</code> <code>str</code> <p>Prefix for runs directory</p> <code>'runs'</code> <code>dataset_prefix</code> <code>str</code> <p>Prefix for datasets directory</p> <code>'datasets'</code> <code>model_prefix</code> <code>str</code> <p>Prefix for models directory</p> <code>'models'</code> Source code in <code>src/mi_crow/store/local_store.py</code> <pre><code>def __init__(\n        self,\n        base_path: Path | str = '',\n        runs_prefix: str = \"runs\",\n        dataset_prefix: str = \"datasets\",\n        model_prefix: str = \"models\",\n):\n    \"\"\"Initialize LocalStore.\n\n    Args:\n        base_path: Base directory path for the store\n        runs_prefix: Prefix for runs directory\n        dataset_prefix: Prefix for datasets directory\n        model_prefix: Prefix for models directory\n    \"\"\"\n    super().__init__(base_path, runs_prefix, dataset_prefix, model_prefix)\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store","title":"Store","text":"<pre><code>Store(\n    base_path=\"\",\n    runs_prefix=\"runs\",\n    dataset_prefix=\"datasets\",\n    model_prefix=\"models\",\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract store optimized for tensor batches grouped by run_id.</p> <p>This interface intentionally excludes generic bytes/JSON APIs. Implementations should focus on efficient safetensors-backed IO.</p> <p>The store organizes data hierarchically: - Runs: Top-level grouping by run_id - Batches: Within each run, data is organized by batch_index - Layers: Within each batch, tensors are organized by layer_signature - Keys: Within each layer, tensors are identified by key (e.g., \"activations\")</p> <p>Initialize Store.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path | str</code> <p>Base directory path for the store</p> <code>''</code> <code>runs_prefix</code> <code>str</code> <p>Prefix for runs directory (default: \"runs\")</p> <code>'runs'</code> <code>dataset_prefix</code> <code>str</code> <p>Prefix for datasets directory (default: \"datasets\")</p> <code>'datasets'</code> <code>model_prefix</code> <code>str</code> <p>Prefix for models directory (default: \"models\")</p> <code>'models'</code> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>def __init__(\n        self,\n        base_path: Path | str = \"\",\n        runs_prefix: str = \"runs\",\n        dataset_prefix: str = \"datasets\",\n        model_prefix: str = \"models\",\n):\n    \"\"\"Initialize Store.\n\n    Args:\n        base_path: Base directory path for the store\n        runs_prefix: Prefix for runs directory (default: \"runs\")\n        dataset_prefix: Prefix for datasets directory (default: \"datasets\")\n        model_prefix: Prefix for models directory (default: \"models\")\n    \"\"\"\n    self.runs_prefix = runs_prefix\n    self.dataset_prefix = dataset_prefix\n    self.model_prefix = model_prefix\n    self.base_path = Path(base_path)\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.get_detector_metadata","title":"get_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>get_detector_metadata(run_id, batch_index)\n</code></pre> <p>Load detector metadata with separate JSON and tensor store.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index</p> required <p>Returns:</p> Type Description <code>tuple[Dict[str, Any], TensorMetadata]</code> <p>Tuple of (metadata dict, tensor_metadata dict). Returns empty dicts if not found.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata format is invalid</p> <code>JSONDecodeError</code> <p>If metadata file exists but contains invalid JSON</p> <code>OSError</code> <p>If tensor files exist but cannot be loaded</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_detector_metadata(\n        self,\n        run_id: str,\n        batch_index: int\n) -&gt; tuple[Dict[str, Any], TensorMetadata]:\n    \"\"\"Load detector metadata with separate JSON and tensor store.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index\n\n    Returns:\n        Tuple of (metadata dict, tensor_metadata dict). Returns empty dicts if not found.\n\n    Raises:\n        ValueError: If parameters are invalid or metadata format is invalid\n        json.JSONDecodeError: If metadata file exists but contains invalid JSON\n        OSError: If tensor files exist but cannot be loaded\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.get_detector_metadata_by_layer_by_key","title":"get_detector_metadata_by_layer_by_key  <code>abstractmethod</code>","text":"<pre><code>get_detector_metadata_by_layer_by_key(\n    run_id, batch_index, layer, key\n)\n</code></pre> <p>Get a specific tensor from detector metadata by layer and key.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index</p> required <code>layer</code> <code>str</code> <p>Layer signature</p> required <code>key</code> <code>str</code> <p>Tensor key (e.g., \"activations\")</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The requested tensor</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid</p> <code>FileNotFoundError</code> <p>If the tensor doesn't exist</p> <code>OSError</code> <p>If tensor file exists but cannot be loaded</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_detector_metadata_by_layer_by_key(\n        self,\n        run_id: str,\n        batch_index: int,\n        layer: str,\n        key: str\n) -&gt; torch.Tensor:\n    \"\"\"Get a specific tensor from detector metadata by layer and key.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index\n        layer: Layer signature\n        key: Tensor key (e.g., \"activations\")\n\n    Returns:\n        The requested tensor\n\n    Raises:\n        ValueError: If parameters are invalid\n        FileNotFoundError: If the tensor doesn't exist\n        OSError: If tensor file exists but cannot be loaded\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.get_run_metadata","title":"get_run_metadata  <code>abstractmethod</code>","text":"<pre><code>get_run_metadata(run_id)\n</code></pre> <p>Load metadata for a run.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Metadata dictionary, or empty dict if not found</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_id is invalid</p> <code>JSONDecodeError</code> <p>If metadata file exists but contains invalid JSON</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef get_run_metadata(self, run_id: str) -&gt; Dict[str, Any]:\n    \"\"\"Load metadata for a run.\n\n    Args:\n        run_id: Run identifier\n\n    Returns:\n        Metadata dictionary, or empty dict if not found\n\n    Raises:\n        ValueError: If run_id is invalid\n        json.JSONDecodeError: If metadata file exists but contains invalid JSON\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.iter_run_batch_range","title":"iter_run_batch_range","text":"<pre><code>iter_run_batch_range(run_id, *, start=0, stop=None, step=1)\n</code></pre> <p>Iterate run batches for indices in range(start, stop, step).</p> <p>If stop is None, it will be set to max(list_run_batches(run_id)) + 1 (or 0 if none). Raises ValueError if step == 0 or start &lt; 0.</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>def iter_run_batch_range(\n        self,\n        run_id: str,\n        *,\n        start: int = 0,\n        stop: int | None = None,\n        step: int = 1,\n) -&gt; Iterator[List[torch.Tensor] | Dict[str, torch.Tensor]]:\n    \"\"\"Iterate run batches for indices in range(start, stop, step).\n\n    If stop is None, it will be set to max(list_run_batches(run_id)) + 1 (or 0 if none).\n    Raises ValueError if step == 0 or start &lt; 0.\n    \"\"\"\n    if step == 0:\n        raise ValueError(\"step must not be 0\")\n    if start &lt; 0:\n        raise ValueError(\"start must be &gt;= 0\")\n    indices = self.list_run_batches(run_id)\n    if not indices:\n        return\n    max_idx = max(indices)\n    if stop is None:\n        stop = max_idx + 1\n    for idx in range(start, stop, step):\n        try:\n            yield self.get_run_batch(run_id, idx)\n        except FileNotFoundError:\n            continue\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.put_detector_metadata","title":"put_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>put_detector_metadata(\n    run_id, batch_index, metadata, tensor_metadata\n)\n</code></pre> <p>Save detector metadata with separate JSON and tensor store.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>batch_index</code> <code>int</code> <p>Batch index (must be non-negative)</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>JSON-serializable metadata dictionary (aggregated from all detectors)</p> required <code>tensor_metadata</code> <code>TensorMetadata</code> <p>Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor            (from all detectors)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full path key used for store (e.g., \"runs/{run_id}/batch_{batch_index}\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata is not JSON-serializable</p> <code>OSError</code> <p>If file system operations fail</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_detector_metadata(\n        self,\n        run_id: str,\n        batch_index: int,\n        metadata: Dict[str, Any],\n        tensor_metadata: TensorMetadata\n) -&gt; str:\n    \"\"\"Save detector metadata with separate JSON and tensor store.\n\n    Args:\n        run_id: Run identifier\n        batch_index: Batch index (must be non-negative)\n        metadata: JSON-serializable metadata dictionary (aggregated from all detectors)\n        tensor_metadata: Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor\n                       (from all detectors)\n\n    Returns:\n        Full path key used for store (e.g., \"runs/{run_id}/batch_{batch_index}\")\n\n    Raises:\n        ValueError: If parameters are invalid or metadata is not JSON-serializable\n        OSError: If file system operations fail\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.put_run_detector_metadata","title":"put_run_detector_metadata  <code>abstractmethod</code>","text":"<pre><code>put_run_detector_metadata(\n    run_id, metadata, tensor_metadata\n)\n</code></pre> <p>Save detector metadata for a whole run in a unified location.</p> <p>This differs from <code>put_detector_metadata</code> which organises data per-batch under <code>runs/{run_id}/batch_{batch_index}</code>.</p> <p><code>put_run_detector_metadata</code> instead stores everything under <code>runs/{run_id}/detectors</code>. Implementations are expected to support being called multiple times for the same <code>run_id</code> and append / aggregate new metadata rather than overwrite it.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>JSON-serialisable metadata dictionary aggregated from all detectors for the current chunk / batch.</p> required <code>tensor_metadata</code> <code>TensorMetadata</code> <p>Dictionary mapping layer_signature to dict of tensor_key -&gt; tensor (from all detectors).</p> required <p>Returns:</p> Type Description <code>str</code> <p>String path/key where metadata was stored</p> <code>str</code> <p>(e.g. <code>runs/{run_id}/detectors</code>).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or metadata is not JSON\u2011serialisable.</p> <code>OSError</code> <p>If file system operations fail.</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_run_detector_metadata(\n        self,\n        run_id: str,\n        metadata: Dict[str, Any],\n        tensor_metadata: TensorMetadata,\n) -&gt; str:\n    \"\"\"\n    Save detector metadata for a whole run in a unified location.\n\n    This differs from ``put_detector_metadata`` which organises data\n    per-batch under ``runs/{run_id}/batch_{batch_index}``.\n\n    ``put_run_detector_metadata`` instead stores everything under\n    ``runs/{run_id}/detectors``. Implementations are expected to\n    support being called multiple times for the same ``run_id`` and\n    append / aggregate new metadata rather than overwrite it.\n\n    Args:\n        run_id: Run identifier\n        metadata: JSON-serialisable metadata dictionary aggregated\n            from all detectors for the current chunk / batch.\n        tensor_metadata: Dictionary mapping layer_signature to dict\n            of tensor_key -&gt; tensor (from all detectors).\n\n    Returns:\n        String path/key where metadata was stored\n        (e.g. ``runs/{run_id}/detectors``).\n\n    Raises:\n        ValueError: If parameters are invalid or metadata is not\n            JSON\u2011serialisable.\n        OSError: If file system operations fail.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/store/#mi_crow.store.Store.put_run_metadata","title":"put_run_metadata  <code>abstractmethod</code>","text":"<pre><code>put_run_metadata(run_id, meta)\n</code></pre> <p>Persist metadata for a run (e.g., dataset/model identifiers).</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str</code> <p>Run identifier</p> required <code>meta</code> <code>Dict[str, Any]</code> <p>Metadata dictionary to save (must be JSON-serializable)</p> required <p>Returns:</p> Type Description <code>str</code> <p>String path/key where metadata was stored (e.g., \"runs/{run_id}/meta.json\")</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If run_id is invalid or meta is not JSON-serializable</p> <code>OSError</code> <p>If file system operations fail</p> Note <p>Implementations should store JSON at a stable location, e.g., runs/{run_id}/meta.json.</p> Source code in <code>src/mi_crow/store/store.py</code> <pre><code>@abc.abstractmethod\ndef put_run_metadata(self, run_id: str, meta: Dict[str, Any]) -&gt; str:\n    \"\"\"Persist metadata for a run (e.g., dataset/model identifiers).\n\n    Args:\n        run_id: Run identifier\n        meta: Metadata dictionary to save (must be JSON-serializable)\n\n    Returns:\n        String path/key where metadata was stored (e.g., \"runs/{run_id}/meta.json\")\n\n    Raises:\n        ValueError: If run_id is invalid or meta is not JSON-serializable\n        OSError: If file system operations fail\n\n    Note:\n        Implementations should store JSON at a stable location, e.g., runs/{run_id}/meta.json.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"}]}