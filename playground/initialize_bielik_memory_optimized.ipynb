{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T08:09:30.628531Z",
     "start_time": "2025-09-09T08:09:28.961255Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device_lm = torch.device(\"mps\" if use_mps else \"cpu\")  # fallback CPU if needed\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# fp16 is the sweet spot on MPS; bf16 also works on newer PyTorch, but fp16 is safer today.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"sdpa\",  # MPS-friendly attention\n",
    ")\n",
    "model.to(device_lm)\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:13:49.434124Z",
     "start_time": "2025-09-09T08:13:46.517616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "inp = tok(prompt, return_tensors=\"pt\").to(device_lm)\n",
    "out = model.generate(**inp, max_new_tokens=50)\n",
    "print(tok.decode(out[0], skip_special_tokens=True))"
   ],
   "id": "43700a5c7f24ef7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain why the sky is blue in simple terms. The sky appears blue because of a phenomenon called Rayleigh scattering, which occurs when sunlight passes through Earth's atmosphere and interacts with the gases present in it. Blue light has shorter wavelengths than other colors of visible light, such as red or green. When\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:14:58.076422Z",
     "start_time": "2025-09-09T08:14:54.334577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# True streaming keeps RAM flat; you can also point this at your own text iterator.\n",
    "ds = load_dataset(\"chrisociepa/wikipedia-pl-20230401\", split=\"train\", streaming=True)\n",
    "\n",
    "max_len = 512    # start modest on MPS\n",
    "stride  = 256\n",
    "\n",
    "def token_blocks():\n",
    "    for item in ds:\n",
    "        ids = tok(item[\"text\"], return_tensors=\"pt\", truncation=False).input_ids[0]\n",
    "        for s in range(0, len(ids), stride):\n",
    "            chunk = ids[s:s+max_len]\n",
    "            if len(chunk) > 1:\n",
    "                yield chunk.unsqueeze(0)  # [1, T]"
   ],
   "id": "c372fc7c9ec46b9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:15:01.394336Z",
     "start_time": "2025-09-09T08:15:01.391916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "\n",
    "L = 12  # pick a mid layer first\n",
    "\n",
    "act_buf: Tensor = None\n",
    "def grab_hook(_m, _inp, out):\n",
    "    # out: [B, T, d_model] on MPS; keep it fp16 and detached\n",
    "    global act_buf\n",
    "    act_buf = out.detach()\n",
    "\n",
    "handle = model.model.layers[L].register_forward_hook(grab_hook)"
   ],
   "id": "d2585457852b613d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:15:03.457896Z",
     "start_time": "2025-09-09T08:15:03.453732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn, torch.optim as optim\n",
    "\n",
    "device_sae = device_lm  # run SAE on MPS too\n",
    "\n",
    "class TopKSAE(nn.Module):\n",
    "    def __init__(self, d_in, d_lat, k):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Linear(d_in, d_lat, bias=True)\n",
    "        self.dec = nn.Linear(d_lat, d_in, bias=True)\n",
    "        self.dec.weight = nn.Parameter(self.enc.weight.T)  # tied weights\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, d_in] (fp16 on MPS)\n",
    "        pre = self.enc(x)                             # [N, d_lat]\n",
    "        vals, idx = torch.topk(pre, k=self.k, dim=-1)\n",
    "        z = torch.zeros_like(pre)\n",
    "        # ReLU only on selected values\n",
    "        z.scatter_(dim=-1, index=idx, src=torch.relu(vals))\n",
    "        xhat = self.dec(z)\n",
    "        return xhat, z"
   ],
   "id": "da31192335fd5550",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:15:10.977763Z",
     "start_time": "2025-09-09T08:15:06.260589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sae, opt = None, None\n",
    "micro_bs = 4  # tiny microbatch fits MPS memory\n",
    "accum = 8  # gradient accumulation\n",
    "n_steps = 100\n",
    "step = 0\n",
    "k = 64  # active features/token\n",
    "torch.set_float32_matmul_precision(\"medium\")  # helps SDPA on Apple\n",
    "\n",
    "pbar = tqdm(total=n_steps, desc=\"SAE training\", initial=step)\n",
    "\n",
    "for chunk in token_blocks():\n",
    "\n",
    "    if step >= n_steps: break\n",
    "\n",
    "    # ---- LM forward (cheap; no grad) ----\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_ids=chunk.to(device_lm))  # fills act_buf via hook\n",
    "\n",
    "    H = act_buf.squeeze(0)  # [T, d_model], still on MPS, fp16\n",
    "    T, d = H.shape\n",
    "    if sae is None:\n",
    "        sae = TopKSAE(d_in=d, d_lat=8192, k=k).to(device_sae, dtype=torch.float16)\n",
    "        opt = optim.AdamW(sae.parameters(), lr=3e-3)\n",
    "\n",
    "    perm = torch.randperm(T, device=device_sae)\n",
    "    running = 0\n",
    "    for i in range(0, T, micro_bs):\n",
    "        idx = perm[i:i + micro_bs]\n",
    "        xb = H.index_select(0, idx)  # stays on MPS\n",
    "\n",
    "        with torch.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "            xhat, z = sae(xb)\n",
    "            recon = ((xhat - xb) ** 2).mean()\n",
    "\n",
    "        (recon / accum).backward()\n",
    "        running += 1\n",
    "        if running % accum == 0:\n",
    "            opt.step();\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "            if step % 500 == 0:\n",
    "                print(f\"step {step}: recon={float(recon):.4f}\")\n",
    "            if step >= n_steps:\n",
    "                break\n",
    "\n",
    "handle.remove()\n",
    "pbar.close()\n"
   ],
   "id": "904628db65727364",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAE training: 100%|██████████| 100/100 [00:04<00:00, 21.23it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:15:28.219291Z",
     "start_time": "2025-09-09T08:15:14.297453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_features(n_steps=3000, threshold=None):\n",
    "    # Running sums; create lazily to stay lean\n",
    "    sums, counts, maxes, fires = defaultdict(float), defaultdict(int), defaultdict(float), defaultdict(int)\n",
    "    total_tokens = 0\n",
    "\n",
    "    step = 0\n",
    "    pbar = tqdm(total=n_steps, desc=\"Scoring features\")\n",
    "    for chunk in token_blocks():\n",
    "        if step >= n_steps: break\n",
    "        _ = model(input_ids=chunk.to(device_lm))  # fills act_buf via hook\n",
    "        H = act_buf.squeeze(0)  # [T, d_model], fp16 on MPS\n",
    "        T, d = H.shape\n",
    "\n",
    "        # SAE encode (no reconstruction needed here)\n",
    "        with torch.autocast(device_type=\"mps\", dtype=torch.float16):\n",
    "            _, Z = sae(H)  # [T, n_latent] (dense or sparse)\n",
    "\n",
    "        # If your SAE is Top-K and returns dense z with zeros elsewhere, this is cheap.\n",
    "        # Otherwise set a threshold τ to count \"fires\".\n",
    "        if threshold is None:\n",
    "            # auto τ as 90th percentile per-batch to keep firing rare:\n",
    "            tau = torch.quantile(Z.detach().to(torch.float32).flatten(), 0.90).item()\n",
    "        else:\n",
    "            tau = threshold\n",
    "\n",
    "        Z32 = Z.to(torch.float32)  # stable stats on fp32\n",
    "        total_tokens += T\n",
    "\n",
    "        # Reduce along tokens in small slices to avoid large temps\n",
    "        bs = 2048\n",
    "        for i in range(0, T, bs):\n",
    "            z = Z32[i:i + bs]  # [b, n_latent]\n",
    "            sums_batch = z.sum(0)  # [n_latent]\n",
    "            max_batch = z.max(0).values\n",
    "            fires_batch = (z > tau).sum(0)\n",
    "\n",
    "            for j, val in enumerate(sums_batch):\n",
    "                v = float(val);\n",
    "                m = float(max_batch[j]);\n",
    "                f = int(fires_batch[j])\n",
    "                sums[j] += v\n",
    "                counts[j] += z.shape[0]\n",
    "                if m > maxes[j]: maxes[j] = m\n",
    "                fires[j] += f\n",
    "\n",
    "        step += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Aggregate stats\n",
    "    stats = {}\n",
    "    for k in sums.keys():\n",
    "        stats[k] = {\n",
    "            \"mean\": sums[k] / max(1, counts[k]),\n",
    "            \"max\": maxes[k],\n",
    "            \"fire_rate\": fires[k] / max(1, total_tokens)\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "\n",
    "stats = score_features(n_steps=3)  # ~quick skim\n"
   ],
   "id": "b298ee9931a09d81",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring features: 100%|██████████| 3/3 [00:13<00:00,  4.64s/it]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T22:00:06.842892Z",
     "start_time": "2025-09-08T22:00:06.838728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "F = 200  # focus on 200 most promising features\n",
    "# Sort by a combined score (max * sqrt(fire_rate))—balances salience & selectivity\n",
    "top_feat = sorted(stats.keys(),\n",
    "                  key=lambda k: stats[k][\"max\"] * math.sqrt(max(1e-8, stats[k][\"fire_rate\"])),\n",
    "                  reverse=True)[:F]\n",
    "\n",
    "top_feat"
   ],
   "id": "6f111233b45ad050",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b97a1abb52f67696"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
