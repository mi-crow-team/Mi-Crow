{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191ae575",
   "metadata": {},
   "source": [
    "# Training TopKSAE Model\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load a language model and dataset\n",
    "2. Save activations from a specific layer\n",
    "3. Train a TopK Sparse Autoencoder (TopKSAE) on those activations using the new `SaeTrainer` composite class\n",
    "4. Save the trained TopKSAE model\n",
    "\n",
    "The training uses overcomplete's `train_sae` functions via the `SaeTrainer` composite class, which is automatically available on all SAE instances via `sae.trainer`.\n",
    "\n",
    "All files (trained TopKSAE model, training metadata, activations) will be saved under `store/{model_id}/` for organized, model-specific storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "21ea60d2efb9b766",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:15.599353Z",
     "start_time": "2025-11-11T19:01:15.567856Z"
    }
   },
   "source": [
    "# Setup and imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Silence Hugging Face tokenizers fork/parallelism warnings\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "from amber.adapters import TextDataset\n",
    "from amber.core.language_model import LanguageModel\n",
    "from amber.mechanistic.sae.modules.topk_sae import TopKSae, TopKSAETrainingConfig\n",
    "from amber.store import LocalStore\n",
    "\n",
    "print(\"‚úÖ Imports completed\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "‚úÖ Imports completed\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "8d87fd502ea77c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:15.634731Z",
     "start_time": "2025-11-11T19:01:15.603709Z"
    }
   },
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"sshleifer/tiny-gpt2\"  # Small model for quick experimentation\n",
    "HF_DATASET = \"roneneldan/TinyStories\"\n",
    "DATA_SPLIT = \"train\"\n",
    "TEXT_FIELD = \"text\"\n",
    "DATA_LIMIT = 1000  # Number of text samples to use\n",
    "MAX_LENGTH = 64  # Maximum sequence length\n",
    "BATCH_SIZE_SAVE = 16  # Batch size for saving activations\n",
    "BATCH_SIZE_TRAIN = 32  # Batch size for SAE training\n",
    "\n",
    "# TopKSAE configuration\n",
    "TOP_K = 8  # Number of top activations to keep (sparsity parameter)\n",
    "\n",
    "# Choose which layer to hook - you can inspect available layers with model.layers.print_layer_names()\n",
    "LAYER_SIGNATURE = 'gpt2lmheadmodel_transformer_h_0_attn_c_attn'  # Attention layer (better activations)\n",
    "\n",
    "# Storage locations - will be updated after model loading to use model_id\n",
    "STORE_DIR = Path(\"store/train_sae\")\n",
    "CACHE_DIR = Path(\"store/cache\")\n",
    "RUN_ID = f\"topk_sae_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# Model-specific paths will be set after loading the model\n",
    "SAE_MODEL_PATH = None  # Will be set to store/{model_id}/topk_sae_model.pt\n",
    "METADATA_PATH = None  # Will be set to store/{model_id}/training_metadata.json\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.float16 if torch.cuda.is_available() else None  # Use half precision on GPU\n",
    "\n",
    "# Wandb configuration (optional - set use_wandb=True in training config to enable)\n",
    "WANDB_PROJECT = \"amber_playground\"  # Wandb project name\n",
    "WANDB_ENTITY = \"amber_team\"\n",
    "\n",
    "print(\"üöÄ Starting TopKSAE Training Example\")\n",
    "print(f\"üì± Using device: {DEVICE}\")\n",
    "print(f\"üîß Model: {MODEL_ID}\")\n",
    "print(f\"üìä Dataset: {HF_DATASET}\")\n",
    "print(f\"üéØ Target layer: {LAYER_SIGNATURE}\")\n",
    "print(f\"üî¢ TopK parameter: {TOP_K}\")\n",
    "print()\n",
    "\n",
    "# Create output directories\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"‚úÖ Output directories created\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting TopKSAE Training Example\n",
      "üì± Using device: cpu\n",
      "üîß Model: sshleifer/tiny-gpt2\n",
      "üìä Dataset: roneneldan/TinyStories\n",
      "üéØ Target layer: gpt2lmheadmodel_transformer_h_0_attn_c_attn\n",
      "üî¢ TopK parameter: 8\n",
      "\n",
      "‚úÖ Output directories created\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "7f4f2e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:16.668768Z",
     "start_time": "2025-11-11T19:01:15.638961Z"
    }
   },
   "source": [
    "# Step 1: Load language model with context\n",
    "print(\"üì• Loading language model...\")\n",
    "\n",
    "# Load model first to get model_idstore = LocalStore(MODEL_DIR)\n",
    "\n",
    "store = LocalStore(STORE_DIR)\n",
    "lm = LanguageModel.from_huggingface(MODEL_ID, store)\n",
    "lm.model.to(DEVICE)\n",
    "\n",
    "# Create model-specific directory for organizing all files\n",
    "\n",
    "# Update paths to use model-specific directory\n",
    "SAE_MODEL_PATH = STORE_DIR / \"topk_sae_model.pt\"\n",
    "METADATA_PATH = STORE_DIR / \"training_metadata.json\"\n",
    "\n",
    "# Print available layers for reference\n",
    "print(\"üîç Available layers:\")\n",
    "lm.layers.print_layer_names()\n",
    "print(f\"‚úÖ Model loaded: {lm.model_id}\")\n",
    "print(f\"üì± Device: {DEVICE}\")\n",
    "print(f\"üìÅ Store base: {STORE_DIR}\")\n",
    "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n",
    "print(f\"üíæ SAE model will be saved to: {SAE_MODEL_PATH}\")\n",
    "print(f\"üíæ Metadata will be saved to: {METADATA_PATH}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading language model...\n",
      "üîç Available layers:\n",
      "gpt2lmheadmodel_transformer: No weight\n",
      "gpt2lmheadmodel_transformer_wte: torch.Size([50257, 2])\n",
      "gpt2lmheadmodel_transformer_wpe: torch.Size([1024, 2])\n",
      "gpt2lmheadmodel_transformer_drop: No weight\n",
      "gpt2lmheadmodel_transformer_h: No weight\n",
      "gpt2lmheadmodel_transformer_h_0: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_ln_1: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_0_attn: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_attn_c_attn: torch.Size([2, 6])\n",
      "gpt2lmheadmodel_transformer_h_0_attn_c_proj: torch.Size([2, 2])\n",
      "gpt2lmheadmodel_transformer_h_0_attn_attn_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_attn_resid_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_ln_2: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_0_mlp: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_c_fc: torch.Size([2, 8])\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_c_proj: torch.Size([8, 2])\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_act: No weight\n",
      "gpt2lmheadmodel_transformer_h_0_mlp_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_1: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_ln_1: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_1_attn: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_attn_c_attn: torch.Size([2, 6])\n",
      "gpt2lmheadmodel_transformer_h_1_attn_c_proj: torch.Size([2, 2])\n",
      "gpt2lmheadmodel_transformer_h_1_attn_attn_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_attn_resid_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_ln_2: torch.Size([2])\n",
      "gpt2lmheadmodel_transformer_h_1_mlp: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_c_fc: torch.Size([2, 8])\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_c_proj: torch.Size([8, 2])\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_act: No weight\n",
      "gpt2lmheadmodel_transformer_h_1_mlp_dropout: No weight\n",
      "gpt2lmheadmodel_transformer_ln_f: torch.Size([2])\n",
      "gpt2lmheadmodel_lm_head: torch.Size([50257, 2])\n",
      "‚úÖ Model loaded: sshleifer_tiny-gpt2\n",
      "üì± Device: cpu\n",
      "üìÅ Store base: store/train_sae\n",
      "üìÅ Store location: store/train_sae\n",
      "üíæ SAE model will be saved to: store/train_sae/topk_sae_model.pt\n",
      "üíæ Metadata will be saved to: store/train_sae/training_metadata.json\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "6d926cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:17.983979Z",
     "start_time": "2025-11-11T19:01:16.688282Z"
    }
   },
   "source": [
    "# Step 2: Load dataset\n",
    "print(\"üì• Loading dataset...\")\n",
    "dataset = TextDataset.from_huggingface(\n",
    "    HF_DATASET,\n",
    "    split=DATA_SPLIT,\n",
    "    store=store,\n",
    "    text_field=TEXT_FIELD,\n",
    "    limit=DATA_LIMIT,\n",
    ")\n",
    "print(f\"‚úÖ Loaded {len(dataset)} text samples\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 453340.25 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1000 text samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "cae06f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:26.549537Z",
     "start_time": "2025-11-11T19:01:17.996520Z"
    }
   },
   "source": [
    "# Step 3: Save activations\n",
    "print(\"üíæ Saving activations...\")\n",
    "\n",
    "# Use the store that was set on the language model\n",
    "lm.activations.infer_and_save(\n",
    "    dataset,\n",
    "    layer_signature=LAYER_SIGNATURE,\n",
    "    run_name=RUN_ID,\n",
    "    store=lm.context.store,  # Use the store from language model\n",
    "    batch_size=BATCH_SIZE_SAVE,\n",
    "    autocast=False,  # Disable autocast for consistency\n",
    ")\n",
    "\n",
    "# Verify activations were saved\n",
    "batches = lm.context.store.list_run_batches(RUN_ID)\n",
    "print(f\"‚úÖ Saved {len(batches)} batches of activations\")\n",
    "print(f\"üìÅ Run ID: {RUN_ID}\")\n",
    "print(f\"üìÅ Store location: {lm.context.store.base_path}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving activations...\n",
      "‚úÖ Saved 63 batches of activations\n",
      "üìÅ Run ID: topk_sae_training_20251111_200115\n",
      "üìÅ Store location: store/train_sae\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "a451d6d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:26.597515Z",
     "start_time": "2025-11-11T19:01:26.567088Z"
    }
   },
   "source": [
    "# Step 4: Create TopKSAE model\n",
    "print(\"üèóÔ∏è Creating TopKSAE model...\")\n",
    "\n",
    "# Get the hidden dimension from the first batch\n",
    "first_batch = lm.context.store.get_run_batch(RUN_ID, 0)\n",
    "if isinstance(first_batch, dict):\n",
    "    activations = first_batch[\"activations\"]\n",
    "else:\n",
    "    activations = first_batch[0]  # Assume first tensor is activations\n",
    "\n",
    "hidden_dim = activations.shape[-1]  # Last dimension is hidden size\n",
    "print(f\"üìè Hidden dimension: {hidden_dim}\")\n",
    "\n",
    "sae = TopKSae(\n",
    "    n_latents=hidden_dim * 4,\n",
    "    n_inputs=hidden_dim,\n",
    "    k=TOP_K,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(f\"üß† TopKSAE architecture: {hidden_dim} ‚Üí {sae.context.n_latents} ‚Üí {hidden_dim}\")\n",
    "print(f\"üî¢ TopK parameter: {sae.k}\")\n",
    "print(f\"üîß Device: {DEVICE}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating TopKSAE model...\n",
      "üìè Hidden dimension: 6\n",
      "üß† TopKSAE architecture: 6 ‚Üí 24 ‚Üí 6\n",
      "üî¢ TopK parameter: 8\n",
      "üîß Device: cpu\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "b3dd61f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:31.950191Z",
     "start_time": "2025-11-11T19:01:26.602443Z"
    }
   },
   "source": [
    "# Step 5: Train TopKSAE using SaeTrainer\n",
    "print(\"üèãÔ∏è Training TopKSAE...\")\n",
    "print(\"üìù Note: Training uses overcomplete's train_sae functions via the SaeTrainer composite class\")\n",
    "print(f\"üîß Trainer available at: sae.trainer (type: {type(sae.trainer).__name__})\")\n",
    "print()\n",
    "\n",
    "# Configure training parameters\n",
    "# Note: TopKSAETrainingConfig is an alias for SaeTrainingConfig\n",
    "# You can also use SaeTrainingConfig directly from sae_trainer module\n",
    "config = TopKSAETrainingConfig(\n",
    "    epochs=100,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    lr=1e-3,\n",
    "    l1_lambda=1e-4,  # L1 sparsity penalty\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    max_batches_per_epoch=50,  # Limit batches per epoch for quick training\n",
    "    verbose=True,  # Enable progress logging\n",
    "    use_amp=True,\n",
    "    amp_dtype=DTYPE,\n",
    "    clip_grad=1.0,  # Gradient clipping (overcomplete parameter)\n",
    "    monitoring=2,  # Detailed monitoring (0=silent, 1=basic, 2=detailed)\n",
    "    # Wandb configuration (set use_wandb=True to enable wandb logging)\n",
    "    use_wandb=True,  # Enable wandb logging\n",
    "    wandb_entity=WANDB_ENTITY,\n",
    "    wandb_project=WANDB_PROJECT,\n",
    "    wandb_name=RUN_ID,  # Use run_id as wandb run name\n",
    "    wandb_tags=[\"topk-sae\", \"sae-training\", MODEL_ID.split(\"/\")[-1]],\n",
    "    wandb_config={\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"dataset\": HF_DATASET,\n",
    "        \"layer_signature\": LAYER_SIGNATURE,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"data_limit\": DATA_LIMIT,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Train using TopKSAE's train method (which delegates to sae.trainer.train())\n",
    "# The trainer uses overcomplete's train_sae_amp or train_sae functions internally\n",
    "history = sae.train(lm.context.store, RUN_ID, config)\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(f\"üìà Final loss: {history['loss'][-1]:.6f}\")\n",
    "print(f\"üìà Final reconstruction MSE: {history['recon_mse'][-1]:.6f}\")\n",
    "print(f\"üìà Final L1 penalty: {history['l1'][-1]:.6f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèãÔ∏è Training TopKSAE...\n",
      "üìù Note: Training uses overcomplete's train_sae functions via the SaeTrainer composite class\n",
      "üîß Trainer available at: sae.trainer (type: SaeTrainer)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/l1_penalty</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/reconstruction_mse</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>final/l1_penalty</td><td>0</td></tr><tr><td>final/loss</td><td>1e-05</td></tr><tr><td>final/reconstruction_mse</td><td>0.00072</td></tr><tr><td>train/l1_penalty</td><td>0</td></tr><tr><td>train/loss</td><td>1e-05</td></tr><tr><td>train/reconstruction_mse</td><td>0.00072</td></tr><tr><td>training/num_epochs</td><td>100</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">topk_sae_training_20251111_195719</strong> at: <a href='https://wandb.ai/amber_team/amber_playground/runs/2z2e5tq2' target=\"_blank\">https://wandb.ai/amber_team/amber_playground/runs/2z2e5tq2</a><br> View project at: <a href='https://wandb.ai/amber_team/amber_playground' target=\"_blank\">https://wandb.ai/amber_team/amber_playground</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20251111_195730-2z2e5tq2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/adam/Projects/Inzynierka/codebase/playground/wandb/run-20251111_200126-44xekhgr</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amber_team/amber_playground/runs/44xekhgr' target=\"_blank\">topk_sae_training_20251111_200115</a></strong> to <a href='https://wandb.ai/amber_team/amber_playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/amber_team/amber_playground' target=\"_blank\">https://wandb.ai/amber_team/amber_playground</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/amber_team/amber_playground/runs/44xekhgr' target=\"_blank\">https://wandb.ai/amber_team/amber_playground/runs/44xekhgr</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 20:01:28,860 [INFO] amber.mechanistic.sae.sae_trainer: [SaeTrainer] Starting training run_id=topk_sae_training_20251111_200115 epochs=100 batch_size=32 device=cpu use_amp=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/100], Loss: 0.0528, R2: -68.4890, L0: 8.0000, Dead Features: 66.7%, Time: 0.0360 seconds\n",
      "Epoch[2/100], Loss: 0.0181, R2: -22.8135, L0: 8.0000, Dead Features: 66.7%, Time: 0.0351 seconds\n",
      "Epoch[3/100], Loss: 0.0054, R2: -6.1489, L0: 8.0000, Dead Features: 62.5%, Time: 0.0319 seconds\n",
      "Epoch[4/100], Loss: 0.0017, R2: -1.2405, L0: 8.0000, Dead Features: 62.5%, Time: 0.0330 seconds\n",
      "Epoch[5/100], Loss: 0.0008, R2: -0.0359, L0: 8.0000, Dead Features: 66.7%, Time: 0.0310 seconds\n",
      "Epoch[6/100], Loss: 0.0006, R2: 0.2347, L0: 8.0000, Dead Features: 62.5%, Time: 0.0283 seconds\n",
      "Epoch[7/100], Loss: 0.0005, R2: 0.3386, L0: 8.0000, Dead Features: 66.7%, Time: 0.0277 seconds\n",
      "Epoch[8/100], Loss: 0.0005, R2: 0.4081, L0: 8.0000, Dead Features: 66.7%, Time: 0.0321 seconds\n",
      "Epoch[9/100], Loss: 0.0004, R2: 0.4667, L0: 8.0000, Dead Features: 66.7%, Time: 0.0312 seconds\n",
      "Epoch[10/100], Loss: 0.0004, R2: 0.5207, L0: 8.0000, Dead Features: 66.7%, Time: 0.0273 seconds\n",
      "Epoch[11/100], Loss: 0.0003, R2: 0.5706, L0: 8.0000, Dead Features: 66.7%, Time: 0.0252 seconds\n",
      "Epoch[12/100], Loss: 0.0003, R2: 0.6157, L0: 8.0000, Dead Features: 66.7%, Time: 0.0288 seconds\n",
      "Epoch[13/100], Loss: 0.0003, R2: 0.6569, L0: 8.0000, Dead Features: 66.7%, Time: 0.0281 seconds\n",
      "Epoch[14/100], Loss: 0.0002, R2: 0.6948, L0: 8.0000, Dead Features: 62.5%, Time: 0.0274 seconds\n",
      "Epoch[15/100], Loss: 0.0002, R2: 0.7338, L0: 8.0000, Dead Features: 66.7%, Time: 0.0275 seconds\n",
      "Epoch[16/100], Loss: 0.0002, R2: 0.7667, L0: 8.0000, Dead Features: 66.7%, Time: 0.0304 seconds\n",
      "Epoch[17/100], Loss: 0.0002, R2: 0.7978, L0: 8.0000, Dead Features: 66.7%, Time: 0.0307 seconds\n",
      "Epoch[18/100], Loss: 0.0001, R2: 0.8266, L0: 8.0000, Dead Features: 66.7%, Time: 0.0268 seconds\n",
      "Epoch[19/100], Loss: 0.0001, R2: 0.8520, L0: 8.0000, Dead Features: 66.7%, Time: 0.0305 seconds\n",
      "Epoch[20/100], Loss: 0.0001, R2: 0.8756, L0: 8.0000, Dead Features: 66.7%, Time: 0.0297 seconds\n",
      "Epoch[21/100], Loss: 0.0001, R2: 0.8958, L0: 8.0000, Dead Features: 62.5%, Time: 0.0283 seconds\n",
      "Epoch[22/100], Loss: 0.0001, R2: 0.9151, L0: 8.0000, Dead Features: 66.7%, Time: 0.0287 seconds\n",
      "Epoch[23/100], Loss: 0.0001, R2: 0.9304, L0: 8.0000, Dead Features: 66.7%, Time: 0.0305 seconds\n",
      "Epoch[24/100], Loss: 0.0001, R2: 0.9441, L0: 8.0000, Dead Features: 66.7%, Time: 0.0304 seconds\n",
      "Epoch[25/100], Loss: 0.0000, R2: 0.9560, L0: 8.0000, Dead Features: 66.7%, Time: 0.0287 seconds\n",
      "Epoch[26/100], Loss: 0.0000, R2: 0.9657, L0: 8.0000, Dead Features: 66.7%, Time: 0.0280 seconds\n",
      "Epoch[27/100], Loss: 0.0000, R2: 0.9734, L0: 8.0000, Dead Features: 66.7%, Time: 0.0294 seconds\n",
      "Epoch[28/100], Loss: 0.0000, R2: 0.9800, L0: 8.0000, Dead Features: 66.7%, Time: 0.0305 seconds\n",
      "Epoch[29/100], Loss: 0.0000, R2: 0.9850, L0: 8.0000, Dead Features: 66.7%, Time: 0.0290 seconds\n",
      "Epoch[30/100], Loss: 0.0000, R2: 0.9884, L0: 8.0000, Dead Features: 66.7%, Time: 0.0297 seconds\n",
      "Epoch[31/100], Loss: 0.0000, R2: 0.9916, L0: 8.0000, Dead Features: 66.7%, Time: 0.0307 seconds\n",
      "Epoch[32/100], Loss: 0.0000, R2: 0.9935, L0: 8.0000, Dead Features: 66.7%, Time: 0.0311 seconds\n",
      "Epoch[33/100], Loss: 0.0000, R2: 0.9951, L0: 8.0000, Dead Features: 66.7%, Time: 0.0318 seconds\n",
      "Epoch[34/100], Loss: 0.0000, R2: 0.9965, L0: 8.0000, Dead Features: 66.7%, Time: 0.0319 seconds\n",
      "Epoch[35/100], Loss: 0.0000, R2: 0.9970, L0: 8.0000, Dead Features: 62.5%, Time: 0.0317 seconds\n",
      "Epoch[36/100], Loss: 0.0000, R2: 0.9981, L0: 8.0000, Dead Features: 66.7%, Time: 0.0357 seconds\n",
      "Epoch[37/100], Loss: 0.0000, R2: 0.9986, L0: 8.0000, Dead Features: 66.7%, Time: 0.0347 seconds\n",
      "Epoch[38/100], Loss: 0.0000, R2: 0.9989, L0: 8.0000, Dead Features: 66.7%, Time: 0.0337 seconds\n",
      "Epoch[39/100], Loss: 0.0000, R2: 0.9990, L0: 8.0000, Dead Features: 66.7%, Time: 0.0305 seconds\n",
      "Epoch[40/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0314 seconds\n",
      "Epoch[41/100], Loss: 0.0000, R2: 0.9990, L0: 8.0000, Dead Features: 66.7%, Time: 0.0315 seconds\n",
      "Epoch[42/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0311 seconds\n",
      "Epoch[43/100], Loss: 0.0000, R2: 0.9995, L0: 8.0000, Dead Features: 66.7%, Time: 0.0321 seconds\n",
      "Epoch[44/100], Loss: 0.0000, R2: 0.9996, L0: 8.0000, Dead Features: 66.7%, Time: 0.0335 seconds\n",
      "Epoch[45/100], Loss: 0.0000, R2: 0.9996, L0: 8.0000, Dead Features: 66.7%, Time: 0.0294 seconds\n",
      "Epoch[46/100], Loss: 0.0000, R2: 0.9996, L0: 8.0000, Dead Features: 66.7%, Time: 0.0299 seconds\n",
      "Epoch[47/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0306 seconds\n",
      "Epoch[48/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0284 seconds\n",
      "Epoch[49/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0284 seconds\n",
      "Epoch[50/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0331 seconds\n",
      "Epoch[51/100], Loss: 0.0000, R2: 0.9995, L0: 8.0000, Dead Features: 66.7%, Time: 0.0343 seconds\n",
      "Epoch[52/100], Loss: 0.0000, R2: 0.9995, L0: 8.0000, Dead Features: 66.7%, Time: 0.0352 seconds\n",
      "Epoch[53/100], Loss: 0.0000, R2: 0.9995, L0: 8.0000, Dead Features: 66.7%, Time: 0.0320 seconds\n",
      "Epoch[54/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0329 seconds\n",
      "Epoch[55/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0316 seconds\n",
      "Epoch[56/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0305 seconds\n",
      "Epoch[57/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0268 seconds\n",
      "Epoch[58/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0337 seconds\n",
      "Epoch[59/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0325 seconds\n",
      "Epoch[60/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0318 seconds\n",
      "Epoch[61/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0322 seconds\n",
      "Epoch[62/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0249 seconds\n",
      "Epoch[63/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0272 seconds\n",
      "Epoch[64/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0275 seconds\n",
      "Epoch[65/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0311 seconds\n",
      "Epoch[66/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0327 seconds\n",
      "Epoch[67/100], Loss: 0.0000, R2: 0.9990, L0: 8.0000, Dead Features: 66.7%, Time: 0.0309 seconds\n",
      "Epoch[68/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0315 seconds\n",
      "Epoch[69/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0302 seconds\n",
      "Epoch[70/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0287 seconds\n",
      "Epoch[71/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0269 seconds\n",
      "Epoch[72/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0294 seconds\n",
      "Epoch[73/100], Loss: 0.0000, R2: 0.9989, L0: 8.0000, Dead Features: 66.7%, Time: 0.0349 seconds\n",
      "Epoch[74/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0326 seconds\n",
      "Epoch[75/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0304 seconds\n",
      "Epoch[76/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0315 seconds\n",
      "Epoch[77/100], Loss: 0.0000, R2: 0.9995, L0: 8.0000, Dead Features: 66.7%, Time: 0.0307 seconds\n",
      "Epoch[78/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0306 seconds\n",
      "Epoch[79/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0321 seconds\n",
      "Epoch[80/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0319 seconds\n",
      "Epoch[81/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0291 seconds\n",
      "Epoch[82/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0277 seconds\n",
      "Epoch[83/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0296 seconds\n",
      "Epoch[84/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0280 seconds\n",
      "Epoch[85/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0295 seconds\n",
      "Epoch[86/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0294 seconds\n",
      "Epoch[87/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0336 seconds\n",
      "Epoch[88/100], Loss: 0.0000, R2: 0.9990, L0: 8.0000, Dead Features: 66.7%, Time: 0.0317 seconds\n",
      "Epoch[89/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0308 seconds\n",
      "Epoch[90/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0273 seconds\n",
      "Epoch[91/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0261 seconds\n",
      "Epoch[92/100], Loss: 0.0000, R2: 0.9993, L0: 8.0000, Dead Features: 66.7%, Time: 0.0276 seconds\n",
      "Epoch[93/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0279 seconds\n",
      "Epoch[94/100], Loss: 0.0000, R2: 0.9987, L0: 8.0000, Dead Features: 66.7%, Time: 0.0320 seconds\n",
      "Epoch[95/100], Loss: 0.0000, R2: 0.9990, L0: 8.0000, Dead Features: 66.7%, Time: 0.0322 seconds\n",
      "Epoch[96/100], Loss: 0.0000, R2: 0.9994, L0: 8.0000, Dead Features: 66.7%, Time: 0.0319 seconds\n",
      "Epoch[97/100], Loss: 0.0000, R2: 0.9992, L0: 8.0000, Dead Features: 66.7%, Time: 0.0330 seconds\n",
      "Epoch[98/100], Loss: 0.0000, R2: 0.9988, L0: 8.0000, Dead Features: 66.7%, Time: 0.0337 seconds\n",
      "Epoch[99/100], Loss: 0.0000, R2: 0.9990, L0: 8.0000, Dead Features: 66.7%, Time: 0.0336 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 20:01:31,942 [INFO] amber.mechanistic.sae.sae_trainer: [SaeTrainer] Metrics logged to wandb: https://wandb.ai/amber_team/amber_playground/runs/44xekhgr\n",
      "2025-11-11 20:01:31,942 [INFO] amber.mechanistic.sae.sae_trainer: [SaeTrainer] Completed training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[100/100], Loss: 0.0000, R2: 0.9991, L0: 8.0000, Dead Features: 66.7%, Time: 0.0350 seconds\n",
      "\n",
      "‚úÖ Training completed!\n",
      "üìà Final loss: 0.000008\n",
      "üìà Final reconstruction MSE: 0.000866\n",
      "üìà Final L1 penalty: 0.000000\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "e2c17d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:32.000360Z",
     "start_time": "2025-11-11T19:01:31.966747Z"
    }
   },
   "source": [
    "# Step 6: Save trained TopKSAE\n",
    "print(\"üíæ Saving trained TopKSAE...\")\n",
    "\n",
    "# Save using TopKSAE's save method (saves overcomplete model + our metadata)\n",
    "sae.save(\n",
    "    name=\"topk_sae_model\",\n",
    "    path=SAE_MODEL_PATH.parent\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ TopKSAE saved to: {SAE_MODEL_PATH}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 20:01:31,998 [INFO] amber.mechanistic.sae.modules.topk_sae: Saved TopKSAE to store/train_sae/topk_sae_model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving trained TopKSAE...\n",
      "‚úÖ TopKSAE saved to: store/train_sae/topk_sae_model.pt\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "535b3b268d95fe2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:32.033420Z",
     "start_time": "2025-11-11T19:01:32.004244Z"
    }
   },
   "source": [
    "lm.layers.register_hook(LAYER_SIGNATURE, sae)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6a51ff1e-9170-4692-8a5a-21fd6e624669'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "a028ef6a3f544970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T19:01:32.037792Z",
     "start_time": "2025-11-11T19:01:32.036222Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
