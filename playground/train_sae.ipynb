{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:16:05.197716Z",
     "start_time": "2025-09-29T20:16:05.174145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Jupyter-friendly script (use in VS Code/JetBrains as a notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# --- Imports ---\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "from amber.store import LocalStore\n",
    "from amber.adapters.text_snippet_dataset import TextSnippetDataset\n",
    "from amber.core.language_model import LanguageModel\n",
    "from amber.mechanistic.autoencoder.autoencoder import Autoencoder\n",
    "from amber.mechanistic.autoencoder.train import SAETrainer, SAETrainingConfig\n"
   ],
   "id": "fb123b488a56a96d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:16:05.499240Z",
     "start_time": "2025-09-29T20:16:05.473881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration ---\n",
    "MODEL_ID = \"sshleifer/tiny-gpt2\"  # tiny model for quick experimentation\n",
    "HF_DATASET = \"roneneldan/TinyStories\"\n",
    "DATA_SPLIT = \"train\"\n",
    "TEXT_FIELD = \"text\"\n",
    "DATA_LIMIT = 200  # keep small for a quick demo\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE_SAVE = 8\n",
    "\n",
    "# Choose which layer to hook. You can use an integer index or a layer name.\n",
    "# Use model.layers.get_layer_names() below to inspect available names.\n",
    "LAYER_SIGNATURE: int | str = 'gpt2lmheadmodel_lm_head'\n",
    "\n",
    "# Storage locations\n",
    "CACHE_DIR = Path(\"./store/tinystories\")\n",
    "STORE_DIR = Path(\"./store/tiny-gpt2\")\n",
    "RUN_ID = f\"tinystories_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = None  # e.g., torch.float16 for lower storage; keep None to preserve dtype\n",
    "\n",
    "# SAE config\n",
    "SAE_EPOCHS = 2\n",
    "SAE_MINIBATCH = 1024  # mini-batch within stored activation batches\n",
    "SAE_LR = 1e-3\n",
    "SAE_L1 = 0.0  # sparsity penalty on latents (set > 0.0 to encourage sparsity)\n"
   ],
   "id": "967a3345dc88604a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:16:08.815043Z",
     "start_time": "2025-09-29T20:16:06.189370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Initialize model and dataset ---\n",
    "model = LanguageModel.from_huggingface(MODEL_ID)\n",
    "model.model.to(DEVICE)\n",
    "\n",
    "# Optionally, inspect available layer names\n",
    "layer_names = model.layers.print_layer_names()\n",
    "print(f\"Discovered {len(layer_names)} layers. Example names: {layer_names[:5]}\")\n",
    "\n",
    "# Load a small text dataset\n",
    "dataset = TextSnippetDataset.from_huggingface(\n",
    "    HF_DATASET,\n",
    "    split=DATA_SPLIT,\n",
    "    cache_dir=str(CACHE_DIR),\n",
    "    text_field=TEXT_FIELD,\n",
    "    limit=DATA_LIMIT,\n",
    ")\n",
    "\n",
    "# Prepare a LocalStore for saving activations\n",
    "store = LocalStore(STORE_DIR)\n",
    "print(f\"Store base path: {store.base_path}\")\n",
    "print(f\"Run id: {RUN_ID}\")"
   ],
   "id": "ad1bb1a6790496d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 33 layers. Example names: ['gpt2lmheadmodel_transformer', 'gpt2lmheadmodel_transformer_wte', 'gpt2lmheadmodel_transformer_wpe', 'gpt2lmheadmodel_transformer_drop', 'gpt2lmheadmodel_transformer_h']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 110492.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store base path: store/tiny-gpt2\n",
      "Run id: tinystories_20250929_221605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:16:11.093924Z",
     "start_time": "2025-09-29T20:16:09.750453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Save activations for the chosen layer ---\n",
    "# This will iterate over dataset in small batches, run the model, capture layer outputs,\n",
    "# and write per-batch safetensors files under STORE_DIR/runs/{RUN_ID}/\n",
    "model.activations.infer_and_save(\n",
    "    dataset,\n",
    "    layer_signature=LAYER_SIGNATURE,\n",
    "    run_name=RUN_ID,\n",
    "    store=store,\n",
    "    batch_size=BATCH_SIZE_SAVE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    autocast=True,\n",
    "    save_inputs=True,\n",
    "    free_cuda_cache_every=0,\n",
    "    verbose=True,\n",
    ")\n"
   ],
   "id": "45203b94a348b019",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 22:16:09,771 [INFO] amber.core.language_model_activations: Starting save_model_activations: run=tinystories_20250929_221605, layer=gpt2lmheadmodel_lm_head, batch_size=8, device=cpu\n",
      "2025-09-29 22:16:09,773 [INFO] amber.core.language_model_activations: Prepared batch 0: items=8, seq_len=128\n",
      "2025-09-29 22:16:09,826 [INFO] amber.core.language_model_activations: Saved batch 0 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:09,828 [INFO] amber.core.language_model_activations: Prepared batch 1: items=8, seq_len=128\n",
      "2025-09-29 22:16:09,880 [INFO] amber.core.language_model_activations: Saved batch 1 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:09,882 [INFO] amber.core.language_model_activations: Prepared batch 2: items=8, seq_len=128\n",
      "2025-09-29 22:16:09,934 [INFO] amber.core.language_model_activations: Saved batch 2 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:09,936 [INFO] amber.core.language_model_activations: Prepared batch 3: items=8, seq_len=128\n",
      "2025-09-29 22:16:09,984 [INFO] amber.core.language_model_activations: Saved batch 3 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:09,986 [INFO] amber.core.language_model_activations: Prepared batch 4: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,034 [INFO] amber.core.language_model_activations: Saved batch 4 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,036 [INFO] amber.core.language_model_activations: Prepared batch 5: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,086 [INFO] amber.core.language_model_activations: Saved batch 5 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,088 [INFO] amber.core.language_model_activations: Prepared batch 6: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,137 [INFO] amber.core.language_model_activations: Saved batch 6 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,139 [INFO] amber.core.language_model_activations: Prepared batch 7: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,189 [INFO] amber.core.language_model_activations: Saved batch 7 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,190 [INFO] amber.core.language_model_activations: Prepared batch 8: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,239 [INFO] amber.core.language_model_activations: Saved batch 8 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,241 [INFO] amber.core.language_model_activations: Prepared batch 9: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,290 [INFO] amber.core.language_model_activations: Saved batch 9 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,292 [INFO] amber.core.language_model_activations: Prepared batch 10: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,340 [INFO] amber.core.language_model_activations: Saved batch 10 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,342 [INFO] amber.core.language_model_activations: Prepared batch 11: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,391 [INFO] amber.core.language_model_activations: Saved batch 11 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,393 [INFO] amber.core.language_model_activations: Prepared batch 12: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,445 [INFO] amber.core.language_model_activations: Saved batch 12 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,447 [INFO] amber.core.language_model_activations: Prepared batch 13: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,499 [INFO] amber.core.language_model_activations: Saved batch 13 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,501 [INFO] amber.core.language_model_activations: Prepared batch 14: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,553 [INFO] amber.core.language_model_activations: Saved batch 14 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,555 [INFO] amber.core.language_model_activations: Prepared batch 15: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,607 [INFO] amber.core.language_model_activations: Saved batch 15 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,609 [INFO] amber.core.language_model_activations: Prepared batch 16: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,663 [INFO] amber.core.language_model_activations: Saved batch 16 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,665 [INFO] amber.core.language_model_activations: Prepared batch 17: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,717 [INFO] amber.core.language_model_activations: Saved batch 17 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,719 [INFO] amber.core.language_model_activations: Prepared batch 18: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,769 [INFO] amber.core.language_model_activations: Saved batch 18 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,771 [INFO] amber.core.language_model_activations: Prepared batch 19: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,822 [INFO] amber.core.language_model_activations: Saved batch 19 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,823 [INFO] amber.core.language_model_activations: Prepared batch 20: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,875 [INFO] amber.core.language_model_activations: Saved batch 20 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,877 [INFO] amber.core.language_model_activations: Prepared batch 21: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,930 [INFO] amber.core.language_model_activations: Saved batch 21 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,931 [INFO] amber.core.language_model_activations: Prepared batch 22: items=8, seq_len=128\n",
      "2025-09-29 22:16:10,985 [INFO] amber.core.language_model_activations: Saved batch 22 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:10,986 [INFO] amber.core.language_model_activations: Prepared batch 23: items=8, seq_len=128\n",
      "2025-09-29 22:16:11,038 [INFO] amber.core.language_model_activations: Saved batch 23 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:11,040 [INFO] amber.core.language_model_activations: Prepared batch 24: items=8, seq_len=128\n",
      "2025-09-29 22:16:11,091 [INFO] amber.core.language_model_activations: Saved batch 24 for run=tinystories_20250929_221605 with keys=['activations', 'input_ids', 'attention_mask']\n",
      "2025-09-29 22:16:11,091 [INFO] amber.core.language_model_activations: Completed save_model_activations: run=tinystories_20250929_221605, batches_saved=25\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:16:23.835986Z",
     "start_time": "2025-09-29T20:16:23.809281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Inspect one saved batch; infer hidden size ---\n",
    "first_batch = next(store.iter_run_batches(RUN_ID))\n",
    "acts = first_batch[\"activations\"] if isinstance(first_batch, dict) else first_batch[0]\n",
    "print(\"Saved activations shape:\", tuple(acts.shape))\n",
    "\n",
    "# Flatten any leading dims to [N, D] to determine input dim\n",
    "hidden_dim = acts.shape[-1]\n",
    "print(\"Inferred hidden_dim:\", hidden_dim)\n"
   ],
   "id": "edcca5dda41f864",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved activations shape: (8, 128, 50257)\n",
      "Inferred hidden_dim: 50257\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:19:24.522321Z",
     "start_time": "2025-09-29T20:18:58.060215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Build the Sparse Autoencoder ---\n",
    "# A common recipe is to use overcomplete latents (e.g., 2x inputs)\n",
    "n_latents = hidden_dim * 2\n",
    "sae = Autoencoder(n_latents=n_latents, n_inputs=hidden_dim, activation=\"TopK_4\", tied=False, device=DEVICE)\n",
    "print(sae)\n"
   ],
   "id": "7895f98369678ddf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (activation): TopK(\n",
      "    k=4, act_fn=Identity(), use_abs=False\n",
      "    (act_fn): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T20:21:34.828318Z",
     "start_time": "2025-09-29T20:19:37.514655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Train the SAE from stored activations ---\n",
    "ckpt_dir = STORE_DIR / \"checkpoints\" / RUN_ID\n",
    "cfg = SAETrainingConfig(\n",
    "    epochs=SAE_EPOCHS,\n",
    "    batch_size=SAE_MINIBATCH,\n",
    "    lr=SAE_LR,\n",
    "    l1_lambda=SAE_L1,\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    max_batches_per_epoch=None,\n",
    "    validate_every=None,\n",
    "    checkpoint_dir=ckpt_dir,\n",
    "    project_decoder_grads=True,\n",
    "    renorm_decoder_every=100,  # maintain stable decoder scale periodically\n",
    ")\n",
    "\n",
    "trainer = SAETrainer(sae, store, RUN_ID, cfg)\n",
    "history = trainer.train()\n",
    "print(\"Training history:\", history)\n",
    "\n",
    "# Optionally, save final model\n",
    "final_dir = STORE_DIR / \"sae_models\" / RUN_ID\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "sae.save(\"final\", path=str(final_dir))\n",
    "print(\"Saved final SAE to:\", final_dir)\n"
   ],
   "id": "d33b202f8840ccce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
